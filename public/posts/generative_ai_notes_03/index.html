<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="utf-8">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="robots" content="noodp" />
        <title>Intro ao Pytorch - LobData</title><meta name="Description" content="This is my cool site"><meta property="og:title" content="Intro ao Pytorch" />
<meta property="og:description" content="Nesse artigo, vamos de m√£o na massa! Mas gostaria de fazer um disclaimer um pouco chato pra voc√™, vamos ver tudo de forma superficial, cada t√≥pico abordado aqui por si s√≥ precisaria de muitas p√°ginas de explica√ß√£o, ent√£o, vou fazer o melhor para a explica√ß√£o n√£o se tornar um Frankstein e o post virar uma cocha de retalhos.
Meu papel aqui √© trazer de forma objetiva cada t√≥pico desse para que voc√™ consiga correlacionar depois com o avan√ßar dos cap√≠tulos do livro que estou trazendo os reviews." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://www.lobdata.com.br/posts/generative_ai_notes_03/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2023-07-06T00:00:00+00:00" />
<meta property="article:modified_time" content="2023-07-06T00:00:00+00:00" /><meta property="og:site_name" content="LOBData" />
<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Intro ao Pytorch"/>
<meta name="twitter:description" content="Nesse artigo, vamos de m√£o na massa! Mas gostaria de fazer um disclaimer um pouco chato pra voc√™, vamos ver tudo de forma superficial, cada t√≥pico abordado aqui por si s√≥ precisaria de muitas p√°ginas de explica√ß√£o, ent√£o, vou fazer o melhor para a explica√ß√£o n√£o se tornar um Frankstein e o post virar uma cocha de retalhos.
Meu papel aqui √© trazer de forma objetiva cada t√≥pico desse para que voc√™ consiga correlacionar depois com o avan√ßar dos cap√≠tulos do livro que estou trazendo os reviews."/>
<meta name="application-name" content="My cool site">
<meta name="apple-mobile-web-app-title" content="My cool site"><meta name="theme-color" content="#ffffff"><meta name="msapplication-TileColor" content="#da532c"><link rel="shortcut icon" type="image/x-icon" href="/favicon.ico" />
        <link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
        <link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png"><link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png"><link rel="mask-icon" href="/safari-pinned-tab.svg" color="#5bbad5"><link rel="manifest" href="/site.webmanifest"><link rel="canonical" href="https://www.lobdata.com.br/posts/generative_ai_notes_03/" /><link rel="prev" href="https://www.lobdata.com.br/posts/generative_ai_notes_02/" /><link rel="next" href="https://www.lobdata.com.br/posts/cnns/" /><link rel="stylesheet" href="/css/style.min.css"><link rel="preload" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.1.1/css/all.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
        <noscript><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.1.1/css/all.min.css"></noscript><link rel="preload" href="https://cdn.jsdelivr.net/npm/animate.css@4.1.1/animate.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
        <noscript><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/animate.css@4.1.1/animate.min.css"></noscript><script type="application/ld+json">
    {
        "@context": "http://schema.org",
        "@type": "BlogPosting",
        "headline": "Intro ao Pytorch",
        "inLanguage": "en",
        "mainEntityOfPage": {
            "@type": "WebPage",
            "@id": "https:\/\/www.lobdata.com.br\/posts\/generative_ai_notes_03\/"
        },"genre": "posts","keywords": "AI, Generative, IA Generativa","wordcount":  2168 ,
        "url": "https:\/\/www.lobdata.com.br\/posts\/generative_ai_notes_03\/","datePublished": "2023-07-06T00:00:00+00:00","dateModified": "2023-07-06T00:00:00+00:00","publisher": {
            "@type": "Organization",
            "name": ""},"author": {
                "@type": "Person",
                "name": "Luciano"
            },"description": ""
    }
    </script></head>
    <body data-header-desktop="fixed" data-header-mobile="auto"><script type="text/javascript">(window.localStorage && localStorage.getItem('theme') ? localStorage.getItem('theme') === 'dark' : ('auto' === 'auto' ? window.matchMedia('(prefers-color-scheme: dark)').matches : 'auto' === 'dark')) && document.body.setAttribute('theme', 'dark');</script>

        <div id="mask"></div><div class="wrapper"><header class="desktop" id="header-desktop">
    <div class="header-wrapper">
        <div class="header-title">
            <a href="/" title="LobData">LOBData</a>
        </div>
        <div class="menu">
            <div class="menu-inner"><a class="menu-item" href="/posts/"> Posts </a><a class="menu-item" href="/tags/"> Tags </a><a class="menu-item" href="/about/"> About </a><a class="menu-item" href="/categories/"> Categories </a><span class="menu-item delimiter"></span><span class="menu-item search" id="search-desktop">
                        <input type="text" placeholder="Search titles or contents..." id="search-input-desktop">
                        <a href="javascript:void(0);" class="search-button search-toggle" id="search-toggle-desktop" title="Search">
                            <i class="fas fa-search fa-fw" aria-hidden="true"></i>
                        </a>
                        <a href="javascript:void(0);" class="search-button search-clear" id="search-clear-desktop" title="Clear">
                            <i class="fas fa-times-circle fa-fw" aria-hidden="true"></i>
                        </a>
                        <span class="search-button search-loading" id="search-loading-desktop">
                            <i class="fas fa-spinner fa-fw fa-spin" aria-hidden="true"></i>
                        </span>
                    </span><a href="javascript:void(0);" class="menu-item theme-switch" title="Switch Theme">
                    <i class="fas fa-adjust fa-fw" aria-hidden="true"></i>
                </a></div>
        </div>
    </div>
</header><header class="mobile" id="header-mobile">
    <div class="header-container">
        <div class="header-wrapper">
            <div class="header-title">
                <a href="/" title="LobData">LOBData</a>
            </div>
            <div class="menu-toggle" id="menu-toggle-mobile">
                <span></span><span></span><span></span>
            </div>
        </div>
        <div class="menu" id="menu-mobile"><div class="search-wrapper">
                    <div class="search mobile" id="search-mobile">
                        <input type="text" placeholder="Search titles or contents..." id="search-input-mobile">
                        <a href="javascript:void(0);" class="search-button search-toggle" id="search-toggle-mobile" title="Search">
                            <i class="fas fa-search fa-fw" aria-hidden="true"></i>
                        </a>
                        <a href="javascript:void(0);" class="search-button search-clear" id="search-clear-mobile" title="Clear">
                            <i class="fas fa-times-circle fa-fw" aria-hidden="true"></i>
                        </a>
                        <span class="search-button search-loading" id="search-loading-mobile">
                            <i class="fas fa-spinner fa-fw fa-spin" aria-hidden="true"></i>
                        </span>
                    </div>
                    <a href="javascript:void(0);" class="search-cancel" id="search-cancel-mobile">
                        Cancel
                    </a>
                </div><a class="menu-item" href="/posts/" title="">Posts</a><a class="menu-item" href="/tags/" title="">Tags</a><a class="menu-item" href="/about/" title="">About</a><a class="menu-item" href="/categories/" title="">Categories</a><a href="javascript:void(0);" class="menu-item theme-switch" title="Switch Theme">
                <i class="fas fa-adjust fa-fw" aria-hidden="true"></i>
            </a></div>
    </div>
</header><div class="search-dropdown desktop">
        <div id="search-dropdown-desktop"></div>
    </div>
    <div class="search-dropdown mobile">
        <div id="search-dropdown-mobile"></div>
    </div><main class="main">
                <div class="container"><div class="toc" id="toc-auto">
            <h2 class="toc-title">Contents</h2>
            <div class="toc-content" id="toc-content-auto"></div>
        </div><article class="page single"><h1 class="single-title animate__animated animate__flipInX">Intro ao Pytorch</h1><h2 class="single-subtitle">Chapter 02 - Generative Deep Learning Book</h2><div class="post-meta">
            <div class="post-meta-line"><span class="post-author"><a href="https://www.linkedin.com/in/lucianobatistads/" title="Author" target="_blank" rel="noopener noreffer author" class="author"><i class="fas fa-user-circle fa-fw" aria-hidden="true"></i>Luciano</a></span>&nbsp;<span class="post-category">included in <a href="/categories/tutorials/"><i class="far fa-folder fa-fw" aria-hidden="true"></i>Tutorials</a></span></div>
            <div class="post-meta-line"><i class="far fa-calendar-alt fa-fw" aria-hidden="true"></i>&nbsp;<time datetime="2023-07-06">2023-07-06</time>&nbsp;<i class="fas fa-pencil-alt fa-fw" aria-hidden="true"></i>&nbsp;2168 words&nbsp;
                <i class="far fa-clock fa-fw" aria-hidden="true"></i>&nbsp;11 minutes&nbsp;</div>
        </div><div class="featured-image"><img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="/img/intro-torch.png"
        data-srcset="/img/intro-torch.png, /img/intro-torch.png 1.5x, /img/intro-torch.png 2x"
        data-sizes="auto"
        alt="/img/intro-torch.png"
        title="/img/intro-torch.png" width="1456" height="816" /></div><div class="details toc" id="toc-static"  data-kept="">
                <div class="details-summary toc-title">
                    <span>Contents</span>
                    <span><i class="details-icon fas fa-angle-right" aria-hidden="true"></i></span>
                </div>
                <div class="details-content toc-content" id="toc-content-static"><nav id="TableOfContents">
  <ul>
    <li><a href="#tudo-come√ßa-com-tensores">Tudo come√ßa com tensores&hellip;</a></li>
    <li><a href="#por-debaixo-do-cap√¥">Por debaixo do cap√¥&hellip;</a>
      <ul>
        <li><a href="#diferen√ß√£o-autom√°tica">Diferen√ß√£o autom√°tica</a></li>
        <li><a href="#optimizers">Optimizers</a></li>
      </ul>
    </li>
    <li><a href="#primeira-rede-neural">Primeira Rede Neural</a>
      <ul>
        <li><a href="#training-loop">Training Loop</a></li>
        <li><a href="#data">Data</a></li>
        <li><a href="#model-e-loss">Model e Loss</a></li>
        <li><a href="#juntando-o-quebra-cabe√ßa-">Juntando o quebra-cabe√ßa ‚ú®</a></li>
        <li><a href="#fun√ß√µes-de-ativa√ß√£o-ao-resgate">Fun√ß√µes de ativa√ß√£o ao resgate</a></li>
      </ul>
    </li>
    <li><a href="#links-√∫teis">Links √∫teis</a></li>
  </ul>
</nav></div>
            </div><div class="content" id="content"><p>Nesse artigo, vamos de m√£o na massa! Mas gostaria de fazer um disclaimer um pouco chato pra voc√™, <strong>vamos ver tudo de forma superficial</strong>, cada t√≥pico abordado aqui por si s√≥ precisaria de muitas p√°ginas de explica√ß√£o, ent√£o, vou fazer o melhor para a explica√ß√£o n√£o se tornar um Frankstein e o post virar uma <em>cocha de retalhos</em>.</p>
<p>Meu papel aqui √© trazer de forma objetiva cada t√≥pico desse para que voc√™ consiga correlacionar depois com o avan√ßar dos cap√≠tulos do livro que estou trazendo os reviews.</p>
<p>E se voc√™ chegou apenas para esse artigo e n√£o sabe do contexto, eu na verdade estou trazendo uma s√©rie de blog posts sobre minhas anota√ß√µes sobre o livro <strong>Generative Deep Learning</strong>. E, j√° rolaram dois posts at√© ent√£o:</p>
<ul>
<li><a href="https://www.lobdata.com.br/posts/my_notes/" target="_blank" rel="noopener noreffer ">parte 1</a></li>
<li><a href="https://www.lobdata.com.br/posts/generative_ai_notes_02/" target="_blank" rel="noopener noreffer ">parte 2</a></li>
</ul>
<p>Voc√™ pode acompanhar na lateral (üëâ) o TOC do post e pular para parte que mais te interessa üòâ. Vamos l√°!!!</p>
<h2 id="tudo-come√ßa-com-tensores">Tudo come√ßa com tensores&hellip;</h2>
<p>De forma simples, tensores s√£o uma forma &lsquo;fancy&rsquo; de se representar arrays multidimensionais.</p>
<p>Voc√™ muito provavelmente j√° est√° acostumado a trabalhar com <code>numpy</code> arrays, por√©m apesar de terem um comportamento parecido, as implementa√ß√µes de tensores te fornecem n√£o s√≥ uma s√©rie de outras opera√ß√µes matem√°ticas e otimiza√ß√µes, mas tamb√©m a capacidade de rodar tudo isso em gpus ou tpus, o que basicamente torna o avan√ßo de Deep Learning poss√≠vel (do ponto de vista de for√ßa bruta para treinar os modelos).</p>
<div class="details admonition tip open">
        <div class="details-summary admonition-title">
            <i class="icon fas fa-lightbulb fa-fw" aria-hidden="true"></i>Nota<i class="details-icon fas fa-angle-right fa-fw" aria-hidden="true"></i>
        </div>
        <div class="details-content">
            <div class="admonition-content">Vale falar que os tensores n√£o s√£o exclusividade do <code>Pytorch</code> ta? O <strong>TENSOR</strong>Flow leva inclusive o termo escrito bem no nome do framework.</div>
        </div>
    </div>
<p>Definir um tensor no <code>Pytorch</code> √© bem simples:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">Float16</span><span class="p">)</span>
</span></span></code></pre></div><p>Devido a capacidade de rodar em diferentes dispositivos, como gpus, cpus e tpus, a biblioteca do <code>Pytorch</code> te possibilita migrar esse dado entre dispositivos, te dando total liberdade de como voc√™ vai executar o treinamento. Liberdade essa que se estende para multi-gpus, multi-tpus&hellip;</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="c1"># simples assim voc√™ leva um tensor para diferentes dispositivos</span>
</span></span><span class="line"><span class="cl"><span class="n">x</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s2">&#34;cuda&#34;</span><span class="p">)</span>
</span></span></code></pre></div><p>Para enviar e armazenar esse tensor na mem√≥ria da GPU, utilizamos o nome <code>cuda</code>. E, para CPU&hellip; √â cpu mesmo.</p>
<p>O nome <code>cuda</code>, para quem nunca ouviu falar, vem de <em>Compute Unified Device Architecture</em> e foi desenvolvido pela NVidia para permitir as implementa√ß√µes de processamento paralelo utilizando placas de v√≠deo.</p>
<p>Provavelmente por uma decis√£o de projeto, o nome <code>cuda</code> permanece sendo utilizado no <code>Pytorch</code> at√© hoje, mesmo n√£o sendo o mais intuitivo (pelo menos na minha humilde opni√£o), mas, bibliotecas mais recentes e que rodam sobre o <code>Pytorch</code>, como <code>pytorch-lightining</code> utiliza <code>gpu</code> para indicar que seu treinamento vai ser executado na GPU. Nada mais intuitivo, concorda?</p>
<p>Com grandes poderes, vem grandes responsabilidades!! E a configura√ß√£o do <code>device</code> hoje gera alguns dos erros mais comuns quando estamos trabalhando com o <code>Pytorch</code>.</p>
<p>Como √© bastante comum estarmos rodando o treinamento utilizando GPUs, para que tudo funcione, todos objetos que voc√™ trabalha precisam estar alocados na mem√≥ria de apenas um dispositivo.</p>
<ul>
<li>modelo</li>
<li>input</li>
<li>labels</li>
<li>pesos</li>
<li>etc</li>
</ul>
<p>Sendo n√≥s os respons√°veis por levar esse dado para o lugar certo, muitas vezes acabamos esquecendo de fazer isso, e o resultado √© uma bela mensagem de erro.</p>
<div class="details admonition danger open">
        <div class="details-summary admonition-title">
            <i class="icon fas fa-skull-crossbones fa-fw" aria-hidden="true"></i>Perigo<i class="details-icon fas fa-angle-right fa-fw" aria-hidden="true"></i>
        </div>
        <div class="details-content">
            <div class="admonition-content">Tente executar o c√≥digo abaixo, para voc√™ tamb√©m entrar para a estat√≠stica:</div>
        </div>
    </div>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">])</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s1">&#39;cuda&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">y</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">])</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s1">&#39;cpu&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="n">x</span> <span class="o">+</span> <span class="n">y</span><span class="p">)</span>
</span></span></code></pre></div><p>Gostaria de fazer uma men√ß√£o honrosa aqui a uma fun√ß√£o que eu de fato n√£o sei se est√° sendo muito utilizada por quem atua diretamente com o desenvolvimento de arquiteturas de Deep Learning, mas que quando eu vi eu achei super interessante, que s√£o os chamados <code>named_tensors</code>.</p>
<p>Basicamente voc√™ pode adicionar um nome para as dimens√µes dos tensores que voc√™ est√° trabalhando, o que possibilita um debug mais f√°cil quando algum problema acontece, e tamb√©m algumas opera√ß√µes podem ser feitas por esses nomes. Vou deixar ao fim do post um link para documenta√ß√£o com uma explica√ß√£o mais profunda e com alguns exemplos.</p>
<p>Temos tamb√©m uma pohaaada de opera√ß√µes que s√£o poss√≠veis de realizar com tensores, mas devido nossa abordagem aqui, vamose v√™-las a medida que fomos utilizando.</p>
<h2 id="por-debaixo-do-cap√¥">Por debaixo do cap√¥&hellip;</h2>
<p>A esse ponto, eu queria que voc√™ tivesse um modelo mental de que um treinamento de uma rede neural √© um encadeamento de opera√ß√µes, arranjadas de certa forma que n√≥s conseguimos atualizar pesos e par√¢metros que ir√£o no final cuminar em um modelo.</p>
<p>Esse conjunto de opera√ß√µes, e as formas como eles se conectam, s√£o criados no momento de execu√ß√£o e &ldquo;armazenados&rdquo; pelo <code>Pytorch</code>. Essa organiza√ß√£o √© feita em grafos que podem ser representados como na imagem abaixo.</p>
<a class="lightgallery" href="https://i.imgur.com/JL2RSfo.png" title="https://i.imgur.com/JL2RSfo.png" data-thumbnail="https://i.imgur.com/JL2RSfo.png">
        <img
            class="lazyload"
            src="/svg/loading.min.svg"
            data-src="https://i.imgur.com/JL2RSfo.png"
            data-srcset="https://i.imgur.com/JL2RSfo.png, https://i.imgur.com/JL2RSfo.png 1.5x, https://i.imgur.com/JL2RSfo.png 2x"
            data-sizes="auto"
            alt="https://i.imgur.com/JL2RSfo.png" width="800" />
    </a>
<p>O Autograd √© um m√≥dulo do <code>Pytorch</code> que permite o c√°lculo do gradiente, de forma perform√°tica e de forma completamente abstra√≠da para n√≥s, usu√°rios do framework.</p>
<p>Voc√™ pode, tamb√©m ir bem deep no entendimento dos detalhes internos do <code>Pytorch</code>, deixarei um post no fim do artigo para isso.</p>
<h3 id="diferen√ß√£o-autom√°tica">Diferen√ß√£o autom√°tica</h3>
<p>Blz!! Temos v√°rias opera√ß√µes para realizar, e consequentemente v√°rias derivadas para calcular, ser√° que precisamos fazer isso na m√£o??</p>
<p>De forma alguma&hellip; √â ent√£o que a diferencia√ß√£o autom√°tica entra em nossas vidas, essa feature do <code>Pytorch</code> permite que o framework consiga calcular o gradiente ao longo de toda a cadeia de opera√ß√µes realizadas pela sua rede neural, em rela√ß√£o a vari√°veis que voc√™ indica pra ele. Similar √† imagem abaixo:</p>
<a class="lightgallery" href="https://i.imgur.com/77Em0MV.png" title="https://i.imgur.com/77Em0MV.png" data-thumbnail="https://i.imgur.com/77Em0MV.png">
        <img
            class="lazyload"
            src="/svg/loading.min.svg"
            data-src="https://i.imgur.com/77Em0MV.png"
            data-srcset="https://i.imgur.com/77Em0MV.png, https://i.imgur.com/77Em0MV.png 1.5x, https://i.imgur.com/77Em0MV.png 2x"
            data-sizes="auto"
            alt="https://i.imgur.com/77Em0MV.png" width="800" />
    </a>
<p>Essa indica√ß√£o das vari√°veis que ser√£o consideradas na hora do c√°lculo do gradiente √© feita pelo par√¢metro <code>require_grad=True</code>. Dessa forma o <code>Pytorch</code> vai armazenar o valor do gradiente em uma propriedade chamada <code>.grad</code>.</p>
<h3 id="optimizers">Optimizers</h3>
<p>No artigo passado n√≥s falamos sobre minimiza√ß√£o, esse processo que acaba sendo chamado de otimiza√ß√£o. Justamente por esse motivo, o <code>Pytorch</code> criou uma abstra√ß√£o chamada, adivinha o nome?, <code>optimizers</code>. Nesse m√≥dulo voc√™ vai encontrar diversos m√©todos de otimiza√ß√£o, entre eles, um dos mais comuns, chamado de Stochastic Gradient Descent (SGB).</p>
<p>A imagem abaixo mostra como o processo de otimiza√ß√£o acontece:</p>
<a class="lightgallery" href="https://i.imgur.com/dCqfggz.png" title="https://i.imgur.com/dCqfggz.png" data-thumbnail="https://i.imgur.com/dCqfggz.png">
        <img
            class="lazyload"
            src="/svg/loading.min.svg"
            data-src="https://i.imgur.com/dCqfggz.png"
            data-srcset="https://i.imgur.com/dCqfggz.png, https://i.imgur.com/dCqfggz.png 1.5x, https://i.imgur.com/dCqfggz.png 2x"
            data-sizes="auto"
            alt="https://i.imgur.com/dCqfggz.png" width="800" />
    </a>
<p>Ent√£o, relembrando, n√≥s temos uma loss, n√≥s precisamos minimizar, esse processo se chama otimiza√ß√£o, e minimizar essa fun√ß√£o implica que os pesos ao longo da arquitetura da rede neural sejam atualizados.</p>
<p>Em c√≥digo, veja abaixo como esses passos se desenrolam:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="c1"># learning rate</span>
</span></span><span class="line"><span class="cl"><span class="n">eta</span> <span class="o">=</span> <span class="mf">0.1</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># vari√°vel que o modelo vai considerar na hora de minimizar a fun√ß√£o</span>
</span></span><span class="line"><span class="cl"><span class="n">x_param</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="o">-</span><span class="mf">3.5</span><span class="p">]),</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># escolha do optimizer a ser utilizado</span>
</span></span><span class="line"><span class="cl"><span class="n">optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">SGD</span><span class="p">([</span><span class="n">x_param</span><span class="p">],</span> <span class="n">lr</span><span class="o">=</span><span class="n">eta</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># as √©pocas s√£o como n√≥s nomeamos as itera√ß√µes</span>
</span></span><span class="line"><span class="cl"><span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">200</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># como a cada itera√ß√£o o torch mant√©m os valores antigos do gradiente</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># o zero_grad() √© justamente para zerar esse dados</span>
</span></span><span class="line"><span class="cl">    <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">    <span class="n">loss_incurred</span> <span class="o">=</span> <span class="n">f</span><span class="p">(</span><span class="n">x_param</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># fazemos o c√°lculo</span>
</span></span><span class="line"><span class="cl">    <span class="n">loss_incurred</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># atualizamos os pesos para pr√≥xima itera√ß√£o</span>
</span></span><span class="line"><span class="cl">    <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="n">x_param</span><span class="o">.</span><span class="n">data</span><span class="p">)</span>
</span></span></code></pre></div><pre tabindex="0"><code>tensor([2.0000])
</code></pre><p>Nosso resultado aqui √© o mesmo do mostrado no artigo passado, s√≥ que dessa vez n√≥s realizamos o processo de forma iterativa. Guarda esse processo, mais abaixo n√≥s tamb√©m vamos utiliz√°-lo para o treinamento da nossa <strong>primeira rede neural</strong>.</p>
<h2 id="primeira-rede-neural">Primeira Rede Neural</h2>
<p>Um insight muito massa que eu tive ao ler o livro <code>Inside Deep Learning</code>, √© que o <code>Pytorch</code> foi constru√≠do com uma premissa bem forte de que todo treinamento de uma rede neural √© na verdade um problema de otimiza√ß√£o.</p>
<p>Ent√£o, independentemente do problema que estamos atacando (classifica√ß√£o, regress√£o&hellip;), temos que pensar o problema como um problema de otimiza√ß√£o. E de fato isso faz muito sentido, dado que todos os pesos dos modelos s√£o alterados com base no processo de minimiza√ß√£o de uma fun√ß√£o, a loss.</p>
<p>No processo de treinamento de uma rede neural fica ent√£o evidenciado um padr√£o. N√≥s teremos sempre <code>dados</code> que ir√£o alimentar o <code>modelo</code>, teremos o modelo (nossa arquitetura) e a <code>loss</code> que vai alterar a depender do tipo de task que estaremos atacando. A seguinte imagem traduz muito bem o processo:</p>
<a class="lightgallery" href="https://i.imgur.com/cftsFDk.png" title="https://i.imgur.com/cftsFDk.png" data-thumbnail="https://i.imgur.com/cftsFDk.png">
        <img
            class="lazyload"
            src="/svg/loading.min.svg"
            data-src="https://i.imgur.com/cftsFDk.png"
            data-srcset="https://i.imgur.com/cftsFDk.png, https://i.imgur.com/cftsFDk.png 1.5x, https://i.imgur.com/cftsFDk.png 2x"
            data-sizes="auto"
            alt="https://i.imgur.com/cftsFDk.png" width="800" />
    </a>
<p>Vamos ent√£o codar pedacinho desse e ver como desenrola na pr√°tica!!</p>
<h3 id="training-loop">Training Loop</h3>
<p>Vimos acima, com um exemplo mais simples, que esse √© um processo iterativo. A implementa√ß√£o que voc√™ v√™ abaixo, √© uma adapta√ß√£o do anterior para contemplar uma situa√ß√£o real de treinamento de uma rede neural.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="c1"># apenas para ter um typehint</span>
</span></span><span class="line"><span class="cl"><span class="n">Loss</span> <span class="o">=</span> <span class="n">Callable</span><span class="p">[[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">],</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">train_simple_network</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">    <span class="n">model</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="n">loss_func</span><span class="p">:</span> <span class="n">Loss</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="n">training_loader</span><span class="p">:</span> <span class="n">DataLoader</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="n">epochs</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">100</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="n">device</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&#34;cuda&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl"><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># 1</span>
</span></span><span class="line"><span class="cl">    <span class="n">optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.001</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">model</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="n">tqdm</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">epochs</span><span class="p">),</span> <span class="n">desc</span><span class="o">=</span><span class="s2">&#34;Epochs&#34;</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># 2</span>
</span></span><span class="line"><span class="cl">        <span class="n">model</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">        <span class="n">running_loss</span> <span class="o">=</span> <span class="mf">0.0</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="k">for</span> <span class="nb">input</span><span class="p">,</span> <span class="n">labels</span> <span class="ow">in</span> <span class="n">tqdm</span><span class="p">(</span><span class="n">training_loader</span><span class="p">,</span> <span class="n">desc</span><span class="o">=</span><span class="s2">&#34;Training&#34;</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">            <span class="c1"># 3</span>
</span></span><span class="line"><span class="cl">            <span class="nb">input</span> <span class="o">=</span> <span class="nb">input</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">            <span class="n">labels</span> <span class="o">=</span> <span class="n">labels</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">            <span class="c1"># 4</span>
</span></span><span class="line"><span class="cl">            <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">            <span class="c1"># 5</span>
</span></span><span class="line"><span class="cl">            <span class="n">output</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">            <span class="n">loss</span> <span class="o">=</span> <span class="n">loss_func</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">            <span class="c1"># 6</span>
</span></span><span class="line"><span class="cl">            <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">            <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">            <span class="n">running_loss</span> <span class="o">+=</span> <span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
</span></span></code></pre></div><p>Nesse fluxo o que est√° acontecendo √© o seguinte:</p>
<ol>
<li>Iniciamos o optimizer e enviamos o modelo para o device correto</li>
<li>Colocamos o modelo em modo de treino, indicando para o <code>Pytorch</code> que eu quero atualizar os pesos</li>
<li>Colocamos os dados para o device correto</li>
<li>Muito importante, zeramos o gradiente</li>
<li>Fazemos o &ldquo;predict&rdquo; e avaliamos o qu√£o distante estamos do valor real, utilizando a loss para isso</li>
<li>Calculamos o gradiente e enfim atualizamos os pesos</li>
</ol>
<h3 id="data">Data</h3>
<p>Como vamos treinar para uma task de regress√£o, vamo gerar aqui alguns dados sint√©ticos com aux√≠lio do <code>numpy</code> e vamos tamb√©m visualizar o resultado.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">20</span><span class="p">,</span> <span class="n">num</span><span class="o">=</span><span class="mi">200</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">y</span> <span class="o">=</span> <span class="n">X</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="n">X</span><span class="p">)</span> <span class="o">*</span> <span class="mi">2</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">sns</span><span class="o">.</span><span class="n">scatterplot</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="n">y</span><span class="p">)</span>
</span></span></code></pre></div><a class="lightgallery" href="https://i.imgur.com/eSUJdmr.png" title="https://i.imgur.com/eSUJdmr.png" data-thumbnail="https://i.imgur.com/eSUJdmr.png">
        <img
            class="lazyload"
            src="/svg/loading.min.svg"
            data-src="https://i.imgur.com/eSUJdmr.png"
            data-srcset="https://i.imgur.com/eSUJdmr.png, https://i.imgur.com/eSUJdmr.png 1.5x, https://i.imgur.com/eSUJdmr.png 2x"
            data-sizes="auto"
            alt="https://i.imgur.com/eSUJdmr.png" width="800" />
    </a>
<p>Como foi dito no √∫ltimo artigo, o <code>Pytorch</code> trabalha com duas abstra√ß√µes chamadas de <code>Dataset</code> e <code>DataLoader</code>. Elas s√£o respons√°veis por alimentar seu treinamento com os dados, fazendo isso de forma bem perform√°tica.</p>
<p>As imagens abaixo ilustram muito bem o papel de cada um:</p>
<p><a class="lightgallery" href="https://i.imgur.com/KTKptDw.png" title="https://i.imgur.com/KTKptDw.png" data-thumbnail="https://i.imgur.com/KTKptDw.png">
        <img
            class="lazyload"
            src="/svg/loading.min.svg"
            data-src="https://i.imgur.com/KTKptDw.png"
            data-srcset="https://i.imgur.com/KTKptDw.png, https://i.imgur.com/KTKptDw.png 1.5x, https://i.imgur.com/KTKptDw.png 2x"
            data-sizes="auto"
            alt="https://i.imgur.com/KTKptDw.png" width="800" />
    </a>
<a class="lightgallery" href="https://i.imgur.com/dhd1XJy.png" title="https://i.imgur.com/dhd1XJy.png" data-thumbnail="https://i.imgur.com/dhd1XJy.png">
        <img
            class="lazyload"
            src="/svg/loading.min.svg"
            data-src="https://i.imgur.com/dhd1XJy.png"
            data-srcset="https://i.imgur.com/dhd1XJy.png, https://i.imgur.com/dhd1XJy.png 1.5x, https://i.imgur.com/dhd1XJy.png 2x"
            data-sizes="auto"
            alt="https://i.imgur.com/dhd1XJy.png" width="800" />
    </a></p>
<p>Na primeira, o que a gente v√™ √© o <code>Dataset</code> sendo o respons√°vel por ir no nosso dado e selecionar um item. Por isso, dois m√©todos s√£o obrigat√≥rios quando estamos implementando o <code>Dataset</code>:</p>
<ul>
<li><code>__len__</code>: vai nos dizer o tamanho do dataset</li>
<li><code>__getitem__</code>: vai coletar um item do dataset</li>
</ul>
<p>Na segunda imagem, voc√™ v√™ a atua√ß√£o do DataLoader, que tem o objetivo de pedir ao <code>Dataset</code> por espec√≠ficos items. Como n√≥s, durante o treinamento, passamos os dados em lote e embaralhados, os √≠ndices que est√£o sendo pedidos ao <code>Dataset</code> acabam n√£o tendo uma ordem.</p>
<p>Do ponto de vista de implementa√ß√£o, basicamente o m√©todo <code>__getitem__</code> <strong>precisa retornar uma tupla com o item + label</strong>, seja os tensores das imagens, de texto, som&hellip; E o seu trabalho √© basicamente adaptar o dado bruto para essa estrutura.</p>
<p>Em alguns casos, o <code>Pytorch</code> facilita esse trabalho e n√≥s n√£o precisamos codar uma classe <code>Dataset</code> customizada, como por exemplo quando trabalhamos com imagens. Veremos mais detalhes sobre, em pr√≥ximos artigos.</p>
<p>Certo, eis aqui nosso <code>Dataset</code> e <code>DataLoader</code>:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">class</span> <span class="nc">SimpleRegressionDataset</span><span class="p">(</span><span class="n">Dataset</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">y</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">X</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">y</span> <span class="o">=</span> <span class="n">y</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="fm">__len__</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="fm">__getitem__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">idx</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">tuple</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]:</span>
</span></span><span class="line"><span class="cl">        <span class="n">X</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">X</span><span class="p">[</span><span class="n">idx</span><span class="p">,</span> <span class="p">:],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">y</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">y</span><span class="p">[</span><span class="n">idx</span><span class="p">,</span> <span class="p">:],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">training_dataset</span> <span class="o">=</span> <span class="n">SimpleRegressionDataset</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">training_loader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span><span class="n">training_dataset</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</span></span></code></pre></div><h3 id="model-e-loss">Model e Loss</h3>
<p>Basicamente voc√™ pode criar uma arquitetura (modelo) de Deep Learning de duas formas. Respeitando a orienta√ß√£o a objeto ou pelo paradigma funcional.</p>
<p>Por OOP n√≥s criamos uma classe e herdamos do <code>Pytorch</code> a classe <code>Module</code> e obrigatoriamente precisamos implementar o m√©todo <code>forward</code>. Vamos simplificar aqui e criar nosso modelo utilizando o paradigma funcional, que ficaria assim:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">simple_model</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">10</span><span class="p">),</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
</span></span></code></pre></div><p>Pronto, temos nosso primeiro modelo üéâ, que de forma visual, seria algo como na seguinte imagem, lendo debaixo para cima:</p>
<a class="lightgallery" href="https://i.imgur.com/ZivUyKV.png" title="https://i.imgur.com/ZivUyKV.png" data-thumbnail="https://i.imgur.com/ZivUyKV.png">
        <img
            class="lazyload"
            src="/svg/loading.min.svg"
            data-src="https://i.imgur.com/ZivUyKV.png"
            data-srcset="https://i.imgur.com/ZivUyKV.png, https://i.imgur.com/ZivUyKV.png 1.5x, https://i.imgur.com/ZivUyKV.png 2x"
            data-sizes="auto"
            alt="https://i.imgur.com/ZivUyKV.png" width="800" />
    </a>
<p>E ent√£o, nossa loss aqui vai ser a MSE (Mean Squared Error):</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">loss_func</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">MSELoss</span><span class="p">()</span>
</span></span></code></pre></div><h3 id="juntando-o-quebra-cabe√ßa-">Juntando o quebra-cabe√ßa ‚ú®</h3>
<p>Just run&hellip;</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">&#34;cuda&#34;</span> <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="s2">&#34;cpu&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">train_simple_network</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">loss_func</span><span class="p">,</span> <span class="n">training_loader</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">,</span> <span class="n">epochs</span><span class="o">=</span><span class="mi">1000</span><span class="p">)</span>
</span></span></code></pre></div><div class="details admonition tip open">
        <div class="details-summary admonition-title">
            <i class="icon fas fa-lightbulb fa-fw" aria-hidden="true"></i>Nota<i class="details-icon fas fa-angle-right fa-fw" aria-hidden="true"></i>
        </div>
        <div class="details-content">
            <div class="admonition-content">√â comum voc√™ encontrar esse condicional para buscar por <code>cuda</code> caso ela esteja dispon√≠vel no seu computador, caso contr√°rio use <code>cpu</code>. Caso esteja confort√°vel em sempre utilizar a GPU, pode remover e apenas deixar <code>cuda</code>.</div>
        </div>
    </div>
<p>Avaliando nossos resultados:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">inference_mode</span><span class="p">():</span>
</span></span><span class="line"><span class="cl">    <span class="n">Y_pred</span> <span class="o">=</span> <span class="p">(</span>
</span></span><span class="line"><span class="cl">        <span class="n">model</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">        <span class="o">.</span><span class="n">cpu</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">        <span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">    <span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">sns</span><span class="o">.</span><span class="n">scatterplot</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="n">y</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">sns</span><span class="o">.</span><span class="n">lineplot</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="n">Y_pred</span><span class="o">.</span><span class="n">ravel</span><span class="p">(),</span> <span class="n">color</span><span class="o">=</span><span class="s2">&#34;red&#34;</span><span class="p">)</span>
</span></span></code></pre></div><a class="lightgallery" href="https://i.imgur.com/fuKABqV.png" title="https://i.imgur.com/fuKABqV.png" data-thumbnail="https://i.imgur.com/fuKABqV.png">
        <img
            class="lazyload"
            src="/svg/loading.min.svg"
            data-src="https://i.imgur.com/fuKABqV.png"
            data-srcset="https://i.imgur.com/fuKABqV.png, https://i.imgur.com/fuKABqV.png 1.5x, https://i.imgur.com/fuKABqV.png 2x"
            data-sizes="auto"
            alt="https://i.imgur.com/fuKABqV.png" width="800" />
    </a>
<p>A linha reta em vermelho aqui s√£o nossas previs√µes. Mas, por que ser√° que o modelo n√£o conseguiu capturar a n√£o lineariedade dos dados?</p>
<p>Isso acontece basicamente por que estamos concatenando opera√ß√µes lineares uma atr√°s da outra. E nesse caso, no final, se voc√™ utilizasse 1000 camadas no <code>nn.Sequential</code> esse modelo n√£o conseguiria capturar esse perfil n√£o linear dos dados.</p>
<h3 id="fun√ß√µes-de-ativa√ß√£o-ao-resgate">Fun√ß√µes de ativa√ß√£o ao resgate</h3>
<p>Para resolver esse problema, n√≥s adicionamos uma perturba√ß√£o nas camadas internas da rede neural, que auxiliam o modelo a representar n√£o lineariedades.</p>
<p>Vou deixar uma imagem aqui com algumas fun√ß√µes de ativa√ß√£o, e em seguimos vamos reimplementar o c√≥digo, usando a <code>Tanh()</code>.</p>
<a class="lightgallery" href="https://i.imgur.com/DRxjyPv.png" title="https://i.imgur.com/DRxjyPv.png" data-thumbnail="https://i.imgur.com/DRxjyPv.png">
        <img
            class="lazyload"
            src="/svg/loading.min.svg"
            data-src="https://i.imgur.com/DRxjyPv.png"
            data-srcset="https://i.imgur.com/DRxjyPv.png, https://i.imgur.com/DRxjyPv.png 1.5x, https://i.imgur.com/DRxjyPv.png 2x"
            data-sizes="auto"
            alt="https://i.imgur.com/DRxjyPv.png" width="800" />
    </a>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">model</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">10</span><span class="p">),</span> <span class="n">nn</span><span class="o">.</span><span class="n">Tanh</span><span class="p">(),</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
</span></span></code></pre></div><p>E ent√£o, rodamos novamente o treinamento:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">train_simple_network</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">loss_func</span><span class="p">,</span> <span class="n">training_loader</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">,</span> <span class="n">epochs</span><span class="o">=</span><span class="mi">1000</span><span class="p">)</span>
</span></span></code></pre></div><p>Agora, como vemos na figura, foi poss√≠vel capturar o formato n√£o linear dos dados.</p>
<a class="lightgallery" href="https://i.imgur.com/Sd3H5sc.png" title="https://i.imgur.com/Sd3H5sc.png" data-thumbnail="https://i.imgur.com/Sd3H5sc.png">
        <img
            class="lazyload"
            src="/svg/loading.min.svg"
            data-src="https://i.imgur.com/Sd3H5sc.png"
            data-srcset="https://i.imgur.com/Sd3H5sc.png, https://i.imgur.com/Sd3H5sc.png 1.5x, https://i.imgur.com/Sd3H5sc.png 2x"
            data-sizes="auto"
            alt="https://i.imgur.com/Sd3H5sc.png" width="800" />
    </a>
<p>Show, para esse artigo era isso, espero que tenha conseguido deixar um pouco mais claro quais s√£o as principais pe√ßas na hora de montar esse puzzle do Deep Learning.</p>
<p>Agora, n√≥s iremos come√ßar a entrar mais nas particularidades das diferentes arquiteturas, come√ßando por CNNs, at√© l√°!</p>
<h2 id="links-√∫teis">Links √∫teis</h2>
<ul>
<li>Books: [Inside Deep Learning, Generative Deep Learning, Deep Learning with Pytorch]</li>
<li>Reposit√≥rio: <a href="https://github.com/LucianoBatista/generative-ai" target="_blank" rel="noopener noreffer ">link</a></li>
<li>Named Tensors doc: <a href="https://pytorch.org/docs/stable/named_tensor.html" target="_blank" rel="noopener noreffer ">link</a></li>
<li>Pytorch Internals: <a href="https://pytorch.org/blog/computational-graphs-constructed-in-pytorch/" target="_blank" rel="noopener noreffer ">link</a></li>
</ul>
</div><div class="post-footer" id="post-footer">
    <div class="post-info">
        <div class="post-info-line">
            <div class="post-info-mod">
                <span>Updated on 2023-07-06</span>
            </div></div>
        <div class="post-info-line">
            <div class="post-info-md"></div>
            <div class="post-info-share">
                <span><a href="javascript:void(0);" title="Share on Linkedin" data-sharer="linkedin" data-url="https://www.lobdata.com.br/posts/generative_ai_notes_03/"><i class="fab fa-linkedin fa-fw" aria-hidden="true"></i></a></span>
            </div>
        </div>
    </div>

    <div class="post-info-more">
        <section class="post-tags"><i class="fas fa-tags fa-fw" aria-hidden="true"></i>&nbsp;<a href="/tags/ai/">AI</a>,&nbsp;<a href="/tags/generative/">Generative</a>,&nbsp;<a href="/tags/ia-generativa/">IA Generativa</a></section>
        <section>
            <span><a href="javascript:void(0);" onclick="window.history.back();">Back</a></span>&nbsp;|&nbsp;<span><a href="/">Home</a></span>
        </section>
    </div>

    <div class="post-nav"><a href="/posts/generative_ai_notes_02/" class="prev" rel="prev" title="Deep Learning"><i class="fas fa-angle-left fa-fw" aria-hidden="true"></i>Deep Learning</a>
            <a href="/posts/cnns/" class="next" rel="next" title="CNNs">CNNs<i class="fas fa-angle-right fa-fw" aria-hidden="true"></i></a></div>
</div>
<div id="comments"></div></article></div>
            </main><footer class="footer">
        <div class="footer-container"><div class="footer-line">Powered by <a href="https://gohugo.io/" target="_blank" rel="noopener noreffer" title="Hugo 0.115.3">Hugo</a> | Theme - <a href="https://github.com/dillonzq/LoveIt" target="_blank" rel="noopener noreffer" title="LoveIt 0.2.11"><i class="far fa-kiss-wink-heart fa-fw" aria-hidden="true"></i> LoveIt</a>
                </div><div class="footer-line" itemscope itemtype="http://schema.org/CreativeWork"><i class="far fa-copyright fa-fw" aria-hidden="true"></i><span itemprop="copyrightYear">2022 - 2023</span><span class="author" itemprop="copyrightHolder">&nbsp;<a href="/" target="_blank">Luba</a></span></div>
        </div>
    </footer></div>

        <div id="fixed-buttons"><a href="#" id="back-to-top" class="fixed-button" title="Back to Top">
                <i class="fas fa-arrow-up fa-fw" aria-hidden="true"></i>
            </a><a href="#" id="view-comments" class="fixed-button" title="View Comments">
                <i class="fas fa-comment fa-fw" aria-hidden="true"></i>
            </a>
        </div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.0/dist/katex.min.css"><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/autocomplete.js@0.38.1/dist/autocomplete.min.js"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/algoliasearch@4.13.1/dist/algoliasearch-lite.umd.min.js"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/lazysizes@5.3.2/lazysizes.min.js"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/clipboard@2.0.11/dist/clipboard.min.js"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/sharer.js@0.5.1/sharer.min.js"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/katex@0.16.0/dist/katex.min.js"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/katex@0.16.0/dist/contrib/auto-render.min.js"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/katex@0.16.0/dist/contrib/copy-tex.min.js"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/katex@0.16.0/dist/contrib/mhchem.min.js"></script><script type="text/javascript">window.config={"code":{"copyTitle":"Copy to clipboard","maxShownLines":50},"comment":{},"math":{"delimiters":[{"display":true,"left":"$$","right":"$$"},{"display":true,"left":"\\[","right":"\\]"},{"display":true,"left":"\\begin{equation}","right":"\\end{equation}"},{"display":true,"left":"\\begin{equation*}","right":"\\end{equation*}"},{"display":true,"left":"\\begin{align}","right":"\\end{align}"},{"display":true,"left":"\\begin{align*}","right":"\\end{align*}"},{"display":true,"left":"\\begin{alignat}","right":"\\end{alignat}"},{"display":true,"left":"\\begin{alignat*}","right":"\\end{alignat*}"},{"display":true,"left":"\\begin{gather}","right":"\\end{gather}"},{"display":true,"left":"\\begin{CD}","right":"\\end{CD}"},{"display":false,"left":"$","right":"$"},{"display":false,"left":"\\(","right":"\\)"}],"strict":false},"search":{"algoliaAppID":"56XLI6KGAP","algoliaIndex":"lobdata","algoliaSearchKey":"d7c82e758baf996bfc63e0fffb1098fd","highlightTag":"em","maxResultLength":10,"noResultsFound":"No results found","snippetLength":30,"type":"algolia"}};</script><script type="text/javascript" src="/js/theme.min.js"></script></body>
</html>
