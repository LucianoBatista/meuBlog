[{"categories":["Tutorials"],"content":"Hoje n√≥s vamos avan√ßar mais um pouco em dire√ß√£o ao universo de arquiteturas de Deep Learning relacionadas a IA Generativa. At√© aqui, n√≥s j√° falamos sobre: IA Generativa, abordando de forma geral Pytorch, explicando um pouco sobre o framework Deep Learning, trazendo conceitos b√°sicos √∫teis para nosso entendimento ao decorrer dos artigos. E agora, vamos falar sobre Redes Neurais Convolucionais (CNNs). ","date":"2023-07-06","objectID":"/posts/cnns/:0:0","tags":["AI","Generative","IA Generativa"],"title":"CNNs","uri":"/posts/cnns/"},{"categories":["Tutorials"],"content":"Por que CNNs? As Convolutional Neural Networs s√£o o lugar comum por onde n√≥s normalmente come√ßamos a estudar a fim de entender sobre arquiteturas de Deep Learning. Acredito que muito pelo trade-of que elas oferecem, em rela√ß√£o a complexidade vs aplica√ß√µes. Entender conceitualmente sobre essas redes neurais acaba sendo mais intuitivo do que entender outras arquiteturas de Deep Learning, como RNNs. Al√©m disso, voc√™ consegue utiliz√°-las para realizar tarefas relacionadas a Vis√£o Computacional. E um √∫ltimo motivo √© que a premissa inicial dessa s√©rie de blog posts √© tentar caminhar junto com o livro (Generative Deep Learning), e por isso que entramos na arquitetura abordada no cap√≠tulo 2. ","date":"2023-07-06","objectID":"/posts/cnns/:1:0","tags":["AI","Generative","IA Generativa"],"title":"CNNs","uri":"/posts/cnns/"},{"categories":["Tutorials"],"content":"Conhecimento a priori No artigo passado‚Ä¶ Falamos sobre imagens serem na realidade um tipo de dado que possui uma estrutura atrelada, contrariando o que aprendemos, onde imagens s√£o um tipo de dado desestruturado. Estrutura aqui, vem muito no sentido da composi√ß√£o dessas imagens, sempre com um ‚Äúarranjo‚Äù de matrizes, onde cada posi√ß√£o desse dado vai representar um pixel. Uma imagem, al√©m das dimens√µes de largura e altura (linhas e colunas), tamb√©m possuem chanels (dimens√µes). O Pytorch, framework de Deep Learning que estamos utlizando, acaba assumindo uma conven√ß√£o para quando uma imagem √© representada como um tensor: N: n√∫mero do batch de imagens C: n√∫mero de channels W: largura H: altura This is a tip Tudo isso vem composto na seguinte ordem (N, C, W, H). Mais a frente veremos na pr√°tica como as imagens s√£o representadas no Pytorch. A estrutura de uma image nos indica que o valor que um pixel pode assumir n√£o √© independente do valor do pixel vizinho, em outras palavras, n√≥s iremos ter uma correla√ß√£o entre os pixels de um determinado conjunto de imagens e eles n√£o podem ser analisados isoladamente. Esse tipo de conhecimento a priori √© tamb√©m chamado de estrutura espacial a priori e √© ela que as CNNs tentam ‚Äúaprender‚Äù. Apenas para ilustrar, se n√≥s embaralhasse-mos os pixels de uma imagem de um n√∫mero 5, o mesmo ficaria irreconhec√≠vel, corroborando para esse conceito de correla√ß√£o dos pixels. Veja na imagem abaixo: ","date":"2023-07-06","objectID":"/posts/cnns/:2:0","tags":["AI","Generative","IA Generativa"],"title":"CNNs","uri":"/posts/cnns/"},{"categories":["Tutorials"],"content":"O que s√£o convolu√ß√µes? Convolu√ß√£o nada mais √© que uma fun√ß√£o matem√°tica que recebe dois inputs: uma imagem e um filtro (tamb√©m chamado de kernel). Como sa√≠da dessa fun√ß√£o temos uma nova imagem. Mas como √© que essa opera√ß√£o pode ser utilizada para treinamento de uma rede neural, e eventual aprendizado de padr√µes sobre as imagens? O principal respons√°vel para isso s√£o os filtros. Esse input da fun√ß√£o √© muito especial e t√™m a capacidade de reconhecer certos padr√µes na entrada e real√ß√°-los na sa√≠da. Tradicionalmente, os filtros t√™m sido utilizados em muitas aplica√ß√µes para capturar padr√µes ou aplicar comportamentos espec√≠ficos √†s imagens, como por exemplo: aplicar desfoque gaussiano detectar linhas horizontais aumentar a nitidez capturar as bordas das imagens Por√©m, quando aplicados ao deep learning, nosso real interesse √© encontrar os melhores filtros que possam criar os melhores padr√µes para representar os dados de maneira mais adequada. No final, com essa √≥tima representa√ß√£o, seremos capazes de classificar, detectar ou realizar qualquer outra tarefa relacionada √† vis√£o computacional. As convolu√ß√µes existem em diferentes dimens√µes (1D, 2D, ‚Ä¶), mas o racional por de tr√°s continur√° sendo o mesmo. Veja na imagem abaixo como o filtro vai percorrer a imagem e ao realizar os c√°lculos vai retornar uma nova imagem. Um ponto importante a observar √© que a imagem resultante (imagem em verde), pela pura opera√ß√£o do convolu√ß√£o √© uma imagem menor que a original. Como resolvemos isso? ","date":"2023-07-06","objectID":"/posts/cnns/:3:0","tags":["AI","Generative","IA Generativa"],"title":"CNNs","uri":"/posts/cnns/"},{"categories":["Tutorials"],"content":"Padding √â justamente para isso que existem os paddings. De forma simples, os paddings v√£o adicionar linhas e colunas ao redor da imagem original, para que durante a opera√ß√£o de convolu√ß√£o, a imagem resultante permane√ßa com a mesma dimens√£o. O mais comum √© o zero-padding, mas as refer√™ncias citam que podemos utilizar outros tipos tamb√©m (eu particularmente nunca usei). ","date":"2023-07-06","objectID":"/posts/cnns/:3:1","tags":["AI","Generative","IA Generativa"],"title":"CNNs","uri":"/posts/cnns/"},{"categories":["Tutorials"],"content":"Processamento de imagens com convolu√ß√µes Como dito acima, convolu√ß√£o √© apenas uma opera√ß√£o matem√°tica. Sendo assim, ao longo do tempo a pesquisa cient√≠fica chegou h√° alguns kernels que aplicam algumas transforma√ß·∫Ωos em imagens. Ent√£o, vamos realizar um experimento e passar alguns kernels por algumas imagens do dataset mais batido da bolha de data science, MNIST. Para isso, vamos utilizar o Torchvision, ele nos fornece um meio de baixar esses dados e automaticamente criar um objeto Dataset: ","date":"2023-07-06","objectID":"/posts/cnns/:4:0","tags":["AI","Generative","IA Generativa"],"title":"CNNs","uri":"/posts/cnns/"},{"categories":["Tutorials"],"content":"Coletando os dados from torchvision import datasets, transforms import torch def get_mnist_dataset( train_transformers=transforms.ToTensor(), test_transformers=transforms.ToTensor() ) -\u003e Dataset: mnist_train_dataset = datasets.MNIST( root=\"./data\", train=True, download=True, transform=train_transformers ) mnist_test_dataset = datasets.MNIST( root=\"./data\", train=False, download=True, transform=test_transformers ) return mnist_train_dataset, mnist_test_dataset def get_mnist_dataloaders(batch_size: int) -\u003e Dataset: mnist_train_dataset = datasets.MNIST( root=\"./data\", train=True, download=True, transform=transforms.ToTensor() ) mnist_test_dataset = datasets.MNIST( root=\"./data\", train=False, download=True, transform=transforms.ToTensor() ) return mnist_train_dataset, mnist_test_dataset Eu vou criar tamb√©m uma fun√ß√£o para nos permitir visualizar uma imagem desse conjunto de dados, utilizando para isso o matplotlib. def plot_dataset_image(idx: int, dataset: Dataset): image, label = dataset[idx] image_reshaped = image.reshape(28, 28) plt.imshow(image_reshaped, cmap=\"gray\") plt.title(label) plt.show() plot_dataset_image(4, mnist_train_dataset) Um breve disclaimer Quando retornamos a imgem direto do Dataset obtemos um tensor no formato: (C, W, H) Por√©m, o matplotlib espera que se a imagem for colorida (RBG), ela tenha o seguinte formato: (W, H, C) Ent√£o para facilitar, e como essas imagem j√° vem com apenas 1 channel, n√≥s simplemente estamos considerando as dimens√µes W, H. Por isso voc√™ vai ver o m√©todo de reshape na fun√ß√£o plot_dataset_image. ","date":"2023-07-06","objectID":"/posts/cnns/:4:1","tags":["AI","Generative","IA Generativa"],"title":"CNNs","uri":"/posts/cnns/"},{"categories":["Tutorials"],"content":"Transforma√ß√µes Vamos vizualizar 4 transforma√ß√µes nos dados que podem ser feitas aplicando diretamente a opera√ß√£o de convolu√ß√£o: Desfoque gaussiano Detec√ß√£o de contornos Detec√ß√£o de linhas horizontais Detec√ß√£o de linhas verticais O ponto comum entre todas opera√ß√µes, √© a necessidade de configura√ß√£o de um kernel. Veja abaixo o kernel que gera um determinado resultado. No Pytorch, uma forma de replicar essa opera√ß√£o √© aplicando um m√©todo de opera√ß√£o convolucional: def blurring_image(image: torch.Tensor, kernel_size: int = 3) -\u003e torch.Tensor: kernel = torch.ones(kernel_size, kernel_size) / kernel_size**2 return F.conv2d(image.unsqueeze(0), kernel.unsqueeze(0).unsqueeze(0)) # calling image, _ = mnist_train_dataset[90] new_image = blurring_image(image) Voc√™ pode encontrar todas as outras opera√ß√µes nesse notebook. E abaixo temos o resultado das 4 transforma√ß√µes que foram citadas acima. Um breve disclaimer Voc√™ vai ver algumas opera√ß·∫Ωos de unsqueeze(0) nesses c√≥digos de transforma√ß√£o das imagens. Isso √© apenas para ajustar as dimens√µes durante as manipula√ß√µes. Esse m√©todo basicamente cria uma nova dimens√£o mantendo o shape do formato inicial: ","date":"2023-07-06","objectID":"/posts/cnns/:4:2","tags":["AI","Generative","IA Generativa"],"title":"CNNs","uri":"/posts/cnns/"},{"categories":["Tutorials"],"content":"Nossa primeira CNN üî• (sem pooling) Bom, agora que n√≥s temos os dados, vamo realizar o treinamento da nossa primeira CNN. Eu vou utilizar aqui tr√™s fun√ß√µes de ajuda do livro Inside Deep Learning, onde essas fun√ß√µes cont√™m alguns c√≥digos que normalmente se repetem para todo tipo de treinamento de modelo de Deep Learning. O que essas fun√ß√µes fazem? movem um objeto em python para o device correto loop de treino salvam alguns dados durante o treinamento: modelo, m√©tricas de performance, etc. Refer√™ncia As fun√ß√µes podem ser acessadas nesse reposit√≥rio, s√£o elas: moveto run_epoch simple_training_loop. Agora, vamos criar o c√≥digo do nosso modelo, passando tamb√©m algumas constantes: B = 32 # batch size D = 28 * 28 # image dimensionality C = 1 # number of channels classes = 10 filters = 16 kernel_size = 3 fc_model = nn.Sequential( nn.Flatten(), # (B, C, W, H) -\u003e (B, C * W * H) = (B, D) nn.Linear(D, 256), nn.Tanh(), nn.Linear(256, classes), ) model_conv = nn.Sequential( nn.Conv2d(C, filters, kernel_size, padding=kernel_size // 2), nn.Tanh(), nn.Flatten(), nn.Linear(filters * D, classes), ) Apenas a n√≠vel de compara√ß√£o, vamos treinar os dois modelos (fully-connected e CNN): loss_func = nn.CrossEntropyLoss() cnn_results = train_simple_network( model_conv, loss_func, train_loader, test_loader, score_funcs={\"accuracy\": accuracy_score}, device=device, epochs=100, checkpoint_file=\"./model/cnn_checkpoint.pth\", ) loss_func = nn.CrossEntropyLoss() fc_results = train_simple_network( fc_model, loss_func, train_loader, test_loader, score_funcs={\"accuracy\": accuracy_score}, device=device, epochs=100, checkpoint_file=\"./model/cnn_checkpoint.pth\", ) Caso esteja demorando muito o treinamento para 100 √©pocas, voc√™ pode diminuir o n√∫mero de √©pocas. Mas, como resultado n√≥s teremos algo semelhante ao do plot abaixo: Aviso O c√≥digo para criar um plot semelhante a esse, pode ser encontrado tamb√©m no reposit√≥rio que estamos utilizando para a s√©ries de blog posts. Apesar de ter obtido bons resultados, nosso modelo ainda √© bem sens√≠vel h√° algumas mudan√ßas no comportamento dos dados, como por exemplo, sens√≠vel a transla√ß√£o do objeto que estamos classificando (n√∫meros). Os n√∫meros do dataset MNIST que estamos tentando prever est√£o todos centralizados, e com isso se algum n√∫mero mudar de posi√ß√£o, provavelmente vai levar a uma varia√ß√£o na acur√°cia do modelo. ","date":"2023-07-06","objectID":"/posts/cnns/:5:0","tags":["AI","Generative","IA Generativa"],"title":"CNNs","uri":"/posts/cnns/"},{"categories":["Tutorials"],"content":"Verificando a movimenta√ß√£o do objeto Vamos utilizar um m√©todo do numpy que faz essencialmente isso, o np.roll: img_idx = 10 img, correct_class = mnist_train_dataset[img_idx] img = img.reshape(28, 28) img_lr = np.roll(np.roll(img, 4, axis=1), 1, axis=0) img_ul = np.roll(np.roll(img, -4, axis=1), -1, axis=0) Ent√£o, o que est√° acontecendo aqui √© o descolando da imagem em uma duas dire√ß√µes distintas: img_lr: lower right img_ul: upper left Tip Para ficar mais claro o que o np.roll faz, eu sempre gosto de rodar o m√©todo em um peda√ßo de c√≥digo mais simples, nesse caso, em um array unidimensional, temos: x = np.arange(10) # [0, 1, 2, 3, 4, 5, 6, 7, 8, 9] np.roll(x, 2) Agora, vamos ver ent√£o como ficaram todas as movimenta√ß√µes em compara√ß√£o com a imagem original: f, axarr = plt.subplots(1, 3) axarr[0].imshow(img, cmap=\"gray\") axarr[1].imshow(img_lr, cmap=\"gray\") axarr[2].imshow(img_ul, cmap=\"gray\") plt.show() Veja que nossa imagem j√° foi movimentada, vamos investigar a acur√°cia em cada uma das situa√ß√µes: # creating a prediction function model = model_conv.cpu().eval() # passing to cpu and eval mode to be simpler def pred(model, img): with torch.no_grad(): w, h = img.shape if not isinstance(img, torch.Tensor): img = torch.tensor(img) # We need add some dimensions to the image so that it is in the correct shape x = img.reshape(1, -1, w, h) logits = model(x) # We need to apply softmax to get the probabilities y_hat = F.softmax(logits, dim=1) return y_hat.numpy().flatten() # calling img_lr_pred = pred(model, img_lr) img_pred = pred(model, img) img_ul_pred = pred(model, img_ul) print( f\"Predicted class for original image: {np.argmax(img_pred)} / Probability: {img_pred[np.argmax(img_pred)]}\" ) print( f\"Predicted class for left-right image: {np.argmax(img_lr_pred)} / Probability: {img_lr_pred[np.argmax(img_lr_pred)]}\" ) print( f\"Predicted class for up-left image: {np.argmax(img_ul_pred)} / Probability: {img_ul_pred[np.argmax(img_ul_pred)]}\" ) imagem normal: 0.99 lower right: 0.73 upper left: 0.61 Para que nosso modelo possa ser mais robusto em rela√ß√£o a transla√ß√£o do objeto, vamos tentar aplicar um pooling na nossa arquitetura. ","date":"2023-07-06","objectID":"/posts/cnns/:5:1","tags":["AI","Generative","IA Generativa"],"title":"CNNs","uri":"/posts/cnns/"},{"categories":["Tutorials"],"content":"Nossa segunda CNN üî• (com pooling) O pooling nada mais √© que uma t√©cnica que nos ajuda a ter uma propriedade chamada translation invariance, ou seja, o aprendizado do nosso modelo fica menos suscet√≠vel a movimenta√ß√£o do objeto de interesse. De forma pr√°tica, aqui n√≥s vamo estar utilizando o max pooling, onde √© utilizado o valor m√°ximo da matriz da imagem de input na camada de pooling. Pela imagem abaixo fica mais claro o que estou querendo dizer: A partir do momento que aumentamos o K (input do m√©todo), acabamos tendo uma janela maior para retornar o valor m√°ximo. Um ponto importante √© que o pooling diminui o tamanho da imagem, ent√£o precisamos de cuidado pois ao final podemos machucar muito a representa√ß√£o do nosso problema, tendo uma perda muito grande de informa√ß√£o √∫til. Nossa CNN com pooling e mais camadas, fica assim: model_cnn_pool = nn.Sequential( nn.Conv2d(C, filters, 3, padding=3 // 2), nn.Tanh(), nn.Conv2d(filters, filters, 3, padding=3 // 2), nn.Tanh(), nn.Conv2d(filters, filters, 3, padding=3 // 2), nn.Tanh(), nn.MaxPool2d(2), nn.Conv2d(filters, 2 * filters, 3, padding=3 // 2), nn.Tanh(), nn.Conv2d(2 * filters, 2 * filters, 3, padding=3 // 2), nn.Tanh(), nn.Conv2d(2 * filters, 2 * filters, 3, padding=3 // 2), nn.Tanh(), nn.MaxPool2d(2), nn.Flatten(), nn.Linear(2 * filters * D // (4**2), classes), ) # training cnn_results_with_pool = train_simple_network( model_cnn_pool, loss_func, train_loader, test_loader, score_funcs={\"accuracy\": accuracy_score}, device=device, epochs=100, checkpoint_file=\"./model/cnn_pooling_checkpoint.pth\", ) Nesse caso para a mesma imagem que foi transladada em diferentes posi√ß√µes, n√≥s tivemos os seguintes scores: imagem normal: 0.99 lower right: 0.94 upper left: 0.73 ","date":"2023-07-06","objectID":"/posts/cnns/:6:0","tags":["AI","Generative","IA Generativa"],"title":"CNNs","uri":"/posts/cnns/"},{"categories":["Tutorials"],"content":"Nossa terceira CNN üî• (com data augmentation) Por fim, uma √∫ltima estrat√©gia que gostaria de trazer aqui √© o Data Augmentation. E a ideia por de tr√°s √© basicamente a de expor o seu modelo ao maior n√∫mero poss√≠vel de varientes da imagem original, para que ele consigar ter um √≥timo grau de generaliza√ß√£o. A sacada √© que voc√™ n√£o precisa necessariamente coletar novos dados, e sim manipular uma imagem para criar diferentes situa√ß√µes para ela: rotacionar ditorcer alterar brilho e contraste alterar cores do canal rgb aplica√ß√£o de ru√≠do A lista √© imensa e o torchvision tem um m√≥dulo que nos ajuda nisso, chamado transforms. Voc√™ tamb√©m pode encontrar m√≥dulos de terceiros como o Albumentation, mas acho que independente da lib que voc√™ estiver utilizando, o importante mesmo vai ser escolher a transforma√ß√£o que faz sentido para o seu dado. Digo isso por qu√™ voc√™ pode aplicar uma transforma√ß√£o de brilho e contraste numa image, e o resultado ser uma imagem completamente ileg√≠vel, que dificilmente um humano vai conseguir distinguir o que √©. Essas transforma√ß√µes precisam ser aplicadas considerando o ‚Äúuniverso‚Äù de possibilidades de transforma√ß√µes que essas imagens podem sofrer. Vamos visualizar como essas transforma√ß√µes se comportam no torchvision e em seguida vamos treinar nossa CNN, por√©m agora passando algumas transforma√ß√µes ao dado. ","date":"2023-07-06","objectID":"/posts/cnns/:7:0","tags":["AI","Generative","IA Generativa"],"title":"CNNs","uri":"/posts/cnns/"},{"categories":["Tutorials"],"content":"Transforms do torchvision O c√≥digo abaixo aplica algumas transforma√ß√µes ao nosso dataset: sample_transforms = { \"Rotation\": transforms.RandomAffine(degrees=45), \"Translation\": transforms.RandomAffine(0, translate=(0.1, 0.1)), \"Shear\": transforms.RandomAffine(0, shear=45), \"RandomCrop\": transforms.RandomCrop((20, 20)), \"Horizontal Flip\": transforms.RandomHorizontalFlip(p=1.0), \"Vertical Flip\": transforms.RandomVerticalFlip(p=1.0), \"Color Jitter\": transforms.ColorJitter( brightness=0.5, contrast=0.5, saturation=0.5, hue=0.5 ), \"Perspective\": transforms.RandomPerspective(p=1.0), } pil_img = transforms.ToPILImage()(img) f, axarr = plt.subplots(2, 4, figsize=(15, 10)) for i, (name, transform) in enumerate(sample_transforms.items()): pil_img_transformed = transform(pil_img) axarr[i // 4, i % 4].imshow(pil_img_transformed, cmap=\"gray\") axarr[i // 4, i % 4].set_title(name) plt.show() ","date":"2023-07-06","objectID":"/posts/cnns/:7:1","tags":["AI","Generative","IA Generativa"],"title":"CNNs","uri":"/posts/cnns/"},{"categories":["Tutorials"],"content":"Treinando Vamos ao que interessa! Como transforms, eu vou utilizar varia√ß√£o de rota√ß√£o, transla√ß√£o e escala. Ao final disso, vamos converter a imagem para tensor. train_transform = transforms.Compose( [ transforms.RandomAffine(degrees=5, translate=(0.05, 0.05), scale=(0.95, 1.05)), transforms.ToTensor(), ] ) test_transform = transforms.ToTensor() mnist_train_dataset_v2 = MNIST( root=\"./data\", train=True, download=True, transform=train_transform ) mnist_test_dataset_v2 = MNIST( root=\"./data\", train=False, download=True, transform=test_transform ) train_loader_v2 = torch.utils.data.DataLoader( mnist_train_dataset_v2, batch_size=B, shuffle=True ) test_loader_v2 = torch.utils.data.DataLoader( mnist_test_dataset_v2, batch_size=B, shuffle=False ) Como a arquitetura do modelo √© a mesma, n√£o vou repetir esse c√≥digo aqui. Mas, ainda assim voc√™ precisa reeinstanciar o objeto, para que o Pytorch n√£o utilize os pesos que j√° fora oriundos do treinamento passado. A √∫nica diferen√ßa √© que agora voc√™ precisa passar o train_loader_v2 e test_loader_v2 para a fun√ß√£o train_simple_network. Dito isso, nossos resultados foram: imagem normal: 0.99 lower right: 0.99 upper left: 0.53 Olha que interessante, n√≥s conseguimos melhorar a acur√°cia para a previs√£o do lower right, por√©m, para o upper left acabamos piorando a situa√ß√£o. Isso pode ter acontecido por conta da transforma√ß√£o que foi aplicada na imagem durante o treinamento do modelo n√£o refletir a situa√ß√£o da imagem que a gente gerou, manualmente, de transla√ß√£o. ","date":"2023-07-06","objectID":"/posts/cnns/:7:2","tags":["AI","Generative","IA Generativa"],"title":"CNNs","uri":"/posts/cnns/"},{"categories":["Tutorials"],"content":"Modelos pr√©-treinados Espero que tenha consigo passar para voc√™ os principais conceitos relacionados as famosas CNNs e como codar o treinamento dessas arquiteturas. Apesar de conseguirmos ter resultados interessantes compondo o b√°sico de uma arquitetura de CNN, esses resultados s√£o ainda melhores quando utilizamos os modelos pr√©-treinados de CNNs mais complexas. N√£o vamos entrar aqui no detalhe dessas arquiteturas mais robustas, mas vou deixar aqui um comando que voc√™ pode rodar para ter acesso h√° todos os modelo dispon√≠veis no torchvision: dir(torchvision.models) Isso vai te retornar uma lista imensa com todos os modelos geridos pelo hub do torchvision. Por hoje foi isso, espero que tenha gostado, e no pr√≥ximo artigo vamos falar sobre autoencoders!! ","date":"2023-07-06","objectID":"/posts/cnns/:8:0","tags":["AI","Generative","IA Generativa"],"title":"CNNs","uri":"/posts/cnns/"},{"categories":["Tutorials"],"content":"Nesse artigo, vamos de m√£o na massa! Mas gostaria de fazer um disclaimer um pouco chato pra voc√™, vamos ver tudo de forma superficial, cada t√≥pico abordado aqui por si s√≥ precisaria de muitas p√°ginas de explica√ß√£o, ent√£o, vou fazer o melhor para a explica√ß√£o n√£o se tornar um Frankstein e o post virar uma cocha de retalhos. Meu papel aqui √© trazer de forma objetiva cada t√≥pico desse para que voc√™ consiga correlacionar depois com o avan√ßar dos cap√≠tulos do livro que estou trazendo os reviews. E se voc√™ chegou apenas para esse artigo e n√£o sabe do contexto, eu na verdade estou trazendo uma s√©rie de blog posts sobre minhas anota√ß√µes sobre o livro Generative Deep Learning. E, j√° rolaram dois posts at√© ent√£o: parte 1 parte 2 Voc√™ pode acompanhar na lateral (üëâ) o TOC do post e pular para parte que mais te interessa üòâ. Vamos l√°!!! ","date":"2023-07-06","objectID":"/posts/generative_ai_notes_03/:0:0","tags":["AI","Generative","IA Generativa"],"title":"Intro ao Pytorch","uri":"/posts/generative_ai_notes_03/"},{"categories":["Tutorials"],"content":"Tudo come√ßa com tensores‚Ä¶ De forma simples, tensores s√£o uma forma ‚Äòfancy‚Äô de se representar arrays multidimensionais. Voc√™ muito provavelmente j√° est√° acostumado a trabalhar com numpy arrays, por√©m apesar de terem um comportamento parecido, as implementa√ß√µes de tensores te fornecem n√£o s√≥ uma s√©rie de outras opera√ß√µes matem√°ticas e otimiza√ß√µes, mas tamb√©m a capacidade de rodar tudo isso em gpus ou tpus, o que basicamente torna o avan√ßo de Deep Learning poss√≠vel (do ponto de vista de for√ßa bruta para treinar os modelos). Nota Vale falar que os tensores n√£o s√£o exclusividade do Pytorch ta? O TENSORFlow leva inclusive o termo escrito bem no nome do framework. Definir um tensor no Pytorch √© bem simples: x = torch.tensor([1, 2, 3], dtype=torch.Float16) Devido a capacidade de rodar em diferentes dispositivos, como gpus, cpus e tpus, a biblioteca do Pytorch te possibilita migrar esse dado entre dispositivos, te dando total liberdade de como voc√™ vai executar o treinamento. Liberdade essa que se estende para multi-gpus, multi-tpus‚Ä¶ # simples assim voc√™ leva um tensor para diferentes dispositivos x.to(\"cuda\") Para enviar e armazenar esse tensor na mem√≥ria da GPU, utilizamos o nome cuda. E, para CPU‚Ä¶ √â cpu mesmo. O nome cuda, para quem nunca ouviu falar, vem de Compute Unified Device Architecture e foi desenvolvido pela NVidia para permitir as implementa√ß√µes de processamento paralelo utilizando placas de v√≠deo. Provavelmente por uma decis√£o de projeto, o nome cuda permanece sendo utilizado no Pytorch at√© hoje, mesmo n√£o sendo o mais intuitivo (pelo menos na minha humilde opni√£o), mas, bibliotecas mais recentes e que rodam sobre o Pytorch, como pytorch-lightining utiliza gpu para indicar que seu treinamento vai ser executado na GPU. Nada mais intuitivo, concorda? Com grandes poderes, vem grandes responsabilidades!! E a configura√ß√£o do device hoje gera alguns dos erros mais comuns quando estamos trabalhando com o Pytorch. Como √© bastante comum estarmos rodando o treinamento utilizando GPUs, para que tudo funcione, todos objetos que voc√™ trabalha precisam estar alocados na mem√≥ria de apenas um dispositivo. modelo input labels pesos etc Sendo n√≥s os respons√°veis por levar esse dado para o lugar certo, muitas vezes acabamos esquecendo de fazer isso, e o resultado √© uma bela mensagem de erro. Perigo Tente executar o c√≥digo abaixo, para voc√™ tamb√©m entrar para a estat√≠stica: x = torch.tensor([1, 2, 3]).to('cuda') y = torch.tensor([2, 3, 4]).to('cpu') print(x + y) Gostaria de fazer uma men√ß√£o honrosa aqui a uma fun√ß√£o que eu de fato n√£o sei se est√° sendo muito utilizada por quem atua diretamente com o desenvolvimento de arquiteturas de Deep Learning, mas que quando eu vi eu achei super interessante, que s√£o os chamados named_tensors. Basicamente voc√™ pode adicionar um nome para as dimens√µes dos tensores que voc√™ est√° trabalhando, o que possibilita um debug mais f√°cil quando algum problema acontece, e tamb√©m algumas opera√ß√µes podem ser feitas por esses nomes. Vou deixar ao fim do post um link para documenta√ß√£o com uma explica√ß√£o mais profunda e com alguns exemplos. Temos tamb√©m uma pohaaada de opera√ß√µes que s√£o poss√≠veis de realizar com tensores, mas devido nossa abordagem aqui, vamose v√™-las a medida que fomos utilizando. ","date":"2023-07-06","objectID":"/posts/generative_ai_notes_03/:1:0","tags":["AI","Generative","IA Generativa"],"title":"Intro ao Pytorch","uri":"/posts/generative_ai_notes_03/"},{"categories":["Tutorials"],"content":"Por debaixo do cap√¥‚Ä¶ A esse ponto, eu queria que voc√™ tivesse um modelo mental de que um treinamento de uma rede neural √© um encadeamento de opera√ß√µes, arranjadas de certa forma que n√≥s conseguimos atualizar pesos e par√¢metros que ir√£o no final cuminar em um modelo. Esse conjunto de opera√ß√µes, e as formas como eles se conectam, s√£o criados no momento de execu√ß√£o e ‚Äúarmazenados‚Äù pelo Pytorch. Essa organiza√ß√£o √© feita em grafos que podem ser representados como na imagem abaixo. O Autograd √© um m√≥dulo do Pytorch que permite o c√°lculo do gradiente, de forma perform√°tica e de forma completamente abstra√≠da para n√≥s, usu√°rios do framework. Voc√™ pode, tamb√©m ir bem deep no entendimento dos detalhes internos do Pytorch, deixarei um post no fim do artigo para isso. ","date":"2023-07-06","objectID":"/posts/generative_ai_notes_03/:2:0","tags":["AI","Generative","IA Generativa"],"title":"Intro ao Pytorch","uri":"/posts/generative_ai_notes_03/"},{"categories":["Tutorials"],"content":"Diferen√ß√£o autom√°tica Blz!! Temos v√°rias opera√ß√µes para realizar, e consequentemente v√°rias derivadas para calcular, ser√° que precisamos fazer isso na m√£o?? De forma alguma‚Ä¶ √â ent√£o que a diferencia√ß√£o autom√°tica entra em nossas vidas, essa feature do Pytorch permite que o framework consiga calcular o gradiente ao longo de toda a cadeia de opera√ß√µes realizadas pela sua rede neural, em rela√ß√£o a vari√°veis que voc√™ indica pra ele. Similar √† imagem abaixo: Essa indica√ß√£o das vari√°veis que ser√£o consideradas na hora do c√°lculo do gradiente √© feita pelo par√¢metro require_grad=True. Dessa forma o Pytorch vai armazenar o valor do gradiente em uma propriedade chamada .grad. ","date":"2023-07-06","objectID":"/posts/generative_ai_notes_03/:2:1","tags":["AI","Generative","IA Generativa"],"title":"Intro ao Pytorch","uri":"/posts/generative_ai_notes_03/"},{"categories":["Tutorials"],"content":"Optimizers No artigo passado n√≥s falamos sobre minimiza√ß√£o, esse processo que acaba sendo chamado de otimiza√ß√£o. Justamente por esse motivo, o Pytorch criou uma abstra√ß√£o chamada, adivinha o nome?, optimizers. Nesse m√≥dulo voc√™ vai encontrar diversos m√©todos de otimiza√ß√£o, entre eles, um dos mais comuns, chamado de Stochastic Gradient Descent (SGB). A imagem abaixo mostra como o processo de otimiza√ß√£o acontece: Ent√£o, relembrando, n√≥s temos uma loss, n√≥s precisamos minimizar, esse processo se chama otimiza√ß√£o, e minimizar essa fun√ß√£o implica que os pesos ao longo da arquitetura da rede neural sejam atualizados. Em c√≥digo, veja abaixo como esses passos se desenrolam: # learning rate eta = 0.1 # vari√°vel que o modelo vai considerar na hora de minimizar a fun√ß√£o x_param = torch.nn.Parameter(torch.tensor([-3.5]), requires_grad=True) # escolha do optimizer a ser utilizado optimizer = torch.optim.SGD([x_param], lr=eta) # as √©pocas s√£o como n√≥s nomeamos as itera√ß√µes for epoch in range(200): # como a cada itera√ß√£o o torch mant√©m os valores antigos do gradiente # o zero_grad() √© justamente para zerar esse dados optimizer.zero_grad() loss_incurred = f(x_param) # fazemos o c√°lculo loss_incurred.backward() # atualizamos os pesos para pr√≥xima itera√ß√£o optimizer.step() print(x_param.data) tensor([2.0000]) Nosso resultado aqui √© o mesmo do mostrado no artigo passado, s√≥ que dessa vez n√≥s realizamos o processo de forma iterativa. Guarda esse processo, mais abaixo n√≥s tamb√©m vamos utiliz√°-lo para o treinamento da nossa primeira rede neural. ","date":"2023-07-06","objectID":"/posts/generative_ai_notes_03/:2:2","tags":["AI","Generative","IA Generativa"],"title":"Intro ao Pytorch","uri":"/posts/generative_ai_notes_03/"},{"categories":["Tutorials"],"content":"Primeira Rede Neural Um insight muito massa que eu tive ao ler o livro Inside Deep Learning, √© que o Pytorch foi constru√≠do com uma premissa bem forte de que todo treinamento de uma rede neural √© na verdade um problema de otimiza√ß√£o. Ent√£o, independentemente do problema que estamos atacando (classifica√ß√£o, regress√£o‚Ä¶), temos que pensar o problema como um problema de otimiza√ß√£o. E de fato isso faz muito sentido, dado que todos os pesos dos modelos s√£o alterados com base no processo de minimiza√ß√£o de uma fun√ß√£o, a loss. No processo de treinamento de uma rede neural fica ent√£o evidenciado um padr√£o. N√≥s teremos sempre dados que ir√£o alimentar o modelo, teremos o modelo (nossa arquitetura) e a loss que vai alterar a depender do tipo de task que estaremos atacando. A seguinte imagem traduz muito bem o processo: Vamos ent√£o codar pedacinho desse e ver como desenrola na pr√°tica!! ","date":"2023-07-06","objectID":"/posts/generative_ai_notes_03/:3:0","tags":["AI","Generative","IA Generativa"],"title":"Intro ao Pytorch","uri":"/posts/generative_ai_notes_03/"},{"categories":["Tutorials"],"content":"Training Loop Vimos acima, com um exemplo mais simples, que esse √© um processo iterativo. A implementa√ß√£o que voc√™ v√™ abaixo, √© uma adapta√ß√£o do anterior para contemplar uma situa√ß√£o real de treinamento de uma rede neural. # apenas para ter um typehint Loss = Callable[[torch.Tensor, torch.Tensor], torch.Tensor] def train_simple_network( model: nn.Module, loss_func: Loss, training_loader: DataLoader, epochs: int = 100, device: str = \"cuda\", ) -\u003e None: # 1 optimizer = torch.optim.SGD(model.parameters(), lr=0.001) model.to(device) for epoch in tqdm(range(epochs), desc=\"Epochs\"): # 2 model = model.train() running_loss = 0.0 for input, labels in tqdm(training_loader, desc=\"Training\"): # 3 input = input.to(device) labels = labels.to(device) # 4 optimizer.zero_grad() # 5 output = model(input) loss = loss_func(output, labels) # 6 loss.backward() optimizer.step() running_loss += loss.item() Nesse fluxo o que est√° acontecendo √© o seguinte: Iniciamos o optimizer e enviamos o modelo para o device correto Colocamos o modelo em modo de treino, indicando para o Pytorch que eu quero atualizar os pesos Colocamos os dados para o device correto Muito importante, zeramos o gradiente Fazemos o ‚Äúpredict‚Äù e avaliamos o qu√£o distante estamos do valor real, utilizando a loss para isso Calculamos o gradiente e enfim atualizamos os pesos ","date":"2023-07-06","objectID":"/posts/generative_ai_notes_03/:3:1","tags":["AI","Generative","IA Generativa"],"title":"Intro ao Pytorch","uri":"/posts/generative_ai_notes_03/"},{"categories":["Tutorials"],"content":"Data Como vamos treinar para uma task de regress√£o, vamo gerar aqui alguns dados sint√©ticos com aux√≠lio do numpy e vamos tamb√©m visualizar o resultado. X = np.linspace(0, 20, num=200) y = X + np.sin(X) * 2 + np.random.normal(size=X.shape) sns.scatterplot(x=X, y=y) Como foi dito no √∫ltimo artigo, o Pytorch trabalha com duas abstra√ß√µes chamadas de Dataset e DataLoader. Elas s√£o respons√°veis por alimentar seu treinamento com os dados, fazendo isso de forma bem perform√°tica. As imagens abaixo ilustram muito bem o papel de cada um: Na primeira, o que a gente v√™ √© o Dataset sendo o respons√°vel por ir no nosso dado e selecionar um item. Por isso, dois m√©todos s√£o obrigat√≥rios quando estamos implementando o Dataset: __len__: vai nos dizer o tamanho do dataset __getitem__: vai coletar um item do dataset Na segunda imagem, voc√™ v√™ a atua√ß√£o do DataLoader, que tem o objetivo de pedir ao Dataset por espec√≠ficos items. Como n√≥s, durante o treinamento, passamos os dados em lote e embaralhados, os √≠ndices que est√£o sendo pedidos ao Dataset acabam n√£o tendo uma ordem. Do ponto de vista de implementa√ß√£o, basicamente o m√©todo __getitem__ precisa retornar uma tupla com o item + label, seja os tensores das imagens, de texto, som‚Ä¶ E o seu trabalho √© basicamente adaptar o dado bruto para essa estrutura. Em alguns casos, o Pytorch facilita esse trabalho e n√≥s n√£o precisamos codar uma classe Dataset customizada, como por exemplo quando trabalhamos com imagens. Veremos mais detalhes sobre, em pr√≥ximos artigos. Certo, eis aqui nosso Dataset e DataLoader: class SimpleRegressionDataset(Dataset): def __init__(self, X: torch.Tensor, y: torch.Tensor) -\u003e None: super().__init__() self.X = X.reshape(-1, 1) self.y = y.reshape(-1, 1) def __len__(self) -\u003e int: return self.X.shape[0] def __getitem__(self, idx: int) -\u003e tuple[torch.Tensor, torch.Tensor]: X = torch.tensor(self.X[idx, :], dtype=torch.float32) y = torch.tensor(self.y[idx, :], dtype=torch.float32) return X, y training_dataset = SimpleRegressionDataset(X, y) training_loader = DataLoader(training_dataset, shuffle=True) ","date":"2023-07-06","objectID":"/posts/generative_ai_notes_03/:3:2","tags":["AI","Generative","IA Generativa"],"title":"Intro ao Pytorch","uri":"/posts/generative_ai_notes_03/"},{"categories":["Tutorials"],"content":"Model e Loss Basicamente voc√™ pode criar uma arquitetura (modelo) de Deep Learning de duas formas. Respeitando a orienta√ß√£o a objeto ou pelo paradigma funcional. Por OOP n√≥s criamos uma classe e herdamos do Pytorch a classe Module e obrigatoriamente precisamos implementar o m√©todo forward. Vamos simplificar aqui e criar nosso modelo utilizando o paradigma funcional, que ficaria assim: simple_model = nn.Sequential(nn.Linear(1, 10), nn.Linear(10, 1)) Pronto, temos nosso primeiro modelo üéâ, que de forma visual, seria algo como na seguinte imagem, lendo debaixo para cima: E ent√£o, nossa loss aqui vai ser a MSE (Mean Squared Error): loss_func = nn.MSELoss() ","date":"2023-07-06","objectID":"/posts/generative_ai_notes_03/:3:3","tags":["AI","Generative","IA Generativa"],"title":"Intro ao Pytorch","uri":"/posts/generative_ai_notes_03/"},{"categories":["Tutorials"],"content":"Juntando o quebra-cabe√ßa ‚ú® Just run‚Ä¶ device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") train_simple_network(model, loss_func, training_loader, device=device, epochs=1000) Nota √â comum voc√™ encontrar esse condicional para buscar por cuda caso ela esteja dispon√≠vel no seu computador, caso contr√°rio use cpu. Caso esteja confort√°vel em sempre utilizar a GPU, pode remover e apenas deixar cuda. Avaliando nossos resultados: with torch.inference_mode(): Y_pred = ( model(torch.tensor(X.reshape(-1, 1), device=device, dtype=torch.float32)) .cpu() .numpy() ) sns.scatterplot(x=X, y=y) sns.lineplot(x=X, y=Y_pred.ravel(), color=\"red\") A linha reta em vermelho aqui s√£o nossas previs√µes. Mas, por que ser√° que o modelo n√£o conseguiu capturar a n√£o lineariedade dos dados? Isso acontece basicamente por que estamos concatenando opera√ß√µes lineares uma atr√°s da outra. E nesse caso, no final, se voc√™ utilizasse 1000 camadas no nn.Sequential esse modelo n√£o conseguiria capturar esse perfil n√£o linear dos dados. ","date":"2023-07-06","objectID":"/posts/generative_ai_notes_03/:3:4","tags":["AI","Generative","IA Generativa"],"title":"Intro ao Pytorch","uri":"/posts/generative_ai_notes_03/"},{"categories":["Tutorials"],"content":"Fun√ß√µes de ativa√ß√£o ao resgate Para resolver esse problema, n√≥s adicionamos uma perturba√ß√£o nas camadas internas da rede neural, que auxiliam o modelo a representar n√£o lineariedades. Vou deixar uma imagem aqui com algumas fun√ß√µes de ativa√ß√£o, e em seguimos vamos reimplementar o c√≥digo, usando a Tanh(). model = nn.Sequential(nn.Linear(1, 10), nn.Tanh(), nn.Linear(10, 1)) E ent√£o, rodamos novamente o treinamento: train_simple_network(model, loss_func, training_loader, device=device, epochs=1000) Agora, como vemos na figura, foi poss√≠vel capturar o formato n√£o linear dos dados. Show, para esse artigo era isso, espero que tenha conseguido deixar um pouco mais claro quais s√£o as principais pe√ßas na hora de montar esse puzzle do Deep Learning. Agora, n√≥s iremos come√ßar a entrar mais nas particularidades das diferentes arquiteturas, come√ßando por CNNs, at√© l√°! ","date":"2023-07-06","objectID":"/posts/generative_ai_notes_03/:3:5","tags":["AI","Generative","IA Generativa"],"title":"Intro ao Pytorch","uri":"/posts/generative_ai_notes_03/"},{"categories":["Tutorials"],"content":"Links √∫teis Books: [Inside Deep Learning, Generative Deep Learning, Deep Learning with Pytorch] Reposit√≥rio: link Named Tensors doc: link Pytorch Internals: link ","date":"2023-07-06","objectID":"/posts/generative_ai_notes_03/:4:0","tags":["AI","Generative","IA Generativa"],"title":"Intro ao Pytorch","uri":"/posts/generative_ai_notes_03/"},{"categories":["Tutorials"],"content":"Chegamos no cap√≠tulo 2 do livro, e o mesmo fala sobre Deep Learning e implementa√ß√µes de CNNs utilizando o Keras como framework base para codar as implementa√ß√µes. Por√©m, as coisas s√£o explicadas de uma forma bem superficial, e dificilmente quem nunca teve contato com esse conte√∫do vai conseguir acompanhar de forma satisfat√≥ria. Pensando nisso, gostaria de fazer algo diferente, pensei em desmembrar o assunto e fazer uma sequ√™ncia de 3 posts para cobrir o cap√≠tulo 2. Esse primeiro vai retratar o que √© Deep Learning, trazendo para voc√™s um pouco do fundamento por de tr√°s, e ao mesmo tempo trazer uma percep√ß√£o de quais s√£o as ‚Äúpecinhas‚Äù que comp√µe esse quebra-cabe√ßa. E sim, eu penso o Deep Learning como um grande Lego que a gente tem montar para que a coisa funcione como esperado. J√° o artigo dois, supondo que voc√™ agora compreende alguns conceitos importantes, vamos ser respons√°veis por implementar um MultiLayer Perceptron utilizando o pytorch-lightning. Aqui tamb√©m iremos trazer mais alguns conceitos √∫teis para nossa caminhada no mundo da IA Generativa. O artigo tr√™s, finaliza a sequ√™ncia cobrindo CNNs. Acho que assim fica mais interessante de mais pessoas acompanharem os artigos seguintes. Dito isso, vamos l√°!! Nota Vale lembrar que vou utilizar uma outra refer√™ncia para escrever esses tr√™s artigos e voc√™ pode encontrar o nome do livro final do post üòâ. ","date":"2023-06-28","objectID":"/posts/generative_ai_notes_02/:0:0","tags":["AI","Generative","IA Generativa"],"title":"Deep Learning","uri":"/posts/generative_ai_notes_02/"},{"categories":["Tutorials"],"content":"O ‚Äútal‚Äù do Deep Learning O √∫nico pressuposto que vou fazer aqui √© que voc√™ j√° possui um entendimento sobre Machine Learning e sobre algumas nomenclaturas e conceitos da √°rea, dessa forma conseguimos com maior facilidade trazer o Deep Learning para o seu contexto. Vamos come√ßar pelo cl√°ssico diagrama de Venn, batido, mas ainda assim interessante para situar voc√™ de onde o Deep Learning est√°, dentro do universo de IA. Como voc√™ pode ver, o Deep Learning √© um ‚Äúsubset‚Äù do Machine Learning e portanto ambos compartilham do mesmo objetivo, o aprendizado. Por√©m, o diferencial √© que o Deep Learning vai trabalhar exclusivamente com redes neurais artificiais para ‚Äúaprender‚Äù a executar as mais diferentes tasks poss√≠veis: image segmentation object detection text generation Essas redes, s√£o compostas por diferentes arquiteturas que nascem de muita pesquisa e que funcionam muito bem para um contexto espec√≠fico. Esse contexto √© chamado aqui de ‚ÄúConhecimento a Priori‚Äù e basicamente isso quer dizer que n√≥s estamos sempre partindo de um tipo de dado que respeita um algum tipo de estrutura. Se eu to partindo de que meu dado possui algum tipo de estrutura, um questionamento pode surgir na sua cabe√ßa: Como assim estruturado? Eu vejo muitos modelos trabalhando com imagens, textos, v√≠deos‚Ä¶ E s√£o dados desestruturados Justamente, esses dados, por um determinado ponto de vista üëÄ, s√£o na verdade estruturados. Vamos pegar uma imagem como exemplo: (https://www.researchgate.net/figure/mage-of-Abraham-Lincoln-as-a-matrix-of-pixel-values_fig1_330902210) Ela √© nada mais que uma matriz multidimensional (imagens coloridas) de n√∫meros dispostos de uma dada forma que cada valor presente na matriz vai ser representado como um pixel. Esse mesmo comportamento se repete para um v√≠deo ou para um texto. Ent√£o, se eu sei que meu dado tem essa caracter√≠stica, eu posso pensar numa arquitetura que consiga responder algumas perguntas sobre esse tipo de dado. Como por exemplo, se uma imagem pode ser classificada como um gato? Ou, qual √© a probabilidade da pr√≥xima palavra ser ‚Äúquente‚Äù, dado que eu falei ‚Äòcachorro-‚Äô ‚Ä¶? Isso n√£o acontece por exemplo com um dado tabular de previs√£o de Churn de clientes. Vamos pensar no seguinte exemplo, na empresa de Jo√£o n√≥s temos as vari√°veis $x$ e $y$ que explicam muito bem os motivos de Churn dos clientes, e √© poss√≠vel ter uma previs√£o que permite a empresa agir de forma proativa e minimizar esse problema com antecipa√ß√£o üëå. Agora, considerando a empresa de Pedro, as vari√°veis que explicam o mesmo fen√¥meno s√£o $z$ e $p$, logo, n√£o existe um Conhecimento a Priori que poderia ser obedecido, veja que o mesmo problema podem ser explicados por vari√°veis diferentes com pesos diferentes. Esse √© um dos motivos que normalmente voc√™ n√£o v√™ um modelo de Deep Learning performando t√£o bem quanto um Xgboost em competi√ß√µes tabulares no Kaggle. ","date":"2023-06-28","objectID":"/posts/generative_ai_notes_02/:1:0","tags":["AI","Generative","IA Generativa"],"title":"Deep Learning","uri":"/posts/generative_ai_notes_02/"},{"categories":["Tutorials"],"content":"Deep Learning como um carro Uma compara√ß√£o que para mim fez bastante sentido foi a da imagem abaixo. Nela, o autor pensa o treinamento de um modelo de Deep Learning como um funcionamento de um carro. Podemos dizer que o combust√≠vel desse carro √© o Dado, logo, n√≥s iremos precisar de uma forma de carregar esse dado para dentro do nosso motor, no caso do Pytorch, esse seria o papel do DataLoader. O motor, respons√°vel pelo aprendizado, precisa de uma forma de ajustar os par√¢metros e pesos (em conjunto com esse ‚ÄúBox of blocks‚Äù) para levar o nosso modelo para a dire√ß√£o certa. O seu papel aqui √© um pouco diferente de quando voc√™ utiliza por exemplo o scikit-learn para treinar uma Random Forest. Nessa abordagem, voc√™ n√£o precisa codar o modelo e muito menos o processo de treinamento, pois tudo isso foi encapsulado em um m√©todo .fit() e em uma classe RandomForest. Diferentemente, no Deep Learning, voc√™ se torna o respons√°vel por juntar cada pecinha que at√© ent√£o, no Machine Learning ‚Äútradicional‚Äù foi abstra√≠do de voc√™, de uma forma que o aprendizado ocorra da forma esperada. A depender de onde voc√™ trabalhe, codar a arquitetura de um modelo de Deep Learning, ou at√© mesmo criar uma arquitetura nova, n√£o ser√£o atividades do seu dia-a-dia. Mas, ainda assim, restam v√°rias pecinhas que voc√™ precisa conectar, afim de realizar um treinamento de Deep Learning corretamente. E ainda assim, caso voc√™ n√£o seja o respons√°vel por implementar arquiteturas do zero, existem alguns ajustes que voc√™ pode vir a fazer para realizar algum retreino ou fine-tunning de um desses modelos, que conhecer a arquitetura pode vir a ajudar. Um ponto chave para entender o aprendizado em Deep Learning, √© pensar que n√≥s estamos sempre tentando minimizar uma fun√ß√£o, ou seja, n√≥s queremos sempre caminhar para encontrar os pesos e par√¢metros que nos dar√£o os menores valores para essa fun√ß√£o. Essa fun√ß√£o √© comumente chamada de Loss Function. ","date":"2023-06-28","objectID":"/posts/generative_ai_notes_02/:2:0","tags":["AI","Generative","IA Generativa"],"title":"Deep Learning","uri":"/posts/generative_ai_notes_02/"},{"categories":["Tutorials"],"content":"Um pouco de matem√°tica Nas disciplinas de c√°lculo, vimos que quando √© necess√°rio encontrar o m√≠nimo de uma fun√ß√£o, n√≥s precisamos calcular a derivada dessa fun√ß√£o e iguala-la a 0. Nessa situa√ß√£o os valores que a fun√ß√£o vai assumir, ser√£o os valores que estaremos interessados (par√¢metros e pesos). Para ilustrar a situa√ß√£o, vamos pensar num problema simples, uma fun√ß√£o que inclusive conseguiremos visualizar graficamente em duas dimens√µes. $ f(x) = (x-2)^{2} $ Nesse caso, n√≥s queremos saber, qual valor $x$ deve assumir para que eu tenha o menor valor de $f(x)$. Para visualizar essa fun√ß√£o, segue o c√≥digo abaixo: Nota Todo c√≥digo mostrado aqui pode ser encontrado no reposit√≥rio dessa sequ√™ncia de blog posts, ao final do cap√≠tulo voc√™ encontra o link. import seaborn as sns import numpy as np import torch def f(x): return torch.pow((x - 2.0), 2) x_axis_vals = np.linspace(-10, 10, 100) y_axis_vals_p = f(torch.tensor(x_axis_vals)).numpy() sns.lineplot(x=x_axis_vals, y=y_axis_vals_p, label=\"$f(x)=(x-2)^2$\") Ao aplicar regras de c√°lculo de derivada, para essa fun√ß√£o ter√≠amos: $f‚Äô(x) = 2x - 4$ Logo, ao iguala-la a 0: $x = 2$ Esse √© o valor respons√°vel por nos dar o m√≠nimo da fun√ß√£o $f(x)$. Como disse acima, essa √© uma representa√ß√£o simplificada do processo. Normalmente o que acontece √© o c√°lculo de gradiente, que explicando de uma forma chula, √© uma derivada para uma fun√ß√£o que possui mais de uma vari√°vel ($g(x, z)$ por exemplo). Na pr√°tica, n√£o √© um processo simples encontrar o valor exato do m√≠nimo de uma fun√ß√£o, at√© por que devido a complexidade dessas fun√ß√µes elas acabam tendo v√°rios m√≠nimos dispon√≠veis. Justamente por essas quest√µes, que utilizamos um processo iterativo para tentar minimizar o m√°ximo que for poss√≠vel. Nesse processo, utilizamos um outro benef√≠cio do c√°lculo do gradiente, que √© o sinal do resultado. Sempre que √© calculado o gradiente de uma fun√ß√£o, o sinal vai nos indicar para onde devemos caminhar afim de encontrar esse ponto de minimiza√ß√£o da fun√ß√£o, ou seja, para quais valores de pesos e par√¢metros eu preciso atualizar para caminhar para o m√≠nimo global da minha loss function. Na pr√°tica, usamos o conceito de ‚ÄúLearning Rate‚Äù afim de ter uma cad√™ncia de como iremos caminhar nos valores dos nossos par√¢metros, afim de minimizar a fun√ß√£o. Resumindo: temos uma loss para minimizar essa minimiza√ß√£o ocorre atrav√©s da varia√ß√£o de par√¢metros esses par√¢metros s√£o atualizados de acordo uma learning rate sempre investigando qual o sinal do c√°lculo do gradiente O gr√°fico abaixo mostra basicamente qual √© o fluxo. ","date":"2023-06-28","objectID":"/posts/generative_ai_notes_02/:3:0","tags":["AI","Generative","IA Generativa"],"title":"Deep Learning","uri":"/posts/generative_ai_notes_02/"},{"categories":["Tutorials"],"content":"O Pytorch nesse processo Bom, provavelmente voc√™ deve ta imaginando a dor de cabe√ßa que seria implementar toda essa matem√°tica para as mais diversas arquiteturas que existem. N√£o se preocupe, o Pytorch ta aqui pra te auxiliar no processo, trazendo v√°rias abstra√ß√µes muito √∫teis na implementa√ß√£o de arquiteturas, treinamento, fine-tunning‚Ä¶ de modelos de Deep Learning. No ecossistema de frameworks de Deep Learning, o Pytorch seria um ‚Äúconcorrente‚Äù do TensorFlow. Basicamente os dois te permite realizar as mesmas atividades, mas recentemente tem tido uma ado√ß√£o muito maior pela utiliza√ß√£o do Pytorch em detrimento do TensorFlow. Vou deixar abaixo um gr√°fico de tend√™ncia de utiliza√ß√£o de diversos frameworks em publica√ß√£o de papers. Veja como a utiliza√ß√£o do TensorFlow (em laranja) vem caindo ao longo do tempo. Mesmo esses papers sendo lan√ßados primeiramente com implementa√ß√£o em Pytorch, n√£o quer dizer que a comunidade aos poucos n√£o fa√ßa uma implementa√ß√£o para o TensorFlow. Para esse post era isso, espero que tenha curtido o conte√∫do e no pr√≥ximo continuaremos falando do Pytorch e Pytorch-Lightining. ","date":"2023-06-28","objectID":"/posts/generative_ai_notes_02/:4:0","tags":["AI","Generative","IA Generativa"],"title":"Deep Learning","uri":"/posts/generative_ai_notes_02/"},{"categories":["Tutorials"],"content":"Fontes Livro: Inside Deep Learning Trend dos frameworks: https://paperswithcode.com/trends C√≥digo: https://github.com/LucianoBatista/generative-ai ","date":"2023-06-28","objectID":"/posts/generative_ai_notes_02/:5:0","tags":["AI","Generative","IA Generativa"],"title":"Deep Learning","uri":"/posts/generative_ai_notes_02/"},{"categories":null,"content":"üëã Meu nome √© Luciano Oliveira Batista, Engenheiro Qu√≠mico formado pela Universidade Estadual de Santa Cruz (UESC), Cientista de Dados atuando em projetos que utilizam Deep Learning, voltados para resolu√ß√£o de problemas de Vis√£o Computacional e NLP. ‚öóÔ∏è H√° aproximadamente 3 anos me apaixonei pela √°rea de Data Science, e decidi estudar e mergulhar de cabela neste t√≥pico extremamente √∫til e aplic√°vel √†s diversas √°reas de neg√≥cio. Durante esse tempo eu era bolsista de mestrado em Ci√™ncia Inova√ß√£o e Modelagem em Materiais na UESC, e no mesmo per√≠odo eu comecei minha jornada na √°rea de tecnologia. Numa decis√£o um tanto quanto diferente do comum, decidi abandonar e voltar meu foco para o que mais fazia sentido para mim nesse momento, Data Science. üíú Sou apaixonado pelo que fa√ßo, e esse √© um cantinho onde utilizo para compartilhar um pouco do que venho aprendendo, aplicando e praticando. Caso queira entrar em contato comigo, pode me enviar um email, se conectar √† minha network pelo LinkedIn ou pelo Github. ","date":"2023-06-21","objectID":"/about/:0:0","tags":null,"title":"About Me","uri":"/about/"},{"categories":["Tutorials"],"content":"Estou iniciando um s√©rie de posts no blog sobre IA Generativa. O conte√∫do postado aqui ser√£o minhas anota√ß√µes sobre um livro que estou lendo, e a inten√ß√£o √© que a cada cap√≠tulo seja um compilado do que aprendi em jun√ß√£o com algum material extra que venha a consumir. Teremos teoria e c√≥digo, j√° que o livro segue essa linha, que eu particularmente curto bastante. Um adendo √© que o hands-on do livro √© em keras, nada contra, mas eu sou time torch üî• üòÖ. O livro em quest√£o √© o Generative Deep Learning - Teaching Machines to Paint, Write, Compose and Play, estarei utilizando a segunda vers√£o. Al√©m disso, qualquer refer√™ncia extra que eu utilizar estar√° linkada abaixo. ","date":"2023-06-21","objectID":"/posts/my_notes/:0:0","tags":["AI","Generative","IA Generativa"],"title":"Generative Modeling","uri":"/posts/my_notes/"},{"categories":["Tutorials"],"content":"Generative Modeling Conceituando IA Generativa √© um ramo do Machine Learning que involve treinamento de um modelo para gera√ß√£o de novos dados que sejam similares a um dado dataset. Um ponto interessante que j√° me chamou aten√ß√£o: \"‚Ä¶gerar novos dados que s√£o similares h√° um dado dataset\". Isso evidencia mais ainda a import√¢ncia do que voc√™ vai jogar para dentro do seu modelo, logo, a qualidade do seu dataset √© muito importante. ","date":"2023-06-21","objectID":"/posts/my_notes/:1:0","tags":["AI","Generative","IA Generativa"],"title":"Generative Modeling","uri":"/posts/my_notes/"},{"categories":["Tutorials"],"content":"Probabil√≠stico vs Determin√≠stico Um modelo generativo √© probabil√≠stico e n√£o determin√≠stico (o que j√° nos abre os olhos para vermos muita probabilidade ao longo dos pr√≥ximos cap√≠tulos). Isso quer dizer que n√≥s n√£o queremos gerar sempre a mesma imagem, n√≥s queremos na verdade uma certa vari√¢ncia nos nossos outputs. Sobre essa perspectiva, podemos imaginar que existe uma distribui√ß√£o de probabilidade desconhecida que explica o por que algumas imagens s√£o similares e outras n√£o. Logo, nosso trabalho durante a modelagem √© encontrar uma distribui√ß√£o o mais pr√≥xima poss√≠vel dessa distribui√ß√£o desconhecida, e por meio dela sermos capazes de amostrar elementos que sejam similares aos elementos da origem (dados de treino) - vai ficar mais claro ao longo post =D. O autor ainda deixa mais claro a diferen√ßa de generativo e discriminativo: Machine learning prevendo se uma imagem √© ou n√£o um quadro de Van Gogh: Discriminativo Machine learning capaz de gerar obras similares as obras de Van Gogh: Generativo E claro, n√£o poderia faltar algumas nota√ß√µes cient√≠ficas: Discriminativo: p(y|x) Generativo estimamos a p(x) ","date":"2023-06-21","objectID":"/posts/my_notes/:2:0","tags":["AI","Generative","IA Generativa"],"title":"Generative Modeling","uri":"/posts/my_notes/"},{"categories":["Tutorials"],"content":"O crescimento da IA Generativa O respaldo que a IA Generativa est√° tendo na ind√∫stria √© incontest√°vel. Um exemplo em particular que me chamou aten√ß√£o, foi sobre a gera√ß√£o de dados artificiais para utiliza√ß√£o como dados de input para treinamento de um modelo. H√° pouco tempo atr√°s, era imposs√≠vel gerar imagens t√£o realistas como as que est√£o sendo geradas nos √∫ltimos meses. Esse n√≠vel de qualidade abre margem para por exemplo a utiliza√ß√£o desses modelos para gerar imagens da retina de um ser humano com presen√ßa de alguma anormalidade de dif√≠cil detec√ß√£o, e assim treinarmos um modelo de detec√ß√£o. L√≥gico que aqui existem muitas preocupa√ß√µes envolvidas, mas s√≥ desse tipo de possibilidade existir, pra mim, √© surreal. ","date":"2023-06-21","objectID":"/posts/my_notes/:3:0","tags":["AI","Generative","IA Generativa"],"title":"Generative Modeling","uri":"/posts/my_notes/"},{"categories":["Tutorials"],"content":"As 3 Fortes raz√µes O autor tr√°s um pensamento bem peculiar que at√© ent√£o eu nunca tinha me questionado ou lido em outro lugar. Ele cita que a evolu√ß√£o da IA Generativa tr√°s 3 fortes raz√µes para desbloquear/possibilitar uma forma de IA muito mais sofisticada do que a que temos hoje. ","date":"2023-06-21","objectID":"/posts/my_notes/:4:0","tags":["AI","Generative","IA Generativa"],"title":"Generative Modeling","uri":"/posts/my_notes/"},{"categories":["Tutorials"],"content":"Raz√£o 1 A IA Generativa permite um entendimento mais completo do seu dado. N√£o se limitando a apenas categoriza√ß√£o, mas sim tendo um entendimento da distribui√ß√£o dos dados que permeiam determinada categoria. ","date":"2023-06-21","objectID":"/posts/my_notes/:4:1","tags":["AI","Generative","IA Generativa"],"title":"Generative Modeling","uri":"/posts/my_notes/"},{"categories":["Tutorials"],"content":"Raz√£o 2 A IA Generativa resvala em outros campos da IA, como por exemplo o Reinforcement Learning. No famoso Aprendizado por Refor√ßo, existe a a√ß√£o de um agente que aprende uma determinada task, como por exemplo um rob√¥ que aprender a andar em um determinado terreno. Com a IA Generativa voc√™ √© capaz de criar um mundo de possibilidades as quais esses rob√¥s podem aprender tornando o processo mais eficiente. ","date":"2023-06-21","objectID":"/posts/my_notes/:4:2","tags":["AI","Generative","IA Generativa"],"title":"Generative Modeling","uri":"/posts/my_notes/"},{"categories":["Tutorials"],"content":"Raz√£o 3 Qualquer forma de Intelig√™ncia que venha a ser desenvolvida para se assemelhar a um humano, precisa ter um poder generativo muito incr√≠vel, pois somos m√°quinas generativas muito incr√≠veis. Estudos neurocient√≠ficos sugerem que a nossa percep√ß√£o de realidade n√£o √© algo discriminativo, mas sim generativo, e que ‚Äúnosso modelo‚Äù est√° sendo treinado desde o nosso nascimento. Se voc√™ observar, a todo momento estamos gerando simula√ß√µes ao nosso redor: gerando idealiza√ß√µes de futuro imaginando um final para uma s√©rie escrevendo esse artigo ","date":"2023-06-21","objectID":"/posts/my_notes/:4:3","tags":["AI","Generative","IA Generativa"],"title":"Generative Modeling","uri":"/posts/my_notes/"},{"categories":["Tutorials"],"content":"Nossa primeira IA Generativa Nesse cap√≠tulo n√≥s ainda n√£o vamos codar (sinto te informar kkk). Mas √© uma abstra√ß√£o bem legal e que ajuda a fixar o entendimento sobre distribui√ß√£o real dos seus dados vs distribui√ß√£o que o seu modelo consegue replicar. Imagine a seguinte imagem: Agora imagine que essa distribui√ß√£o de pontos segue um determinado padr√£o, e que voc√™ precisa encontrar esse padr√£o (modelo), que vai permitir que voc√™ amostre diferentes pontos correspondentes a distribui√ß√£o real dos seus dados. Nesse caso, o autor utilizou um ret√¢ngulo para descrever esse modelo. Logo, qualquer ponto gerado a partir dessa representa√ß√£o, seria um ponto semelhante a distribui√ß√£o real. Acontece que, pela imagem acima, na verdade o modelo que descreveria aonde n√≥s temos pontos, seriam regi√µes continentais do mapa mundi. E sendo assim, os pontos gerados nas regi√µes de oceano, estariam fora dessa distribui√ß√£o. ","date":"2023-06-21","objectID":"/posts/my_notes/:5:0","tags":["AI","Generative","IA Generativa"],"title":"Generative Modeling","uri":"/posts/my_notes/"},{"categories":["Tutorials"],"content":"Latent Space (Espa√ßo Latente) Pra mim, esse √© o conceito mais importante do cap√≠tulo!! Fica claro ao final do cap√≠tulo que uma ideia chave que estamos tentando resolver √© encontrar uma forma de representa√ß√£o de um espa√ßo multidimensional, em um espa√ßo menor por√©m representativo. E √© justamente isso que √© o espa√ßo latente. Por exemplo, vamos imaginar que para descrever o rosto de uma pessoa a gente precise de 100 caracter√≠sticas visuais. Agora imagine que voc√™ consiga criar um modelo que com apenas 3 caracter√≠sticas seja poss√≠vel descrever o mesmo rosto, e ainda mapear essas tr√™s caracter√≠sticas para o espa√ßo das 100 caracter√≠sticas iniciais. Dessa forma voc√™ ta saindo do espa√ßo latente e voltando a representa√ß√£o de alta dimensionalidade. E √© justamente isso que queremos aqui, descobrir esse espa√ßo latente para os mais variados de problemas. Se voc√™ tem um background estat√≠stico, ou de machine learning, j√° deve ter utilizado o PCA para redu√ß√£o de dimensionalidade, de forma simplificada √© mais ou menos isso que estamos fazendo aqui. ","date":"2023-06-21","objectID":"/posts/my_notes/:6:0","tags":["AI","Generative","IA Generativa"],"title":"Generative Modeling","uri":"/posts/my_notes/"},{"categories":["Tutorials"],"content":"Taxonomia da IA Generativa Para finalizar, temos aqui uma categoriza√ß√£o das diferentes abordagens nesse universo generativo, de forma geral, s√£o citados 3 poss√≠veis abordagens: Explicitamente modelar a fun√ß√£o de densidade Explicitamente modelar uma aproxima√ß√£o trat√°vel da fun√ß√£o de densidade Implicitamente modelar a fun√ß√£o de densidade, atrav√©s de um processo estoc√°stico que gera dados diretamente Espero entender mais nos pr√≥ximos cap√≠tulos üòÖ! √â sobre, at√© a pr√≥xima e no cap√≠tulo 2 vamos falar um pouquinho sobre CNNs e vamos codar um pouquinho!!! ","date":"2023-06-21","objectID":"/posts/my_notes/:7:0","tags":["AI","Generative","IA Generativa"],"title":"Generative Modeling","uri":"/posts/my_notes/"},{"categories":["Projetos"],"content":"Resolvemos criar um novo artigo para detalhar melhor o que foi feito na etapa de modelagem do projeto do Classificador BNCC. Assim conseguimos dar mais aten√ß√£o e justificar algumas escolhas. ","date":"2022-06-14","objectID":"/posts/bncc-modeling/:0:0","tags":["Bag of words","TFIDF","N-Grams","Word2Vec","Data Science","NLP","Education"],"title":"Classificador BNCC Pt.2","uri":"/posts/bncc-modeling/"},{"categories":["Projetos"],"content":"Cronograma de modelagem Nosso intuito foi buscar o melhor baseline para nosso problema, mapeando alguns universos de possibilidades, dentre modelos de machine learning e estrat√©gias de transforma√ß√£o de texto em dado num√©rico. V√°lido lembrar que o dado que est√° entrando nessa etapa do pipeline est√° ‚Äúlimpo‚Äù, ou seja, passou pelas etapas de pr√©-processamentos que julgamos necess√°rias. Resumido abaixo: df[\"questions_clean\"] = ( df[\"questions\"] .astype(str) .apply(html.unescape) .apply(lambda x: cleaning.remove_html(x)) .apply(lambda x: x.lower()) .apply(lambda x: cleaning.remove_punctuation_2(x)) .apply(cleaning.remove_italic_quotes) .apply(cleaning.remove_open_quotes) .apply(cleaning.remove_end_quotes) .apply(cleaning.remove_italic_dquotes) .apply(cleaning.remove_open_dquotes) .apply(cleaning.remove_quote) .apply(lambda x: cleaning.remove_pt_stopwords(x)) .apply(lambda x: cleaning.remove_en_stopwords(x)) .apply(word_tokenize) .apply(lambda x: cleaning.remove_punctuation_2(x)) ) Em quest√£o de modelos, utilizamos: Regress√£o Log√≠stica Random Forest LightGBM Al√©m disso, para cada um dos modelos utilizamos diferentes tipos de feature engineering, apresentadas a seguir: Bag of words TFIDF N-grams Word2Vec OBS: Tamb√©m tentamos utilizar o algor√≠timo de machine learning Gaussian Naive Bayes, mas por conta do mesmo n√£o trabalhar com matriz esparsa (na implementa√ß√£o do Scikit-Learn), n√£o tivemos recurso computacional para rodar o algor√≠timo. O dado como matriz n√£o esparsa ocuparia +40 Gb em mem√≥ria. ","date":"2022-06-14","objectID":"/posts/bncc-modeling/:1:0","tags":["Bag of words","TFIDF","N-Grams","Word2Vec","Data Science","NLP","Education"],"title":"Classificador BNCC Pt.2","uri":"/posts/bncc-modeling/"},{"categories":["Projetos"],"content":"Import√¢ncia da valida√ß√£o Como estamos trabalhando com m√∫ltiplas classes nos dois classificadores que estamos otimizando, e como tamb√©m estamos buscando a solu√ß√£o mais robusta poss√≠vel, √© importante que utilizemos alguma estrat√©gia de valida√ß√£o. A biblioteca do scikit-learn oferece uma s√©rie m√©todos que podem ser utilizados para essa finalidade. Aqui, optamos por utilizar o StratifiedKFold com 5 splits. Esse m√©todo de valida√ß√£o preserva a propor√ß√£o inicial de cada uma das classes envolvidas no target. ","date":"2022-06-14","objectID":"/posts/bncc-modeling/:2:0","tags":["Bag of words","TFIDF","N-Grams","Word2Vec","Data Science","NLP","Education"],"title":"Classificador BNCC Pt.2","uri":"/posts/bncc-modeling/"},{"categories":["Projetos"],"content":"Pipelines Os baselines que ir√£o ser utilizados abaixo foram configurados como o seguinte dicion√°rio: # baselines models = { \"rg_lg\": LogisticRegression(max_iter=500, n_jobs=8), \"r_forest\": RandomForestClassifier(max_depth=500, n_jobs=8), \"lgbm\": LGBMClassifier(n_jobs=8), } Dessa forma, conseguimos iterar em cada um dos modelos e automatizar nossas avalia√ß√µes. ","date":"2022-06-14","objectID":"/posts/bncc-modeling/:3:0","tags":["Bag of words","TFIDF","N-Grams","Word2Vec","Data Science","NLP","Education"],"title":"Classificador BNCC Pt.2","uri":"/posts/bncc-modeling/"},{"categories":["Projetos"],"content":"Bag Of Words O BoW (Bag of Words), ou saco de palavras, √© uma t√©cnica onde criamos um dicion√°rio de palavras que contemplam nosso dataset, e contamos onde cada uma delas est√° presente ou n√£o. kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=2020) for train_idx, val_idx in kfold.split( modeling.X_train[\"questions_clean\"], modeling.y_train ): for name, model in models.items(): x_train, y_train = ( modeling.X_train[\"questions_clean\"].iloc[train_idx], modeling.y_train.iloc[train_idx], ) x_val, y_val = ( modeling.X_train[\"questions_clean\"].iloc[val_idx], modeling.y_train.iloc[val_idx], ) # this will make our bag of words strategy count_vectorizer = CountVectorizer() count_vectorizer.fit(x_train) X_train_cv = count_vectorizer.transform(x_train) X_val_cv = count_vectorizer.transform(x_val) # lgbm does not work with int type of data, so # we need to convert to float to use if name == \"lgbm\": X_train_cv = X_train_cv.astype(\"float32\") X_val_cv = X_val_cv.astype(\"float32\") y_train = y_train.astype(\"float32\") y_val = y_val.astype(\"float32\") cv_classifier = model cv_classifier.fit(X_train_cv, y_train) y_pred = cv_classifier.predict(X_val_cv) f1 = f1_score(y_val, y_pred, average=\"macro\") print(\"Model: {}. Macro avg F1: {}\".format(name, f1)) Resultados abaixo: Modelos Macro Avg F1 Regress√£o Log√≠stica 0.743 +/- 0.03 Random Forest 0.657 +/- 0.05 LightGBM 0.745 +/- 0.002 ","date":"2022-06-14","objectID":"/posts/bncc-modeling/:3:1","tags":["Bag of words","TFIDF","N-Grams","Word2Vec","Data Science","NLP","Education"],"title":"Classificador BNCC Pt.2","uri":"/posts/bncc-modeling/"},{"categories":["Projetos"],"content":"N-Grams Esta t√©cnica visa realizar o agrupamento de tokens. O tamanho desse agrupamento √© escolhido pelo valor do N: Uni: agrupamento um a um Bi: agrupamento dois a dois Tri: agrupamento tr√™s a tr√™s ‚Ä¶ O interessante aqui √© que conseguimos pegar um pouco de contexto, j√° que algumas palavras normalmente aparecem acompanhdas de outras. Por exemplo: Pedro √Ålvares Cabral, Papai Noel, bom dia‚Ä¶ Aqui n√≥s utilizamos duas abordagens: Somente bi-gram uni-gram + bi-gram Como podemos ver no c√≥digo abaixo: kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=2020) for train_idx, val_idx in kfold.split( modeling.X_train[\"questions_clean\"], modeling.y_train ): for name, model in models.items(): x_train, y_train = ( modeling.X_train[\"questions_clean\"].iloc[train_idx], modeling.y_train.iloc[train_idx], ) x_val, y_val = ( modeling.X_train[\"questions_clean\"].iloc[val_idx], modeling.y_train.iloc[val_idx], ) # the param ngram_range is responsible to set # how we'll want the n-grams. (1, 2) is setting # to bring uni and bi-gram combination. count_vectorizer = CountVectorizer(ngram_range=(1, 2)) count_vectorizer.fit(x_train) X_train_cv = count_vectorizer.transform(x_train) X_val_cv = count_vectorizer.transform(x_val) if name == \"lgbm\": X_train_cv = X_train_cv.astype(\"float32\") X_val_cv = X_val_cv.astype(\"float32\") y_train = y_train.astype(\"float32\") y_val = y_val.astype(\"float32\") cv_classifier = model cv_classifier.fit(X_train_cv, y_train) y_pred = cv_classifier.predict(X_val_cv) f1 = f1_score(y_val, y_pred, average=\"macro\") # print('ROC AUC - {}: {}'.format(name, mean)) print(\"Model: {}. Macro avg F1: {}\".format(name, f1)) Resultados apenas para o uni-gram + bi-gram: Modelos Macro Avg F1 Regress√£o Log√≠stica 0.739 +/- 0.03 Random Forest 0.652 +/- 0.05 LightGBM 0.729 +/- 0.003 ","date":"2022-06-14","objectID":"/posts/bncc-modeling/:3:2","tags":["Bag of words","TFIDF","N-Grams","Word2Vec","Data Science","NLP","Education"],"title":"Classificador BNCC Pt.2","uri":"/posts/bncc-modeling/"},{"categories":["Projetos"],"content":"TFIDF Como pode ser visto na imagem acima, aqui n√≥s conseguimos identificar o qu√£o importante cada palavra √©, em rela√ß√£o ao todo que estamos tentando prever. Por exemplo: quantas vezes a palavra soma aparece no texto de uma quest√£o de matem√°tica, frente a quantidade de vezes que ela aparece em todas as quest√µes de matem√°tica que temos na base? Utilizamos para isso, o c√≥digo abaixo: kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=2020) for train_idx, val_idx in kfold.split( modeling.X_train[\"questions_clean\"], modeling.y_train ): for name, model in models.items(): x_train, y_train = ( modeling.X_train[\"questions_clean\"].iloc[train_idx], modeling.y_train.iloc[train_idx], ) x_val, y_val = ( modeling.X_train[\"questions_clean\"].iloc[val_idx], modeling.y_train.iloc[val_idx], ) # here we are performing tfidf on training data # and choosing n-grams of 1 and 2 tfidf = TfidfVectorizer(ngram_range=(1, 2)) tfidf.fit(x_train) X_train_cv = tfidf.transform(x_train) X_val_cv = tfidf.transform(x_val) if name == \"lgbm\": X_train_cv = X_train_cv.astype(\"float32\") X_val_cv = X_val_cv.astype(\"float32\") y_train = y_train.astype(\"float32\") y_val = y_val.astype(\"float32\") cv_classifier = model cv_classifier.fit(X_train_cv, y_train) y_pred = cv_classifier.predict(X_val_cv) f1 = f1_score(y_val, y_pred, average=\"macro\") print(\"Model: {}. Macro avg F1: {}\".format(name, f1)) Resultados apenas para o uni-gram + bi-gram:: Modelos Macro Avg F1 Regress√£o Log√≠stica 0.7151 +/- 0.006 Random Forest 0.6609 +/- 0.010 LightGBM 0.7230 +/- 0.007 ","date":"2022-06-14","objectID":"/posts/bncc-modeling/:3:3","tags":["Bag of words","TFIDF","N-Grams","Word2Vec","Data Science","NLP","Education"],"title":"Classificador BNCC Pt.2","uri":"/posts/bncc-modeling/"},{"categories":["Projetos"],"content":"Word2Vec Esse √© modelo de word embeddings. Esse tipo de representa√ß√£o de texto busca identificar o significado de uma palavra no seu contexto (conota√ß√£o). O Word2Vec foi um dos primeiros a aplicar esse tipo de conceito. Com o Word2vec, foi poss√≠vel, por exemplo, realizar a interpreta√ß√£o abaixo: King - Man + Woman = Queen Com o Word2vec n√≥s podemos contornar o problema de dimensionalidade, e representar um texto qualquer em um n√∫mero de features pre-determinado que melhor capte o contexto analisado. Abaixo voc√™ pode ver como fizemos. # creating the wv object wv = Word2Vec( sentences=modeling.X_train[\"questions_clean\"].apply(lambda x: x.split()), vector_size=100, window=3, min_count=1, workers=10 ) # function to transform those words def transforma_palavra(question): lista_vetores = [wv.wv.get_vector(x) for x in question.split()] return np.sum(lista_vetores, axis=0) # applying to those questions vetores_embeddings = modeling.X_train[\"questions_clean\"].apply(transforma_palavra) # Criando um Dataframe com os resultados df_embeddings = pd.DataFrame.from_dict( dict(zip(vetores_embeddings.index, vetores_embeddings.values)) ).T # Definindo os nomes das colunas df_embeddings.columns = [\"embedding_\" + str(i) for i in range(1, 101)] # Vamos tamb√©m trazer o tweet original e o sentimento df_embeddings[\"question\"] = modeling.X_train[\"questions_clean\"] df_embeddings[\"question_target\"] = modeling.X_train[\"target\"].values # splitting those embeddings X_emb = df_embeddings[df_embeddings.columns[:100]] y_emb = df_embeddings.question_target X_train_emb, X_test_emb, y_train_emb, y_test_emb = train_test_split( X_emb, y_emb, test_size=0.2, stratify=y_emb ) for nome, modelo in modelos_teste.items(): metrica = cross_val_score( modelo, X_train_emb, y_train_emb, cv=3, scoring=\"f1_macro\" ).mean() Modelos Macro Avg F1 Regress√£o Log√≠stica 0.565 Random Forest 0.551 LightGBM 0.581 ","date":"2022-06-14","objectID":"/posts/bncc-modeling/:3:4","tags":["Bag of words","TFIDF","N-Grams","Word2Vec","Data Science","NLP","Education"],"title":"Classificador BNCC Pt.2","uri":"/posts/bncc-modeling/"},{"categories":["Projetos"],"content":"Tuning Ap√≥s toda experimenta√ß√£o, vimos que o modelo mais promissor, em termos de m√©trica, tempo de processamento e simplicidade, foi a regress√£o log√≠stica em conjunto com o Bag of Words. Da√≠ em diante, realizamos o tuning do par√¢metro C da regress√£o log√≠stica, e conseguimos subir a acur√°cia para 0.8 em m√©dia dentre as classes, para o modelo que realiza a classifica√ß√£o do segundo classificador. Al√©m de ajustar o C, n√≥s tamb√©m retiramos o class_balanced, pois o mesmo estava prejudicando a performance. ","date":"2022-06-14","objectID":"/posts/bncc-modeling/:4:0","tags":["Bag of words","TFIDF","N-Grams","Word2Vec","Data Science","NLP","Education"],"title":"Classificador BNCC Pt.2","uri":"/posts/bncc-modeling/"},{"categories":["Projetos"],"content":"Conclus√£o Ainda existe muito espa√ßo para melhoria do modelo e espa√ßo para melhoria da aplica√ß√£o como um todo. Continuaremos atualizando de acordo fomos avan√ßado, obrigado pela leitura. ","date":"2022-06-14","objectID":"/posts/bncc-modeling/:5:0","tags":["Bag of words","TFIDF","N-Grams","Word2Vec","Data Science","NLP","Education"],"title":"Classificador BNCC Pt.2","uri":"/posts/bncc-modeling/"},{"categories":["Projetos"],"content":"Projeto desenvolvido pelos alunos Brisa Rosatti, Luciano Batista, Pedro Moreau, Wilson Fran√ßa do Curso de Data Science \u0026 Machine Learning da Tera em colabora√ß√£o com a Studos e ArcoTech. ","date":"2022-06-08","objectID":"/posts/bncc-classifier/:0:0","tags":["Tutorial","Data Science","NLP","Education"],"title":"Classificador BNCC","uri":"/posts/bncc-classifier/"},{"categories":["Projetos"],"content":"PARTE I: Estrutura do projeto ","date":"2022-06-08","objectID":"/posts/bncc-classifier/:1:0","tags":["Tutorial","Data Science","NLP","Education"],"title":"Classificador BNCC","uri":"/posts/bncc-classifier/"},{"categories":["Projetos"],"content":"Contexto Segundo o IBGE, no Brasil, existem registradas 124840 escolas de ensino fundamental e 28933 escolas do ensino m√©dio. Em termos de n√∫mero de matr√≠culas, isso representa 26718830 matr√≠culas para o ensino fundamental e 7550753 para o ensino m√©dio https://cidades.ibge.gov.br/brasil/pesquisa/13/5908. Com o intuito de harmonizar a base curricular desse grande n√∫mero escolas e estudantes, oferecendo assim um modelo de ensino de qualidade e que vise capacitar os alunos para as consecutivas fases da sua vida, o Minist√©rio da Educa√ß√£o publicou, em 16 de setembro de 2015 a primeira vers√£o da [Base Nacional Comum Curricular (BNCC)]~(http://basenacionalcomum.mec.gov.br/). Este documento normativo define o conjunto de aprendizagens que todos os alunos devem desenvolver ao longo da sua forma√ß√£o na Educa√ß√£o B√°sica. A BNCC est√° estruturada em c√≥digos alfanum√©ricos, que contemplam as compet√™ncias a serem desenvolvidas em cada etapa da forma√ß√£o do aluno na Educa√ß√£o B√°sica. O c√≥digo √© dividido em quatro partes, como o exemplo EF 09 AR 01. O primeiro par de letras indica a qual etapa do ensino b√°sico essa compet√™ncia est√° inserida, neste caso, Ensino Fundamental. O primeiro par de n√∫meros indica o ano ou bloco de anos em que esta compet√™ncia est√° inclu√≠da, neste caso 9¬∞ ano. O segundo par de letras indica qual a qual √°rea do conhecimento a compet√™ncia pertence, AR = Artes e o segundo par de n√∫meros mostra a posi√ß√£o da habilidade na numera√ß√£o sequencial do ano ou bloco de anos referente √† compet√™ncia. As imagens abaixo apresentam outros exemplos da descri√ß√£o dos c√≥digos da BNCC. A partir da homologa√ß√£o do relat√≥rio final da BNCC em 2018, pelo Minist√©rio da Educa√ß√£o, as escolas devem alinhar seus curr√≠culos em conson√¢ncia √†s diretrizes estabelecidas pela BNCC. E isso impacta em um replanejamento de estruturas curriculares, alinhamento da coordena√ß√£o pedag√≥gica, treinamento de professores para que apliquem de forma eficiente as normativas √†s suas aulas e avalia√ß√µes, entre outras. Todo esse esfor√ßo demanda tempo, investimento e realinhamento de uma estrutura de ensino j√° estabelecida, para que o novo programa de curr√≠culos seja aplicado de forma eficiente. Os conselhos de educa√ß√£o de cada estado, juntamente com as secretarias de educa√ß√£o, s√£o respons√°veis pela fiscaliza√ß√£o da aplica√ß√£o das normativas da BNCC em cada escola, sejam elas p√∫blicas ou privadas, j√° que a aplica√ß√£o desta normativa √© obrigat√≥ria a todas as escolas. ","date":"2022-06-08","objectID":"/posts/bncc-classifier/:1:1","tags":["Tutorial","Data Science","NLP","Education"],"title":"Classificador BNCC","uri":"/posts/bncc-classifier/"},{"categories":["Projetos"],"content":"Problema de neg√≥cio √â percebido que apesar da obrigatoriedade de seguir a BNCC, as Escolas geralmente n√£o t√™m o devido preparo e professores n√£o s√£o bem capacitados para relacionar os c√≥digos √†s quest√µes aplicadas em provas e simulados. Essa classifica√ß√£o se faz necess√°ria para que as avalia√ß√µes contemplem as etapas de aprendizado previstas para cada fase do ensino b√°sico, fundamental e m√©dio. Uma das raz√µes para esses problemas √© que al√©m do documento da BNCC ser extenso, possui mais de mil c√≥digos em sua totalidade, tornando todo processo de classifica√ß√£o moroso e ineficiente. Assim, vemos que tr√™s problemas de neg√≥cio emergem: O primeiro est√° relacionado √† pr√≥pria escola do Ensino B√°sico, que, em fun√ß√£o da necessidade de adequa√ß√£o curricular √† BNCC, demanda uma ferramenta de acompanhamento das distintas disciplinas quanto ao cumprimento da base curricular reformulada, para que se cumpra a narrativa; Al√©m disso, h√° a necessidade de treinamento de professores para a implementa√ß√£o das normativas da BNCC √†s disciplinas √†s quais s√£o respons√°veis, visto que, em sua forma√ß√£o acad√™mica, as normativas da BNCC n√£o foram contempladas; √â sabido, ainda, que profissionais das Ci√™ncias da Natureza e Matem√°tica apresentam dificuldade e resist√™ncia quanto √† aplica√ß√£o da BNCC em suas disciplinas, dado o car√°ter subjetivo de alguns c√≥digos e habilidades, a falta de preparo em traduzir esses c√≥digos em suas quest√µes, durante a avalia√ß√£o e a falta de experi√™ncia com esse tipo de abordagem curricular. Esta informa√ß√£o foi coletada a partir de entrevistas com profissionais da educa√ß√£o na cidade de Ilh√©us - BA. Diante desses problemas, foi proposta, neste trabalho de conclus√£o de curso, uma solu√ß√£o de dados que auxiliar√° as escolas e suas divis√µes pedag√≥gicas quanto ao acompanhamento e cumprimento das normativas estabelecidas pela BNCC nas diferentes disciplinas e etapas dos ensinos fundamental e m√©dio. Essa solu√ß√£o consiste em uma aplica√ß√£o que ir√° classificar quest√µes enviadas pelo usu√°rio (profissional de educa√ß√£o) no formato texto, retornando para o mesmo os prov√°veis c√≥digos correspondentes √† BNCC. Dessa forma, o processo pedag√≥gico poder√° ser mediado por esta solu√ß√£o, trazendo celeridade √† reorganiza√ß√£o demandada pelo MEC. Al√©m das escolas, EdTechs que tem como finalidade elaborar quest√µes para compor servi√ßos de bancos de quest√µes tamb√©m podem se valer desta solu√ß√£o de dados para fornecer as quest√µes j√° classificadas de acordo √† BNCC, agregando valor ao seu servi√ßo. ","date":"2022-06-08","objectID":"/posts/bncc-classifier/:1:2","tags":["Tutorial","Data Science","NLP","Education"],"title":"Classificador BNCC","uri":"/posts/bncc-classifier/"},{"categories":["Projetos"],"content":"Impacto Porque nosso projeto importa? Acreditamos que o Projeto impactar√° duas √°reas diferentes: o campo dos neg√≥cios e o pedag√≥gico. No quesito de neg√≥cios presumimos √† diminui√ß√£o da necessidade de profissionais envolvidos no processo de classifica√ß√£o de quest√µes, diminuindo assim o custo da empresa e possibilitando que esses profissionais possam atuar de forma mais eficiente em diferentes demandas, como na elabora√ß√£o de quest√µes para o banco de dados, por exemplo. No √¢mbito de vantagem competitiva, as empresas que incorporarem nossa tecnologia inovadora teriam um diferencial de mercado dentre outras EdTechs - startups focadas no desenvolvimento de solu√ß√µes tecnol√≥gicas para a educa√ß√£o. No campo pedag√≥gico, a implementa√ß√£o do Classificador tornaria a aplica√ß√£o da BNCC mais acess√≠vel aos profissionais da educa√ß√£o, possibilitando assim que as avalia√ß√µes e testes dos curr√≠culos escolares estejam melhor alinhados √† normativa do MEC. Desse modo, alcan√ßaremos o principal objetivo do Projeto, no qual teremos um ensino mais direcionado e eficiente, impactando diretamente e de forma positiva o aprendizado estudantil. Al√©m disso, temos o intuito de tornar gratuito o acesso a essa tecnologia para profissionais de institui√ß√µes p√∫blicas de ensino, sem que necessariamente haja investimento estatal ou necessidade de participa√ß√£o de licita√ß√µes, contribuindo assim para a constru√ß√£o de um ensino b√°sico de qualidade. ","date":"2022-06-08","objectID":"/posts/bncc-classifier/:1:3","tags":["Tutorial","Data Science","NLP","Education"],"title":"Classificador BNCC","uri":"/posts/bncc-classifier/"},{"categories":["Projetos"],"content":"Desenho da solu√ß√£o Este projeto visa a implementa√ß√£o de um algoritmo de Machine Learning usando t√©cnicas de Processamento de Linguagem Natural para classifica√ß√£o dos c√≥digos da Base Nacional Comum Curricular (BNCC). Ser√° utilizada uma abordagem de segmenta√ß√£o do problema em quatro partes, referentes √†s quatro partes do c√≥digo BNCC. Dessa forma, ser√° obtida, ao final de todas as etapas, a classe mais prov√°vel de cada parte do c√≥digo da BNCC referente √† quest√£o que est√° sendo inserida no modelo para ser predita. Na presente data da publica√ß√£o deste artigo, o grupo construiu os classificadores 1 e 3. Os classificadores 2 e 4 est√£o nos pr√≥ximos passos para o trabalho, j√° que demandam uma base de dados maior e com o tageamento destas classes, que at√© o momento n√£o foram disponibilizadas para o grupo. Assim, para os Classificadores 1 e 3 a solu√ß√£o foi desenhada da seguinte forma: Extra√ß√£o dos dados a partir da base de dados da Studos Limpeza e pr√©-processamento dos dados Cria√ß√£o de um modelo de classifica√ß√£o para √Årea do Conhecimento e para Componente Curricular Produtiza√ß√£o do modelo no Heroku Cria√ß√£o de um frontend no Streamlit Integra√ß√£o do modelo produtizado com o frontend para fornecimento do servi√ßo a usu√°rios finais Como modelo de Machine Learning, foi adotada a Regress√£o Log√≠stica e, levando em conta a diferen√ßa de balanceamento entre as classes de cada target utilizada para os classificadores 1 e 3. ","date":"2022-06-08","objectID":"/posts/bncc-classifier/:1:4","tags":["Tutorial","Data Science","NLP","Education"],"title":"Classificador BNCC","uri":"/posts/bncc-classifier/"},{"categories":["Projetos"],"content":"PARTE II: Dados e solu√ß√£o ","date":"2022-06-08","objectID":"/posts/bncc-classifier/:2:0","tags":["Tutorial","Data Science","NLP","Education"],"title":"Classificador BNCC","uri":"/posts/bncc-classifier/"},{"categories":["Projetos"],"content":"Dados Para uma melhor visibilidade da problem√°tica envolvida neste classificador BNCC, o quadro abaixo apresenta algumas hip√≥teses levantadas para o desenvolvimento do produto, elucidando as fontes de dados, os dados utilizados e as demandas de neg√≥cios √†s quais se pretende atender. Data Tracking Sheet Perguntas de Neg√≥cio Dados Base ou calculada C√°lculo Fonte de dado Como classificar quest√µes de acordo com a BNCC? Quest√µes previamente classificadas Calculada √° definir Base dados da Studos Como desenvolver racioc√≠nio l√≥gico/matem√°tico nas escolas? Quest√µes Classificadas Calculada √° definir Base dados da Studos Como melhorar a efici√™ncia do ensino na sala de aula? App com Quest√µes Classificadas Calculada √° definir Base de dados Studos Como ajudar o professor na elabora√ß√£o de avalia√ß√µes e atividades nas escolas? App com Quest√µes Classificadas Calculada √° definir Base de dados Studos Fonte Como fonte de dados, iremos utilizar uma base disponibilizada pela Studos, uma EdTech onde o integrante do grupo Luciano trabalha. A base n√£o cont√©m informa√ß√µes sens√≠veis e conta com quest√µes classificadas dentro dos c√≥digos da BNCC, assim como informa√ß√µes necess√°rias para a cria√ß√£o dos quatro classificadores. Datasets Contamos com a libera√ß√£o de +4M de quest√µes classificadas com suas √°reas de conhecimento e assuntos. Dessas, 8.4k possuem classifica√ß√£o das habilidades da BNCC. Al√©m disso, tem acesso p√∫blico ao documento da (BNCC) contendo informa√ß√µes espec√≠ficas dos c√≥digos. Para o primeiro modelo de classifica√ß√£o foi utilizado a coluna ‚ÄòetapaEnsino‚Äô como target, a qual designa cada quest√£o como Ensino M√©dio, Fundamental I ou II. Para o segundo classificador ser√° utilizado a coluna ‚Äòmat√©ria‚Äô como target, na qual cada quest√£o √© classificada de acordo com o componente curricular. O dataset cont√©m as seguintes vari√°veis: id (int): identificador √∫nico de cada quest√£o questoes (str): texto contendo o conte√∫do da quest√£o tipoQuestoes (int): tipo da quest√£o, se foi m√∫ltipla-escolha, certo ou errado, discursiva, etc. topico (str): assunto do componente curricular materia (str): componente curricular etapaEnsino (str): √°rea de conhecimento (Fundamental I, Fundamental II ou Ensino M√©dio) A seguir tem-se uma visualia√ß√£o do head do dataset utilizado. ","date":"2022-06-08","objectID":"/posts/bncc-classifier/:2:1","tags":["Tutorial","Data Science","NLP","Education"],"title":"Classificador BNCC","uri":"/posts/bncc-classifier/"},{"categories":["Projetos"],"content":"EDA (An√°lise Explorat√≥ria de dados) Targets Nesse primeiro momento, n√≥s estamos focados na cria√ß√£o dos classificadores 1 e 3. Foram coletados 100k de observa√ß√µes em que seguem a seguinte distribui√ß√£o para a coluna Target ‚ÄòetapaEnsino‚Äô: data_pre_process['etapaEnsino'].value_counts() M√©dio \u0026 Pr√©-Vestibular 35676 Fundamental II 33790 Fundamental I 18379 Name: etapaEnsino, dtype: int64 E para o Target do segundo Classificador temos 76K observa√ß√µes distribu√≠das em 12 componentes curriculares distintos conforme pode ser visto abaixo: # Observing if the filtering was correctly applied by visualizing the clases df_bncc_copy_targets_bncc[\"target\"].value_counts() Matem√°tica 12539 L√≠ngua Portuguesa 11887 Geografia 9266 Hist√≥ria 9072 Ingl√™s 8103 Ci√™ncias 7099 F√≠sica 5537 Arte 3681 Qu√≠mica 3276 Biologia 3133 Educa√ß√£o F√≠sica 2386 Ensino Religioso 468 Name: target, dtype: int64 Sendo assim, nos resta analisar a vari√°vel principal do dataset, a coluna ‚Äòquestoes‚Äô. Apesar de ter uma s√©rie de an√°lises que seria interessante realizar, observamos que a maior parte delas necessitaria de um texto o mais limpo poss√≠vel. O texto das quest√µes s√£o exibidos no formato web, ent√£o temos uma quantidade bem grande de tags html para remover. Ent√£o esse se tornou nosso primeiro passo do data cleaning. Seguimos ent√£o com uma remo√ß√£o de n√∫meros, pois existem muitos n√∫meros indicando alternativas das quest√µes, assim como em quest√µes somat√≥rias existe a contagem das respostas (00, 11, 22 ‚Ä¶), remo√ß√£o de caracteres, de palavras raras e frequentes, entre outras t√©cnicas descritas a seguir. ","date":"2022-06-08","objectID":"/posts/bncc-classifier/:2:2","tags":["Tutorial","Data Science","NLP","Education"],"title":"Classificador BNCC","uri":"/posts/bncc-classifier/"},{"categories":["Projetos"],"content":"Abordagem MLOps e Configura√ß√£o de Ambiente O projeto de Ci√™ncia de Dados foi abordado pelo grupo com a inten√ß√£o de utilizar conceitos de MLOps. Dessa forma, todo o projeto foi versionado utilizando GitHub neste link. O reposit√≥rio foi organizado em pastas que separam as fun√ß√µes utilizadas para o pr√© processamento, as an√°lises realizadas e notebooks contendo os modelos treinados. Todo este ambiente foi configurado utilizando pyenv, de forma que as bibliotecas pudessem ser reproduzidas localmente em outras m√°quinas sem correr risco de quebrar por diferen√ßa nas vers√µes das bibliotecas e de Python utilizados. Fluxo para o treinamento Para implementa√ß√£o do algoritmo de classifica√ß√£o dos c√≥digos na BNCC, seguem as seguintes etapas: Coleta dos dados rotulados e n√£o rotulados da Base de quest√µes; Separa√ß√£o dos dados em dois (treino e teste), ent√£o decidir as m√©tricas de avalia√ß√£o; Pr√©-processamento: remo√ß√£o de pontua√ß√£o, lowercase das palavras, remo√ß√£o de stopwords, stemming, lemmatization, remo√ß√£o palavras raras, N-grams. O c√≥digo do pipeline de pr√© processamento das quest√µes pode ser visualizado nas imagens a seguir: import re import string from collections import Counter import nltk import pandas as pd from nltk.corpus import stopwords from nltk.tokenize import word_tokenize nltk.download(\"stopwords\") nltk.download(\"punkt\") PUNCT_TO_REMOVE = string.punctuation pt_stopwords = set(stopwords.words(\"portuguese\")) en_stopwords = set(stopwords.words(\"english\")) def to_lower(text: str) -\u003e str: return text.lower() def remove_tags(text: str) -\u003e str: pattern = re.compile(\"\u003c.*?\u003e\") cleantext = re.sub(pattern, \" \", text).replace(\"\\xa0\", \" \") return cleantext def remove_punctuation(text: str) -\u003e str: return re.sub(r\"[^\\w\\s]\", \" \", text) def remove_numbers(text: str) -\u003e str: pattern = re.compile(\"[0-9]+\") clean_text = re.sub(pattern, \" \", text) return clean_text def remove_standard_stopwords(text: str) -\u003e str: stop_words = set(stopwords.words(\"portuguese\")) word_tokens = word_tokenize(text) filtered_sentence = [w for w in word_tokens if w not in stop_words] final_sentence = \" \".join(filtered_sentence) return final_sentence def remove_html(text): html_pattern = re.compile(\"\u003c.*?\u003e\") return html_pattern.sub(r\"\", text) def remove_punctuation_2(text): return text.translate(str.maketrans(\"\", \"\", PUNCT_TO_REMOVE)) # Fun√ß√£o para remover aspas em it√°lico def remove_italic_dquotes(text: str) -\u003e str: pattern = re.compile(r'\"') clean_text = re.sub(pattern, \" \", text) return clean_text # Fun√ß√£o para remover aspas de abertura def remove_open_dquotes(text: str) -\u003e str: pattern = re.compile(r\"‚Äú\") clean_text = re.sub(pattern, \" \", text) return clean_text # Fun√ß√£o para remover aspas de fechamento def remove_end_dquotes(text: str) -\u003e str: pattern = re.compile(r\"‚Äù\") clean_text = re.sub(pattern, \" \", text) return clean_text # Fun√ß√£o para remover single quotes normal def remove_italic_quotes(text: str) -\u003e str: pattern = re.compile(r\"'\") clean_text = re.sub(pattern, \" \", text) return clean_text # Fun√ß√£o para remover single quotes abertura def remove_open_quotes(text: str) -\u003e str: pattern = re.compile(r\"‚Äò\") clean_text = re.sub(pattern, \" \", text) return clean_text # Fun√ß√£o para remover single quotes fechamento def remove_end_quotes(text: str) -\u003e str: pattern = re.compile(r\"‚Äô\") clean_text = re.sub(pattern, \" \", text) return clean_text # Fun√ß√£o para remover aspas de abertura def remove_quote(text: str) -\u003e str: pattern = re.compile(r\"‚Äõ\") clean_text = re.sub(pattern, \" \", text) return clean_text def remove_pt_stopwords(text): return \" \".join([word for word in str(text).split() if word not in pt_stopwords]) def remove_en_stopwords(text): return \" \".join([word for word in str(text).split() if word not in en_stopwords]) class RemoveFrqRare: def __init__(self, df: pd.DataFrame, n_frq_words: int = 10, n_rare_words: int = 10): self.df = df self.n_frq_words = n_frq_words self.n_rare_words = n_rare_words self.frq_words = [] self.rare_words = [] def calc_frq_words(self): final_list =","date":"2022-06-08","objectID":"/posts/bncc-classifier/:2:3","tags":["Tutorial","Data Science","NLP","Education"],"title":"Classificador BNCC","uri":"/posts/bncc-classifier/"},{"categories":["Projetos"],"content":"PARTE III: Colocando a solu√ß√£o √† prova ","date":"2022-06-08","objectID":"/posts/bncc-classifier/:3:0","tags":["Tutorial","Data Science","NLP","Education"],"title":"Classificador BNCC","uri":"/posts/bncc-classifier/"},{"categories":["Projetos"],"content":"Interface Para implementa√ß√£o da solu√ß√£o como um todo, constru√≠mos uma API (Application Programming Interface), que foi desenvolvida utilizando FastAPI, um framework em python. Essa API, funciona para servir o modelo de classifica√ß√£o para quem desejar utiliz√°-lo. Como inputs, a API vai inicialmente receber apenas texto, com possibilidade de aumento de complexidade para receber tamb√©m imagens ou extra√ß√£o de texto de documentos words em etapas futuras deste trabalho. E como outputs, retornaremos ao usu√°rio os mais prov√°veis c√≥digos da BNCC para aquela quest√£o. O usu√°rio final, o profissional de educa√ß√£o, ir√° interagir com a API atrav√©s de uma tela interativa criada a partir do framework Streamlit. A imagem a seguir demonstra esta interface. Este frontend tem o objetivo de ser pr√°tico para o usu√°rio, consistindo em um campo onde o usu√°rio pode inserir a quest√£o que deseja analisar e teclar ‚ÄúEnter‚Äù para que esta quest√£o seja carregada no modelo. Assim, aparecer√° um bot√£o Predict! que expor√° as classes mais prov√°veis para o Componente Curricular e Etapa do Ensino, como apresentado na imagem anterior para a quest√£o de exemplo, sendo o componente curricular F√≠sica e a etapa do ensino Ensino M√©rico e Pr√©-Vestibular. ","date":"2022-06-08","objectID":"/posts/bncc-classifier/:3:1","tags":["Tutorial","Data Science","NLP","Education"],"title":"Classificador BNCC","uri":"/posts/bncc-classifier/"},{"categories":["Projetos"],"content":"Implica√ß√µes e pr√≥ximos passos A solu√ß√£o de dados proposta foi desenhada para classificar quest√µes de Ensino Fundamental I e II e Ensino M√©dio quanto √†s normas da BNCC. Para isso, uma base de dados previamente classificada foi utilizada no treinameto. Visto que essa classifica√ß√£o foi realizada utilizando quest√µes escritas na L√≠ngua Portuguesa (com exce√ß√£o das quest√µes em ingl√™s) e seguindo a norma culta, algumas instru√ß√µes e premissas emergem destes fatos: ","date":"2022-06-08","objectID":"/posts/bncc-classifier/:4:0","tags":["Tutorial","Data Science","NLP","Education"],"title":"Classificador BNCC","uri":"/posts/bncc-classifier/"},{"categories":["Projetos"],"content":"Instru√ß√µes Para a utiliza√ß√£o eficiente desta solu√ß√£o de dados, √© recomendada a utiliza√ß√£o de quest√µes estruturadas na L√≠ngua Portugesa, que contemplem conte√∫dos abordados nas etapas de ensino comentadas anteriormente e que estejam escritas seguindo a norma culta. A forma de utiliza√ß√£o desta solu√ß√£o √© pela inser√ß√£o de quest√µes, individualmente, no campo de texto da interface da solu√ß√£o. A solu√ß√£o n√£o contempla, at√© o momento, a classifica√ß√£o de m√∫ltiplas quest√µes ao mesmo tempo. O n√£o cumprimento dessas instru√ß√µes pode ocasionar em falhas de previs√£o al√©m das relacionadas √†s m√©tricas do modelo, mas devido √† m√° utiliza√ß√£o da solu√ß√£o, como por exemplo devido a quest√µes que de assuntos que n√£o s√£o contemplados nos curr√≠culos escolares ou pela utiliza√ß√£o de linguagem cotidiana, como g√≠rias e express√µes idiom√°ticas. ","date":"2022-06-08","objectID":"/posts/bncc-classifier/:4:1","tags":["Tutorial","Data Science","NLP","Education"],"title":"Classificador BNCC","uri":"/posts/bncc-classifier/"},{"categories":["Projetos"],"content":"Implica√ß√µes Analisando criticamente os impactos dessa solu√ß√£o e externalidades que possam afetar as mesmas, principalmente devido a natureza dos dados coletados, os seguintes pontos est√£o associados ao classificador BNCC: Impacto positivo sobre ganho de efici√™ncia no cumprimento da BNCC pelos profissionais da educa√ß√£o, sejam professores ou profissionais atrelados √† gest√£o pedag√≥gica de alguma institui√ß√£o de ensino. Ganho de efici√™ncia por parte de EdTechs que prestam servi√ßo de oferecimento de quest√µes a institui√ß√µes de ensino, possibilitando uma classifica√ß√£o pr√©via desse banco de quest√µes quanto aos c√≥digos da BNCC. Um poss√≠vel impacto negativo seria a desaloca√ß√£o de profissionais que antes eram respons√°veis pela classifica√ß√£o de quest√µes caso a solu√ß√£o de dados seja empregada para este prop√≥sito. Por√©m, como uma alternativa para remediar este problema √© recomendado que estes profissionais sejam direcionados para a cria√ß√£o de novas quest√µes para alimenta√ß√£o do banco de quest√µes e supervis√£o da classifica√ß√£o gerada pela solu√ß√£o de dados. ","date":"2022-06-08","objectID":"/posts/bncc-classifier/:4:2","tags":["Tutorial","Data Science","NLP","Education"],"title":"Classificador BNCC","uri":"/posts/bncc-classifier/"},{"categories":["Projetos"],"content":"Poss√≠veis fontes de vi√©s Dado que a solu√ß√£o foi desenvolvida utilizando uma base de quest√µes previamente classificadas por professores que comp√µem um f√≥rum espec√≠fico (forum da empresa Studos), os dados possuem o vi√©s de que a classifica√ß√£o depende da habilidade destes professores em classificar as quest√µes e, no caso de uma futura classifica√ß√£o da compet√™ncia espec√≠fica (classificador 4) que esta quest√£o aborda, h√° um vi√©s da subjetividade do tema. Analisando este √∫ltimo fato criticamente, dada a prov√°vel subjetividade da compet√™ncia abordada na quest√£o e que essa classifica√ß√£o por parte dos professores pode ter influ√™ncia de vari√°veis que descrevem os professores que o modelo n√£o controla, como por exemplo vari√°veis demogr√°ficas, n√≠vel de forma√ß√£o, etnia, fonte de renda, quantidade de bens, n√≠vel de forma√ß√£o dos seus pais, entre outras. Diante disso, levanta-se o seguinte questionamento: ser√° que a classifica√ß√£o de quest√µes, por parte de professores, ser√° sempre igual? Ser√° que professores com diferentes condi√ß√µes de vida e de diferentes contextos classificar√£o as quest√µes da mesma forma, ou existe uma margem de erro atrelada a estas vari√°veis n√£o controladas, que explicam o contexto dos professores? Al√©m disso, dada essa prov√°vel diferen√ßa de classifica√ß√£o, ser√° que a BNCC √© a melhor forma de se abordar uma estrutura√ß√£o curricular a n√≠vel de ensino b√°sico? Ou a fonte do problema da educa√ß√£o no pa√≠s √© outra, que n√£o o curr√≠culo em si, como por exemplo o incentivo salarial aos professores? ","date":"2022-06-08","objectID":"/posts/bncc-classifier/:4:3","tags":["Tutorial","Data Science","NLP","Education"],"title":"Classificador BNCC","uri":"/posts/bncc-classifier/"},{"categories":["Projetos"],"content":"Conclus√£o Como conclus√µes, o grupo entende que √© poss√≠vel utilizar solu√ß√µes de Machine Learning com fins de aplica√ß√£o na educa√ß√£o. E mais especificamente voltado a emprego de Processamento de Linguagem Natural na classifica√ß√£o de textos segundo um target que se deseja estudar, no nosso caso os c√≥digos da BNCC. Durante o desenvolvimento da solu√ß√£o, percebemos o que existem diversas oportunidades de emprego de Machine Learning para educa√ß√£o. No contexto de quest√µes e avalia√ß√µes, al√©m da classifica√ß√£o de quest√µes nos c√≥digos da BNCC, uma poss√≠vel solu√ß√£o seria a partir de c√≥digos ou de compet√™ncias que se deseja trabalhar no ensino dos alunos, uma solu√ß√£o baseada em dados recomendaria uma lista de quest√µes que contemple estes c√≥digos ou compet√™ncias desejadas. Isso tornaria poss√≠vel os dois cen√°rios, tanto classificar quanto recomendar quest√µes segundo a BNCC. Como pr√≥ximos passos para este projeto, o grupo visa continuar com a obten√ß√£o de dados que viabilize o desenvolvimento dos classificadores 2 e 4, complementando assim com as demais partes do c√≥digo BNCC, trazendo a classifica√ß√£o completa. Diante disso, h√° a etapa de atualiza√ß√£o da produtiza√ß√£o desta solu√ß√£o de dados e atualiza√ß√£o da interface no Streamlit. Um outro projeto de interesse da equipe √© a recomenda√ß√£o de quest√µes baseada no input do c√≥digo BNCC ou compet√™ncia/habilidade que se deseja trabalhar. O grupo agradece seu tempo e leitura! ","date":"2022-06-08","objectID":"/posts/bncc-classifier/:5:0","tags":["Tutorial","Data Science","NLP","Education"],"title":"Classificador BNCC","uri":"/posts/bncc-classifier/"},{"categories":["Tutorials"],"content":"O intuito desse post √© compartilhar uma solu√ß√£o simples que resolvou um problema complexo que est√°vamos enfrentando em de nossos produtos aqui na Studos, a Leitora de Gabaritos. Dessa forma, outros que estejam com problemas similares possam talvez ter um ponto de vista diferente na resolu√ß√£o do problema. ","date":"2022-01-26","objectID":"/posts/image-cleaning/:0:0","tags":["Tutorial","Data Science","Vis√£o Computacional"],"title":"Computer Vision","uri":"/posts/image-cleaning/"},{"categories":["Tutorials"],"content":"O que √© a leitora de gabaritos? Dando um pouco de contexto sobre o produto, a Leitora de Gabaritos √© uma API desenvolvida utilizando (principalmente) a seguinte stack: FastAPI Celery ReportLab OpenCV RabbitMQ Redis Docker Kubernets Essa API √© respons√°vel por automatizar a cria√ß√£o e a leitura de cart√µes respostas. Utilizando ativamente o OpenCV nas etapas de leitura. Algumas features do ponto de vista t√©cnico da leitora, envolvem: Extra√ß√£o do qrcode da imagem Rota√ß√£o de imagens em diversas orienta√ß√µes Identifica√ß√£o de shapes Recorte Extra√ß√£o de segmentos de leitura Extra√ß√£o das marca√ß√µes Esses segmentos, no nosso caso, s√£o as regi√µes: das marca√ß√µes dos itens, do c√≥digo de matr√≠cula, das quest√µes optativas e do modelo de prova. As regi√µes amarelas da imagem abaixo representa melhor os segmentos. ","date":"2022-01-26","objectID":"/posts/image-cleaning/:1:0","tags":["Tutorial","Data Science","Vis√£o Computacional"],"title":"Computer Vision","uri":"/posts/image-cleaning/"},{"categories":["Tutorials"],"content":"Problema O problema que est√°vamos enfrentando era justamente na leitura desses segmentos. Mais especificamente, na etapa extra√ß√£o das marca√ß√µes, onde √© necess√°rio que todas as marca√ß√µes sejam extra√≠das sem trazer nenhum res√≠duo do gabarito. √â nesse ponto que nos deparamos com a complexidade do problema, pois a quantidade de vari√°veis envolvidas no processo √© absurda, e nem sempre consegu√≠amos fazer a melhor separa√ß√£o poss√≠vel, impactando na ponta e gerando insatisfa√ß√£o do usu√°rio. Essa variabilidade de imagens, conta com: ‚Ä¶imagens de qualquer faixa de DPI, imagens coloridas, escala de cinza e preto e branco, canetas pretas e azuis, pouca luz, sombra e ru√≠dos ao longo da imagem, imagens tiradas com foto, imagens digitalizadas com diferentes scanners‚Ä¶ O ponto espec√≠fico onde o OpenCV entra para realizar a transforma√ß√£o que precisamos √©, mostrado abaixo no c√≥digo: img_black_and_white = cv.threshold( img_warp_gray, thresh_param, 255, cv.THRESH_BINARY)[1] img_threshold = cv.threshold(img_black_and_white, 215, 350, cv.THRESH_BINARY_INV)[1] Veja que, como input para a fun√ß√£o cv.threshold, precisamos fornecer uma imagem em escala de cinza e um par√¢metro de threshold que ser√° utilizado como um ponto de corte. √â justamente esse threshold que precisava ser otimizado. Antes dessa solu√ß√£o, realiz√°vamos dois checks: a imagem √© colorida, ou escala de cinza? e a imagem tem caneta azul ou preta?. Com base nos outputs, ajust√°vamos o threshold de acordo. Por√©m, tal ajuste n√£o funcionava 100% das vezes, e algumas imagens estavam com a leitura impossibilitada justamente por n√£o conseguir ajustar o threshold. Veja abaixo um caso onde o ajuste errado do threshold leva a uma leitura incorreta. Com a aplica√ß√£o desse threshold, poucas marca√ß√µes seriam de fato lidas pela aplica√ß√£o. ","date":"2022-01-26","objectID":"/posts/image-cleaning/:1:1","tags":["Tutorial","Data Science","Vis√£o Computacional"],"title":"Computer Vision","uri":"/posts/image-cleaning/"},{"categories":["Tutorials"],"content":"Solu√ß√£o Sendo assim, tivemos a ideia de buscar por um padr√£o entre as diferentes imagens que populavam nosso banco. Coletamos 200 imagens das mais diversas poss√≠vel e come√ßamos olhando para a distribui√ß√£o de pixels. Nosso objetivo nesse primeiro momento era conseguir definir uma hip√≥tese mais acertiva de como abordar o problema de forma mais generaliz√°vel poss√≠vel. Essa √© a distribui√ß√£o dos pixels para o segmento do cart√£o apresentado acima. Quanto mais valores em dire√ß√£o ao 255, mais marcado est√° esse cart√£o. Valores tendendo ao 0, est√£o relacionados ao preto. ","date":"2022-01-26","objectID":"/posts/image-cleaning/:2:0","tags":["Tutorial","Data Science","Vis√£o Computacional"],"title":"Computer Vision","uri":"/posts/image-cleaning/"},{"categories":["Tutorials"],"content":"Distribui√ß√£o de pixels Ao analisar a distribui√ß√£o de pixels, vimos que as imagens similares possuem distribui√ß√µes de pixels bem caracter√≠sticas. E que todas possuem uma calda longa a esquerda com um ponto onde a distribui√ß√£o come√ßa a ‚Äúsubir‚Äù (aumento no valor de pixel). Imagem digitalizada em preto e branco Imagem em escala de cinza Imagem colorida de alto dpi Imagem colorida de baixo dpi Esse ponto de inflex√£o da curva nos chamou a aten√ß√£o, por ser uma regi√£o que estava pr√≥xima do threshold que j√° v√≠nhamos utilizando. Verificamos visualmente para todas as imagens e o padr√£o se repetiu. Dae em diante conseguimos definir melhor nossa abordagem e o que de fato gostar√≠amos de coletar das distribui√ß√µes: ‚ÄúComo identificar automaticamente esse ponto de inflex√£o da distribui√ß√£o de pixel, para qualquer distribui√ß√£o?‚Äù ","date":"2022-01-26","objectID":"/posts/image-cleaning/:2:1","tags":["Tutorial","Data Science","Vis√£o Computacional"],"title":"Computer Vision","uri":"/posts/image-cleaning/"},{"categories":["Tutorials"],"content":"M√©todo utilizado Antes de chegar na t√©cnica que resolveu o problema, algumas outras foram testadas, algumas mais complexas e outras mais simples, mas nenhuma generalizou do jeito que precis√°vamos. Nossa hip√≥tese aqui foi a seguinte: Considerando que a maior parte da distribui√ß√£o, com pixels maiores, correspondem as marca√ß√µes, a zona mais escura (regi√£o da calda longa da distribui√ß√£o) corresponde ao gabarito. Dessa forma, se considerarmos uma porcentagem e eliminarmos parte da calda, seria poss√≠vel saber onde est√° o ponto √≥timo de corte. Para melhor visualizar essa abordagem, criamos um ECDF das distribui√ß√µes dos pixels e tra√ßamos uma linha horizontal na porcentagem de pontos que gostar√≠amos de remover da distribui√ß√£o. Dae em diante, para testar se realmente esse m√©todo funcionava, o que precis√°vamos fazer era encontrar os pontos de interserc√ß√£o da reta na horizontal com a distribui√ß√£o, coletar o m√°ximo e retornar como ponto de corte. A implementa√ß√£o est√° no c√≥digo abaixo. def get_param(img_warp_gray): # x and y for the ecdf plot x = np.sort(img_warp_gray.ravel()) y = np.arange(1, len(x) + 1) / len(x) # ecdf dataframe ecdf_plot = pd.DataFrame({\"img\": x, \"prop\": y}) # get intersection values on 5% of the distribution thresh_params = ecdf_plot[round(ecdf_plot[\"prop\"], 2) == 0.05][\"img\"].values thresh_params_max = max(thresh_params) return thresh_params_max # running def main(): img = cv.imread(f\"src/files/snippets/pic_7.png\") img_warp_gray = cv.cvtColor(img, cv.COLOR_BGR2GRAY) try: thresh_param = get_param(img_warp_gray) img_threshold = image_transformation(thresh_param, img) except ValueError: thresh_param = 0 img_threshold = image_transformation(thresh_param, img) # plotting the ecdf, 0.05 horizontal line, image before and image after the # transformation plt.subplot(3, 1, 1) _ = sns.ecdfplot(x=img_warp_gray.ravel()) plt.hlines(xmin=0, xmax=255, y=0.05) plt.subplot(3, 1, 2) plt.imshow(img_warp_gray, cmap=\"gray\") plt.subplot(3, 1, 3) plt.imshow(img_threshold, cmap=\"gray\") plt.show() if __name__ == \"__main__\": main() Espantosamente, nessa linha de 5%, foi justamente a regi√£o de inflex√£o da distribui√ß√£o. E aplicando o par√¢metro que a fun√ß√£o get_param() retornou, o resultado da transforma√ß√£o foi o seguinte: Ap√≥s encontrar esse pontos, rodamos o script para as 200 imagens que est√°vamos utilizando como teste e o resulado foi 100% de remo√ß√£o do gabarito das marca√ß√µes. Dessa forma, agora conseguimos identificar as marca√ß√µes, em qualquer tipo de imagem, qualidade e cor de caneta marcada, sem propagar para as pr√≥ximas etapas de leitura os ru√≠dos do cart√£o. Caso queira conferir o c√≥digo e as imagens testes, elas est√£o nesse reposit√≥rio. ","date":"2022-01-26","objectID":"/posts/image-cleaning/:2:2","tags":["Tutorial","Data Science","Vis√£o Computacional"],"title":"Computer Vision","uri":"/posts/image-cleaning/"},{"categories":["Tutorials"],"content":" Fala galera, tudo certo com voc√™s? Se voc√™ √© um Cientista/Analista de dados, ou curte utilizar python para realizar diferentes an√°lises em seu dataset, certamente j√° utilizou a biblioteca Pandas. Pandas √© uma biblioteca open-source para estrutura√ß√£o e an√°lise de dados, seus comandos se assemelham muitas vezes ao SQL, por√©m sua API traz um conjunto maior opera√ß√µes, maior robustez e se executado de forma correta, mais parformance. Nesse tutorial estou me baseando em um minicurso do DataCamp Writing Efficient Code with pandas. Como nem todos tem acesso a esse material, pode ser de grande valia compartilhar esse conhecimento com a comunidade. Bom, mas sem enrola√ß√£o, vamos tratar aqui de 4 principais t√≥picos: Parte 1: Selecionando colunas e linhas de forma eficiente. Parte 2: Substituindo valores em colunas do DataFrame. Parte 3: Iterando no dataset de forma eficiente. Parte 4: Manipula√ß√£o de dados com o .groupby, .transform e .filter. Todos os t√≥picos ser√£o abordados utilizando tarefas cotidianas de manipula√ß√£o de dados. Estaremos utilizando uma base de dados p√∫blica oriunda do kaggle, que s√£o os dados das Olimp√≠adas de Tokyo 2021. Voc√™ pode realizar o download dos arquivos pelo site no link abaixo: Kaggle: https://www.kaggle.com/arjunprasadsarkhel/2021-olympics-in-tokyo Como bibliotecas estarei utilizando: numpy, pandas, pyjanitor e time. Certifique-se de ter todas elas instaladas no seu ambiente. import time import numpy as np import pandas as pd from janitor import clean_names # importing and cleaning the data medals_df = pd.read_excel(\"Data/Medals.xlsx\").clean_names() # tidying the data medals_df.drop([\"total\", \"rank_by_total\"], axis=1, inplace=True) medals_reshaped_df = medals_df.melt( id_vars=[\"rank\", \"team_noc\"], value_vars=[\"gold\", \"silver\", \"bronze\"], var_name=\"medals\", value_name=\"qtt\", ) Ap√≥s importar os dados eu realizei algumas transforma√ß√µes: .clean_names: √© um m√©todo do pyjanitor, que limpa o nome das colunas automaticamente. .melt: √© um pivoteamento nos dados para o formato longer, melhor para an√°lise. .drop: removi algumas colunas que n√£o s√£o necess√°rias nesse momento. Como iremos calcular um comparativo v√°rias vezes durante o tutorial, eu criei uma fun√ß√£o para nos ajudar a calcular o quanto um m√©todo √© mais r√°pido que o outro. Aqui entramos como input, com os valores do tempos gastos por cada m√©todo e depois calculamos o quanto um √© maior que o outro. # helper function to calculate pct diff between the times def calculate_how_much_diff(method_1: float, method_2: float) -\u003e str: diff = method_2 / method_1 if diff \u003e 1: ans = f\"M√©todo 1 √© {round(diff*100, 2)}% vezes mais r√°pido que o M√©todo 2\" elif diff \u003c 1: new_diff = method_1 / method_2 ans = f\"M√©todo 2 √© {round(new_diff*100, 2)}% vezes mais r√°pido que o M√©todo 1\" return ans Agora estamos prontos para come√ßar!! ","date":"2021-10-24","objectID":"/posts/efficient-pandas/:0:0","tags":["Tutorial","Data Science","Pandas"],"title":"Efficient Pandas","uri":"/posts/efficient-pandas/"},{"categories":["Tutorials"],"content":"Parte I: Selecionando colunas e linhas de forma eficiente ","date":"2021-10-24","objectID":"/posts/efficient-pandas/:1:0","tags":["Tutorial","Data Science","Pandas"],"title":"Efficient Pandas","uri":"/posts/efficient-pandas/"},{"categories":["Tutorials"],"content":".iloc e .loc Essa √© uma tarefa bem comum utilizada com DataFrames do Pandas, muitas vezes precisamos selecionar apenas uma por√ß√£o dos dados, seja por colunas, seja por linhas. √â ae que entra a utiliza√ß√£o dos m√©todos .iloc e .loc. .iloc: Localizador num√©rico .loc: Localizador nominal Existe diferen√ßa em utilizar um ou outro? Vamos decobrir: # timing how long will run each of those: .iloc and .loc # loc time start_time = time.time() medals_reshaped_df.loc[:, [\"team_noc\", \"medals\", \"qtt\"]] delta_loc = time.time() - start_time # iloc time start_time = time.time() medals_reshaped_df.iloc[:, [1, 2, 3]] delta_iloc = time.time() - start_time # who is faster? calculate_how_much_diff(method_1=delta_loc, method_2=delta_iloc) 'M√©todo 2 √© 219.69% vezes mais r√°pido que o M√©todo 1' # loc time start_time = time.time() medals_reshaped_df.loc[1:30, :] delta_loc = time.time() - start_time # iloc time start_time = time.time() medals_reshaped_df.iloc[1:30, :] delta_iloc = time.time() - start_time # who is faster? calculate_how_much_diff(method_1=delta_loc, method_2=delta_iloc) 'M√©todo 2 √© 169.56% vezes mais r√°pido que o M√©todo 1' Pelos resultados acima, conseguimos ver que em ambos os casos, utilizar o .iloc √© muito mais eficiente para selecionar tanto linhas quanto colunas de um DataFrame do Pandas. Por√©m, √© muito comum tamb√©m utilizarmos apenas uma lista com o nome das colunas desejadas do dataset, vamos verificar ent√£o se o .iloc continua vitorioso nessa compara√ß√£o: # iloc time start_time = time.time() medals_reshaped_df.iloc[:, 1:4] delta_iloc = time.time() - start_time # directly selecting columns start_time = time.time() medals_reshaped_df[[\"team_noc\", \"medals\", \"qtt\"]] delta_direct = time.time() - start_time # who is faster? calculate_how_much_diff(method_1=delta_iloc, method_2=delta_direct) 'M√©todo 1 √© 179.69% vezes mais r√°pido que o M√©todo 2' Como vimos, o .iloc ainda se mostra vitorioso nesta compara√ß√£o. Neste t√≥pico n√≥s vemos uma diverg√™ncia entre o que est√° no material do DataCamp e o que observamos em nosso c√≥digo, pois l√° o m√©todo de selecionar as colunas diretamente √© mais eficiente. Como toda biblioteca sofre atualiza√ß√µes e otimiza√ß√µes ao longo do tempo, √© poss√≠vel que o m√©todo .iloc tenha sido otimizado desde que o curso do DataCamp foi lan√ßado. ","date":"2021-10-24","objectID":"/posts/efficient-pandas/:1:1","tags":["Tutorial","Data Science","Pandas"],"title":"Efficient Pandas","uri":"/posts/efficient-pandas/"},{"categories":["Tutorials"],"content":"Amostragem aleat√≥ria Outra tarefa bem comum dentro do dia-a-dia de Cientistas e Analistas de dados √© a amostragem aleat√≥ria do nosso dataset, tal tarefa pode ter diferentes objetivos: diminuir o tamanho do dataset original valida√ß√£o de alguma t√©cnica separa√ß√£o entre dados de teste e treino Para isso, podemos prosseguir por duas formas diferentes com o Pandas: utilizar o m√≥dulo do numpy de gera√ß√£o de n√∫meros rand√¥micos e aplicar ao DataFrame utilizar o m√©todo .sample do Pandas Vamos checar quem √© mais eficiente: # numpy random integers start_time = time.time() medals_reshaped_df.iloc[np.random.randint(low=0, high=medals_reshaped_df.shape[0], size=50), :] delta_numpy = time.time() - start_time # using .sample start_time = time.time() medals_reshaped_df.sample(50, axis=0) delta_sample = time.time() - start_time # who is faster? calculate_how_much_diff(method_1=delta_numpy, method_2=delta_sample) 'M√©todo 2 √© 192.69% vezes mais r√°pido que o M√©todo 1' Veja que o m√©todo do Pandas √© muito mais efetivo, ent√£o, prefira utilizar o .sample caso precise de uma amostra aleat√≥ria a n√≠vel de linhas do seu dataset. Caso precise de selecionar amostras aleat√≥rias a n√≠vel de colunas (menos comum), tamb√©m podemos adaptar o que foi feito acima e verificar se o .sample continua vencendor. # numpy random integers start_time = time.time() medals_reshaped_df.iloc[:, np.random.randint(low=0, high=medals_reshaped_df.shape[1], size=3)] delta_numpy = time.time() - start_time # using .sample start_time = time.time() medals_reshaped_df.sample(3, axis=1) # adjusting the axis = 1 for columns delta_sample = time.time() - start_time # who is faster? calculate_how_much_diff(method_1=delta_numpy, method_2=delta_sample) 'M√©todo 2 √© 188.66% vezes mais r√°pido que o M√©todo 1' Ainda assim, temos o .sample como vencedor! Com isso n√≥s finalizamos a Parte I dessa s√©rie de tutoriais, e em resumo temos o seguinte: .iloc √© mais r√°pido que .loc .iloc √© mais r√°pido que selecionar diretamente as colunas .sample √© mais r√°pido utilizar numpy para gerar um array de n√∫meros aleat√≥rios Vamos √† segunda parte!! ","date":"2021-10-24","objectID":"/posts/efficient-pandas/:1:2","tags":["Tutorial","Data Science","Pandas"],"title":"Efficient Pandas","uri":"/posts/efficient-pandas/"},{"categories":["Tutorials"],"content":"Parte II: Substituindo valores em colunas do dataset ","date":"2021-10-24","objectID":"/posts/efficient-pandas/:2:0","tags":["Tutorial","Data Science","Pandas"],"title":"Efficient Pandas","uri":"/posts/efficient-pandas/"},{"categories":["Tutorials"],"content":"Substituindo valores √∫nicos utilizando o .replace Voc√™ provavelmente deve ter cruzado com situa√ß√µes onde precisou substituir valores de algumas colunas baseando-se em algum condicional, algumas linguagens de programa√ß√£o chamam esse padr√£o de switch case, em python n√£o possu√≠amos essa feature anterior a vers√£o 3.10 (rec√©m lan√ßada). No Pandas, esse padr√£o j√° existia no m√©todo .replace, sendo assim, vamos analisar qual √© forma mais efetiva de realizar tal tarefa. # pure Pandas medals_reshaped_replace_pd_df = medals_reshaped_df.copy() start_time = time.time() medals_reshaped_replace_pd_df.medals.loc[medals_reshaped_replace_pd_df.medals==\"gold\"] = 'Gold' delta_pure_pandas = time.time() - start_time # using .replace medals_reshaped_replace_df = medals_reshaped_df.copy() start_time = time.time() medals_reshaped_replace_df[\"medals\"].replace(\"gold\", \"Gold\") # adjusting the axis = 1 for columns delta_replace = time.time() - start_time # who is faster? calculate_how_much_diff(method_1=delta_pure_pandas, method_2=delta_replace) 'M√©todo 2 √© 204.11% vezes mais r√°pido que o M√©todo 1' Como podemos ver, utilizar o .replace √© incrivelmente mais r√°pido e pyth√¥nico que apenas fazer isso com o Pandas b√°sico. ","date":"2021-10-24","objectID":"/posts/efficient-pandas/:2:1","tags":["Tutorial","Data Science","Pandas"],"title":"Efficient Pandas","uri":"/posts/efficient-pandas/"},{"categories":["Tutorials"],"content":"Substituindo m√∫ltiplos valores com listas e dicion√°rios Uma outra compara√ß√£o que podemos fazer √© em rela√ß√£o a altera√ß√£o de m√∫ltiplos valores numa columa de um DataFrame, e para isso o .replace nos oferece o recurso de utilizar listas e dicion√°rios. Os quais veremos agora. Antes de realizar os comparativos, eu montei um esquema que nos permite visualizar como funciona a sintaxe para o .replace com listas e dicin√°rios Utilizando listas Utilizando dicion√°rios # pure Pandas medals_reshaped_replace_pd_df = medals_reshaped_df.copy() start_time = time.time() medals_reshaped_replace_pd_df[\"rank\"].loc[ (medals_reshaped_replace_pd_df[\"rank\"] == 1) | (medals_reshaped_replace_pd_df[\"rank\"] == 2) | (medals_reshaped_replace_pd_df[\"rank\"] == 3) ] = \"TOP 3\" medals_reshaped_replace_pd_df[\"rank\"].loc[ (medals_reshaped_replace_pd_df[\"rank\"] != \"TOP 3\") ] = \"OUTRAS\" delta_pure_pandas = time.time() - start_time # using .replace medals_reshaped_replace_df = medals_reshaped_df.copy() start_time = time.time() max_rank = medals_reshaped_replace_df[\"rank\"].max() medals_reshaped_replace_df[\"rank\"].replace( [[1, 2, 3], [range(4, max_rank + 1)]], [\"TOP 3\", \"OUTRAS\"], inplace=True ) delta_replace = time.time() - start_time # who is faster? calculate_how_much_diff(method_1=delta_pure_pandas, method_2=delta_replace) 'M√©todo 2 √© 136.03% vezes mais r√°pido que o M√©todo 1' Novamente, obtemos o mesmo resultado e de forma muito mais eficiente ao utilizar o .replace do pandas, mesmo quando queremos substituir valores baseando-se em mais de uma condi√ß√£o ao dataset. Uma outra estrutura de dados que o Pandas nos permite usar √© o dicion√°rio, vamos realizar o comparativo: # pure Pandas medals_reshaped_replace_df = medals_reshaped_df.copy() start_time = time.time() medals_reshaped_replace_df[\"medals\"].replace(\"gold\", \"GOLD\", inplace=True) medals_reshaped_replace_df[\"medals\"].replace(\"silver\", \"Silver\", inplace=True) medals_reshaped_replace_df[\"medals\"].replace(\"bronze\", \"Bronze\", inplace=True) delta_pure_pandas = time.time() - start_time # using .replace medals_reshaped_replace_df = medals_reshaped_df.copy() start_time = time.time() medals_reshaped_replace_df[\"medals\"].replace( {\"gold\": \"Gold\", \"silver\": \"Silver\", \"bronze\": \"Bronze\"}, inplace=True ) delta_replace = time.time() - start_time # who is faster? calculate_how_much_diff(method_1=delta_pure_pandas, method_2=delta_replace) 'M√©todo 2 √© 171.75% vezes mais r√°pido que o M√©todo 1' Aqui vemos que at√© mesmo dentro do pr√≥prio m√©todo .replace √© poss√≠vel encontrar formas mais perform√°ticas de realizar uma terefa, que nesse caso √© por meio dos dicion√°rios. Com isso, finalizamos a segunda parte do tutorial. Resumindo o que vimos at√© aqui, temos: .replace √© muito mais efetivo que opera√ß√µes ‚Äúpuras‚Äù do Pandas .replace utilizando dicion√°rios √© mais efetivo que utilizando listas. Fique ligado para o pr√≥ximo post, onde ser√° abordado a Parte III e a Parte IV. ","date":"2021-10-24","objectID":"/posts/efficient-pandas/:2:2","tags":["Tutorial","Data Science","Pandas"],"title":"Efficient Pandas","uri":"/posts/efficient-pandas/"},{"categories":["Tutorials"],"content":"Nos Cap√≠tulos Anteriores ‚Ä¶ Como dito anteriormente, hoje vamos configurar o SQLAlchemy!! Essa √© a parte 5 do nosso projeto do EconoWallet e se voc√™ quiser verificar o que j√° fizemos at√© o momento, acesse os links abaixo: Parte 1 Parte 2 Parte 3 Parte 4 Parte 5 Sem enrola√ß√£o, vamos logo ao que interessa!!! ","date":"2021-08-01","objectID":"/posts/econowallet-fastapi-pt5/:1:0","tags":["RESTFull API","Web Development","Backend","Asynchronous","FastAPI","Docker","SQLAlchemy"],"title":"Criando uma API Pronta para Produ√ß√£o com FastAPI - PT.5","uri":"/posts/econowallet-fastapi-pt5/"},{"categories":["Tutorials"],"content":"Reorganizando o projeto Antes de configurar o ORM, vamos ajustar algumas coisas no nosso diret√≥rio. Como estamos seguindo para um est√°gio mais maduro da aplica√ß√£o e que voc√™ j√° compreende o b√°sico de configura√ß√£o do FastAPI, precisaremos agora separar melhor as compet√™ncias dentro do projeto. mkdir project/app/api mkdir project/app/database mkdir project/app/models touch project/app/api/__init__.py touch project/app/database/__init__.py touch project/app/models/__init__.py touch project/app/api/status.py Veja que agora n√≥s temos alguns diret√≥rios novos‚ú® que ir√£o comportar diferentes l√≥gicas no nosso projeto: api: todas as nossas rotas da api. database: toda a l√≥gica de configura√ß√£o do SQLAlchemy. models: toda a especifica√ß√£o de como dever√£o ser nossas tabelas. N√≥s tamb√©m adicionamos um arquivo chamado status.py, esse ir√° conter a rota de status da nossa aplica√ß√£o, que antes estava no nosso arquivo main.py. Veja como est√° nosso main.py e nosso status.py: # main.py import logging from fastapi import FastAPI from app.api import status log = logging.getLogger(\"uvicorn\") def create_application() -\u003e FastAPI: application = FastAPI() application.include_router( status.router, tags=[\"Hello\"], prefix=\"/api/v1\" ) return application app = create_application() @app.on_event(\"startup\") async def startup_event(): log.info(\"Starting up...\") Aqui n√≥s n√£o configuramos mais diretamente as rotas no main.py, elas agora ficam em diret√≥rios separados, e sempre na inicializa√ß√£o da aplica√ß√£o √© carregado todas as rotas ao chamar a fun√ß√£o create_application. Dentro dessa fun√ß√£o n√≥s adicionamos um include_router que faz algumas coisas interessantes: inclui todas as rotas de um determinado arquivo. adiciona uma tag para separar melhor no contexto de exibi√ß√£o no swager. adiciona um prefixo √† rota: eu normalmente utilizo /api/v1 pelo motivo de que fica f√°cil alterar para uma vers√£o 2 caso a mesma venha a existir. Um decorator muito legal do FastAPI √© @app.on_event, onde voc√™ pode configurar m√©todos que ir√£o rodar sempre que a aplica√ß√£o inicializar ou finalizar. Iremos adicionar mais coisas nesse event, mas por enquanto apenas estamos printando um log no console. # status.py import os from fastapi import APIRouter, Depends from app.config import Settings, get_settings router = APIRouter() APP_VERSION = os.getenv(\"APP_VERSION\", \"1.0.0\") @router.get(\"/status\") async def ping(settings: Settings = Depends(get_settings)): return { \"ping\": \"pong!\", \"version\": APP_VERSION, \"environment\": settings.environment, \"testing\": settings.testing } Veja que aqui n√≥s temos uma modifica√ß√£o da rota /ping para /status e ela √© definida por @router.get e n√£o mais @app.get. Eu adicionei tamb√©m uma vari√°vel chamada APP_VERSION que √© respons√°vel por manter atualizado o registro da verss√£o da aplica√ß√£o. Executamos assim nosso workflow de teste, para saber se tudo est√° funcionando: docker-compose down -v docker-compose up --build -d Ao acessar http://127.0.0.1:8004/docs/ voc√™ dever√° ver uma tela igual a de baixo: E nossa √°rvore de diret√≥rios deve estar como no seguinte snippet: . ‚îú‚îÄ‚îÄ docker-compose.yml ‚îú‚îÄ‚îÄ Dockerfile ‚îú‚îÄ‚îÄ Pipfile ‚îú‚îÄ‚îÄ Pipfile.lock ‚îú‚îÄ‚îÄ project ‚îÇ¬†‚îî‚îÄ‚îÄ app ‚îÇ¬†‚îú‚îÄ‚îÄ api ‚îÇ¬†‚îÇ¬†‚îú‚îÄ‚îÄ __init__.py ‚îÇ¬†‚îÇ¬†‚îî‚îÄ‚îÄ status.py ‚îÇ¬†‚îú‚îÄ‚îÄ config.py ‚îÇ¬†‚îú‚îÄ‚îÄ database ‚îÇ¬†‚îÇ¬†‚îî‚îÄ‚îÄ __init__.py ‚îÇ¬†‚îú‚îÄ‚îÄ __init__.py ‚îÇ¬†‚îú‚îÄ‚îÄ main.py ‚îÇ¬†‚îî‚îÄ‚îÄ models ‚îÇ¬†‚îî‚îÄ‚îÄ __init__.py ‚îî‚îÄ‚îÄ README.md ","date":"2021-08-01","objectID":"/posts/econowallet-fastapi-pt5/:2:0","tags":["RESTFull API","Web Development","Backend","Asynchronous","FastAPI","Docker","SQLAlchemy"],"title":"Criando uma API Pronta para Produ√ß√£o com FastAPI - PT.5","uri":"/posts/econowallet-fastapi-pt5/"},{"categories":["Tutorials"],"content":"Configurando o SQLAlchemy no Projeto No intuito de ter uma tabela inicial bem simples no nosso banco de dados, apenas para iniciar a configura√ß√£o do SQLAlchemy, vamos iniciar o processo criando um model. Como dito no post anterior, estamos no que estou chamando de primeiro cen√°rio do SQLAlchemy, onde precisamos da classe declarative_base antes de iniciar a configura√ß√£o do model. Antes de tudo, primeiro iremos instalar o SQLAlchemy: pipenv install sqlalchemy==1.4.20. Agora, vamos criar dois novos arquivos: touch project/app/database/modelbase.py touch project/app/models/register.py O modelbase.py √© onde eu normalmente coloco a declarative_base, sendo assim, √© desse arquivo que nossos models ir√£o buscar a classe m√£e. # modelbase.py from sqlalchemy.ext import declarative Base = declarative.declarative_base() Podemos agora criar o model. Um model √© uma abstra√ß√£o de tudo que voc√™ faria se estivesse criando uma tabela diretamente no banco, com a diferen√ßa de que aqui a gente tem uma abordagem python-like. Cada tipo de ORM tem uma sintaxe pr√≥pria, e no SQLAlchemy a gente tem a configura√ß√£o como no arquivo abaixo: # register.py from datetime import date from sqlalchemy import Column, Date, BigInteger, String from app.database.modelbase import Base class Register(Base): __tablename__ = \"registers\" id: int = Column(BigInteger, primary_key=True, autoincrement=True) products: str = Column(String(20), nullable=False) created_at: date = Column(Date, nullable=False, index=True) expire_at: date = Column(Date, nullable=False, index=True) Pronto, nosso primeiro model est√° criado. Como voc√™ pode ver, √© basicamente uma classe que herda da declarative_base, e nosso dever √© configurar como vai ser cada coluna da tabela. Diferentemente de outros ORMs como o django, aqui n√≥s precisamos explicitamente determinar a coluna de id. Para as outras colunas, n√≥s temos formato de string e data. Sendo que nenhuma delas podem ser atribu√≠dos valores nulos, ou seja, s√£o campos obrigat√≥rios da nossa tabela. Veja que eu j√° estou adicionando alguns indexes, que basicamente permitem que a consulta √† essas colunas seja feita de forma mais r√°pida. At√© agora n√≥s j√° temos alguns elementos do primeiro cen√°rio do SQLAlchemy, por√©m, para criar as tabelas no banco e expor uma session, n√≥s iremos precisar criar um novo arquivo: touch project/app/database/database_session.py. # database_session.py import logging import os from typing import Callable, Optional import sqlalchemy from sqlalchemy.orm import Session from app.database.modelbase import Base __factory: Optional[Callable[[], Session]] = None log = logging.getLogger(\"uvicorn\") def get_db() -\u003e Session: db = create_session() try: yield db finally: db.close() def global_init() -\u003e None: global __factory if __factory: return conn_str = str( os.environ.get(\"DATABASE_URL\", \"sqlite:///project/db/local_database.db\") ) log.info(\"Connecting to the database...\") engine = sqlalchemy.create_engine(conn_str, echo=False) __factory = sqlalchemy.orm.sessionmaker(bind=engine) from app.models.register import Register Base.metadata.create_all(engine) def create_session() -\u003e Session: global __factory if not __factory: raise Exception(\"You must call global_init() before using this method\") session: Session = __factory() session.expire_on_commit = False return session Calma, vou traduzir o que est√° acontecendo aqui: Criamos uma fun√ß√£o para inicializar uma vari√°vel global chamada __factory, essa √© respons√°vel por expor uma conex√£o com o banco para toda a aplica√ß√£o. Por meio do create_session() essa vari√°vel √© acessada e a session √© enfim estabelecida. Setamos uma forma padr√£o de acessar essa session por meio da fun√ß√£o get_db(), que exp√µe a session e garante que a mesma vai ser finalizada (finaly), aconte√ßa o que acontecer. Eu ainda n√£o comentei sobre vari√°veis de ambiente, mas vai ser algo que precisaremos fazer aqui. Uma vari√°vel de ambiente √© basicamente um valor atribu√≠do dinamicamente que pode afetar o modo como alguns processos ir√£o se com","date":"2021-08-01","objectID":"/posts/econowallet-fastapi-pt5/:3:0","tags":["RESTFull API","Web Development","Backend","Asynchronous","FastAPI","Docker","SQLAlchemy"],"title":"Criando uma API Pronta para Produ√ß√£o com FastAPI - PT.5","uri":"/posts/econowallet-fastapi-pt5/"},{"categories":["Tutorials"],"content":"Alerta Boas Pr√°ticas üö® Gostaria de pontuar aqui a import√¢ncia de utilizarmos vari√°veis de ambiente em nossos projetos. Normalmente projetos reais possuem informa√ß√µes sens√≠veis que pessoas externas ao projeto n√£o podem ter acesso, ou informa√ß√µes que precisam ser alteradas dinamicamente, como: Senha do banco Token de um bucket Token de alguma API de terceiro Senha de login em outro servi√ßo ‚Ä¶ √â extremamente indicado n√£o hard coded essas informa√ß√µes diretamente no c√≥digo. Ao inv√©s disso, que coloquemos as mesmas como vari√°veis de ambiente em um arquivo separado e oculto do reposit√≥rio onde voc√™ est√° versionando o seu projeto. Vamos ver ent√£o como fazer isso!! ","date":"2021-08-01","objectID":"/posts/econowallet-fastapi-pt5/:3:1","tags":["RESTFull API","Web Development","Backend","Asynchronous","FastAPI","Docker","SQLAlchemy"],"title":"Criando uma API Pronta para Produ√ß√£o com FastAPI - PT.5","uri":"/posts/econowallet-fastapi-pt5/"},{"categories":["Tutorials"],"content":"Configurando Vari√°veis de Ambiente Gra√ßas ao Docker, √© simples configurar vari√°veis de ambiente para nosso projeto. Primeiro, vamos criar alguns arquivos: mkdir env touch env/.dev Dentro do .dev √© onde voc√™ vai colocar todas as suas vari√°veis de ambiente: ENVIRONMENT=dev TESTING=0 DATABASE_URL=mysql://user:password@db:3306/econowallet SQL_DATABASE=econowallet SQL_USER=user SQL_PASSWORD=password SQL_HOST=db SQL_PORT=3306 E agora, atualizamos nosso .gitignore: # .gitignore __pycache__ # new env Show! Agora, localmente teremos como testar nossa aplica√ß√£o, e quando a mesma for funcionar em um ambiente de produ√ß√£o, essas vari√°veis de ambiente ser√£o substitu√≠das dinamicamente para funcionar com as configura√ß√µes de produ√ß√£o üòç. Por fim, dentro do docker-compose.yml n√≥s iremos apontar onde o arquivo de vari√°veis de ambiente est√° localizado: version: \"3.8\" services: web: build: . volumes: - .project:/usr/src/app ports: - 8004:8000 # new env_file: - env/.dev environment: - ENVIRONMENT=dev - TESTING=0 depends_on: - db db: image: mysql:8.0 command: --default-authentication-plugin=mysql_native_password restart: always volumes: - mysql_data:/var/lib/mysql_data/data/ environment: MYSQL_DATABASE: econowallet MYSQL_USER: user MYSQL_PASSWORD: password MYSQL_ROOT_PASSWORD: password ports: - 3320:3306 volumes: mysql_data: ","date":"2021-08-01","objectID":"/posts/econowallet-fastapi-pt5/:3:2","tags":["RESTFull API","Web Development","Backend","Asynchronous","FastAPI","Docker","SQLAlchemy"],"title":"Criando uma API Pronta para Produ√ß√£o com FastAPI - PT.5","uri":"/posts/econowallet-fastapi-pt5/"},{"categories":["Tutorials"],"content":"Inicializando em Conjunto com o SQLAlchemy Lembrando que precisamos inicializar uma sess√£o global por meio do global_init() que vimos anteriormente, vamos ent√£o fazer uso de uma feature do FastAPI que vimos anteriormente, @app.on_event(): import logging from fastapi import FastAPI from app.api import status from app.database.database_session import global_init log = logging.getLogger(\"uvicorn\") def create_application() -\u003e FastAPI: application = FastAPI() application.include_router( status.router, tags=[\"Hello\"], prefix=\"/api/v1\" ) return application app = create_application() # updated @app.on_event(\"startup\") async def startup_event(): log.info(\"Starting up...\") global_init() Sendo assim, sempre que o app inicializar, vamos iniciar as configura√ß√µes do SQLAlchemy em conjunto!! Estamos quaseee l√°! ","date":"2021-08-01","objectID":"/posts/econowallet-fastapi-pt5/:3:3","tags":["RESTFull API","Web Development","Backend","Asynchronous","FastAPI","Docker","SQLAlchemy"],"title":"Criando uma API Pronta para Produ√ß√£o com FastAPI - PT.5","uri":"/posts/econowallet-fastapi-pt5/"},{"categories":["Tutorials"],"content":"Bug do MySQL üêõ Se voc√™ tentar buildar o projeto, provavelmente ir√° se deparar com um problema onde o servi√ßo web n√£o ir√° conseguir inicializar pois n√£o vai ter encontrado um banco MySQL dispon√≠vel. Eu j√° busquei explica√ß√£o do por que disso acontecer, mesmo especificando o depends_on: db no docker-compose.yml o servi√ßo web tenta inicializar primeiro, e como n√£o encontra um banco online, temos uma mensagem de erro. Para driblar isso, vamos utilizar um pouquinho de conhecimento de bash e adicionar o que chamamos de entrypoint para garantir que nosso servi√ßo web apenas siga em frente ap√≥s conectar com o banco. Crie um arquivo chamado local-entrypoint.sh e copie o seguinte c√≥digo: #!/bin/bash # if any of the commands in your code fails for any reason, the entire script fails set -o errexit # fail exit if one of your pipe command fails set -o pipefail # exits if any of your variables is not set set -o nounset mysql_ready() { python \u003c\u003c END import sys import mysql.connector from mysql.connector import Error try: connection = mysql.connector.connect(host=\"${SQL_HOST}\", database=\"${SQL_DATABASE}\", user=\"${SQL_USER}\", password=\"${SQL_PASSWORD}\") except: sys.exit(-1) sys.exit(0) END } until mysql_ready; do \u003e\u00262 echo 'Waiting for Mysql to become available...' sleep 1 done \u003e\u00262 echo 'Mysql is available' uvicorn project.app.main:app --reload --workers 1 --host 0.0.0.0 --port 8000 Nesse script √© executado um loop que finaliza apenas quando o MySQL estiver dispon√≠vel. Precisamos agora apontar nosso Dockerfile para esse entrypoint. # Dockerfile FROM python:3.9.0-slim-buster WORKDIR /usr/src/app ENV PYTHONDONTWRITEBYTECODE 1 ENV PYTHONUNBUFFERED 1 RUN apt-get update \\ \u0026\u0026 apt-get -y install netcat gcc \\ # mysql dependencies \u0026\u0026 apt-get -y install default-libmysqlclient-dev build-essential \\ \u0026\u0026 apt-get clean RUN pip install --upgrade pip \\ \u0026\u0026 pip install pipenv COPY ./Pipfile . COPY ./Pipfile.lock . RUN pipenv install --deploy --system COPY . . # entrypoint COPY ./local-entrypoint.sh /usr/src/app/local-entrypoint.sh RUN chmod +x /usr/src/app/local-entrypoint.sh CMD [\"/bin/bash\", \"-c\", \"./local-entrypoint.sh\"] Nosso docker-compose.yml tamb√©m precisar√° ser alterado: # docker-compose.yml version: \"3.8\" services: # updated web: build: . volumes: - .:/usr/src/app ports: - 8004:8000 env_file: - env/.dev environment: - ENVIRONMENT=dev - TESTING=0 depends_on: - db db: image: mysql:8.0 command: --default-authentication-plugin=mysql_native_password restart: always volumes: - mysql_data:/var/lib/mysql_data/data/ environment: MYSQL_DATABASE: econowallet MYSQL_USER: user MYSQL_PASSWORD: password MYSQL_ROOT_PASSWORD: password ports: - 3320:3306 volumes: mysql_data: Aqui tivemos duas mudan√ßas: nosso volume n√£o √© referente ao .project e sim a pasta root . (caso contr√°rio o local-entrypoint.sh n√£o ser√° encontrado). removemos o command que estava inicializando app, e jogamos para dentro do entrypoint. Por fim, garanta a instala√ß√£o do drive do MySQL e do pacote mysql-connector-python: pipenv install mysqlclient==2.0.3 pipenv install mysql-connector-python==8.0.25 ","date":"2021-08-01","objectID":"/posts/econowallet-fastapi-pt5/:3:4","tags":["RESTFull API","Web Development","Backend","Asynchronous","FastAPI","Docker","SQLAlchemy"],"title":"Criando uma API Pronta para Produ√ß√£o com FastAPI - PT.5","uri":"/posts/econowallet-fastapi-pt5/"},{"categories":["Tutorials"],"content":"Refatora√ß√£o üî® No in√≠cio do projeto n√≥s fizemos uma configura√ß√£o no PyCharm para considerar o diret√≥rio project como root. Por√©m, teremos que retornar para o diret√≥rio anterior (efetuando os mesmos passos link de como fazer. E al√©m disso, iremos alterar todas as refer√™ncias nos nossos arquivos de volta para project.app‚Ä¶ Os arquivos que precisar√£o sofrer essa refatora√ß√£o s√£o: status.py database_session.py modelbase.py register.py main.py Sorry! Agora sim, vamos rebuildar a aplica√ß√£o e observar no DBeaver se veremos uma nova tabela (como especificado nosso model). docker-compose down -v docker-compose up --build -d docker-compose logs -f üéä üéâ Agora temos nossa tabela materializada no banco e podemos iniciar as transa√ß√µes!! üéä üéâ ","date":"2021-08-01","objectID":"/posts/econowallet-fastapi-pt5/:3:5","tags":["RESTFull API","Web Development","Backend","Asynchronous","FastAPI","Docker","SQLAlchemy"],"title":"Criando uma API Pronta para Produ√ß√£o com FastAPI - PT.5","uri":"/posts/econowallet-fastapi-pt5/"},{"categories":["Tutorials"],"content":"Pr√≥ximo Cap√≠tulo‚Ä¶ Na pr√≥xima etapa, iremos configurar quais rotas teremos na aplica√ß√£o. Assim como que as rotas interagem com o banco de dados üëãüèΩ. Voc√™ pode acompanhar o reposit√≥rio do projeto no link abaixo: GitHub - Econowallet ü•Ç ","date":"2021-08-01","objectID":"/posts/econowallet-fastapi-pt5/:4:0","tags":["RESTFull API","Web Development","Backend","Asynchronous","FastAPI","Docker","SQLAlchemy"],"title":"Criando uma API Pronta para Produ√ß√£o com FastAPI - PT.5","uri":"/posts/econowallet-fastapi-pt5/"},{"categories":["Tutorials"],"content":"Nos Cap√≠tulos Anteriores ‚Ä¶ Ap√≥s um tempinho sem postar nada, vamos dar continuidade a essa s√©rie sobre como desenvolver uma API pronta pro Deploy! üöÄ üöÄ Essa √© a parte 4 do nosso projeto do EconoWallet e se voc√™ quiser verificar o que j√° fizemos at√© o momento, acesse os links abaixo: Parte 1 Parte 2 Parte 3 Parte 4 Parte 5 Dando continuidade a nossa aplica√ß√£o, hoje vamos configurar qual ser√° nosso banco de dados. Apesar de ter tido que nesse post ir√≠amos tamb√©m configurar o SQLAlchemy, prefirir deixar para o pr√≥ximo post, pois iria ficar muito conte√∫do nesse artigo. ","date":"2021-07-31","objectID":"/posts/econowallet-fastapi-pt4/:1:0","tags":["RESTFull API","Web Development","Backend","Asynchronous","FastAPI","Docker","SQLAlchemy"],"title":"Criando uma API Pronta para Produ√ß√£o com FastAPI - PT.4","uri":"/posts/econowallet-fastapi-pt4/"},{"categories":["Tutorials"],"content":"Modifica√ß√µes Eu fiz algumas modifica√ß√µes de localiza√ß√£o de arquivos de uma forma que fica simples a separa√ß√£o de compet√™ncias do nosso app. Basicamente agora temos tudo que diz respeito √†s configura√ß√µes do Docker (Dockerfile, docker-compose, .dockerignore) fora do diret√≥rio do projeto. Veja o resultado: . ‚îú‚îÄ‚îÄ docker-compose.yml ‚îú‚îÄ‚îÄ Dockerfile ‚îú‚îÄ‚îÄ Pipfile ‚îú‚îÄ‚îÄ Pipfile.lock ‚îú‚îÄ‚îÄ project ‚îÇ¬†‚îî‚îÄ‚îÄ app ‚îÇ¬†‚îú‚îÄ‚îÄ config.py ‚îÇ¬†‚îú‚îÄ‚îÄ __init__.py ‚îÇ¬†‚îî‚îÄ‚îÄ main.py ‚îî‚îÄ‚îÄ README.md E al√©m de alterar a localiza√ß√£o dos arquivos, foi preciso fazer um pequeno ajuste no docker-compose.yml para referenciar o Dockerfile no root do projeto. # modified version: \"3.8\" services: web: build: . command: uvicorn app.main:app --reload --workers 1 --host 0.0.0.0 --port 8000 volumes: - ./project:/usr/src/app ports: - 8004:8000 environment: - ENVIRONMENT=dev - TESTING=0 Agora provavelmente tudo deve estar funcionando normalmente. Build novamente e tente acessar o swager da API (docker-compose up --build -d). ","date":"2021-07-31","objectID":"/posts/econowallet-fastapi-pt4/:2:0","tags":["RESTFull API","Web Development","Backend","Asynchronous","FastAPI","Docker","SQLAlchemy"],"title":"Criando uma API Pronta para Produ√ß√£o com FastAPI - PT.4","uri":"/posts/econowallet-fastapi-pt4/"},{"categories":["Tutorials"],"content":"Database Nossa inten√ß√£o durante o desenvolvimento √© buscar ao m√°ximo que nosso ambiente se assemelhe ao ambiente de produ√ß√£o, onde nossa API vai estar sendo acessada por usu√°rios reais. Nesse cen√°rio, as informa√ß√µes trafegadas precisam ser persistidas em algum lugar, e normalmente √© nesse ponto onde o banco de dados se encaixa. Saiba que existem diferentes banco de dados para diferentes tipos de problemas, para nossa aplica√ß√£o iremos utilizar o MySQL. ","date":"2021-07-31","objectID":"/posts/econowallet-fastapi-pt4/:3:0","tags":["RESTFull API","Web Development","Backend","Asynchronous","FastAPI","Docker","SQLAlchemy"],"title":"Criando uma API Pronta para Produ√ß√£o com FastAPI - PT.4","uri":"/posts/econowallet-fastapi-pt4/"},{"categories":["Tutorials"],"content":"Configurando o MySQL √â realmente muito simples configurar suporte √† uma base de dados √† sua aplica√ß√£o utilizando o docker-compose. Tudo que voc√™ precisa fazer √© adicionar mais um service, veja: version: \"3.8\" services: web: build: . command: uvicorn app.main:app --reload --workers 1 --host 0.0.0.0 --port 8000 volumes: - ./project:/usr/src/app ports: - 8004:8000 environment: - ENVIRONMENT=dev - TESTING=0 # new depends_on: - db # new db: image: mysql:8.0 command: --default-authentication-plugin=mysql_native_password restart: always volumes: - mysql_data:/var/lib/mysql_data/data/ environment: MYSQL_DATABASE: econowallet MYSQL_USER: user MYSQL_PASSWORD: password MYSQL_ROOT_PASSWORD: password ports: - 3320:3306 volumes: mysql_data: O que est√° acontecendo aqui √© bem semelhante ao servi√ßo web, com algumas ressalvas: depends_on: aqui √© utilizado pois agora o nosso servi√ßo web depende que o db esteja pronto primeiro, antes de iniciar. estamos partindo de uma imagem base oficial do MySQL. command e restart s√£o instru√ß√µes oficiais da imagem do MySQL, doc acima üëÜ. Al√©m disso, agora precisamos especificar algumas vari√°veis de ambiente que ser√£o utilizadas para criar nossa base de dados para o EconoWallet e a porta que iremos nos conectar para acessar as informa√ß√µes. E, como para nosso banco de dados estamos especificando um volume que n√£o √© um diret√≥rio, o mesmo precisa ser indicado ao final do docker-compose.yml. ","date":"2021-07-31","objectID":"/posts/econowallet-fastapi-pt4/:3:1","tags":["RESTFull API","Web Development","Backend","Asynchronous","FastAPI","Docker","SQLAlchemy"],"title":"Criando uma API Pronta para Produ√ß√£o com FastAPI - PT.4","uri":"/posts/econowallet-fastapi-pt4/"},{"categories":["Tutorials"],"content":"Conectando ao MySQL Agora que nosso docker-compose.yml est√° configurado, podemos rebuildar nossa aplica√ß√£o, eu gosto de derrubar o servi√ßo removendo os volumes sempre que tem uma modifica√ß√£o relativamente grande, e em seguida subir novamente. docker-compose down -v docker-compose up --build -d Voc√™ pode tamb√©m consultar os logs para saber se tudo correu bem, basta rodar docker-compose logs -f. Se tudo correu bem voc√™ j√° est√° apto a conectar no nosso banco de dados. Se voc√™ tem uma vers√£o premium do PyCharm √© bem simples de fazer isso, mas aqui eu mostrarei como configurar pelo DBeaver um software gratuito e que todos tem acesso. Aqui tem o link oficial do site deles, a instala√ß√£o √© super direta para qualquer SO. Passo a Passo Com o software instalado, v√° na lateral superior esquerda e clique no s√≠mbolo de uma tomada: Agora voc√™ vai precisar preencher os campos as informa√ß√µes que est√£o no docker-compose.yml e clicar em Test connection. Provavelmente ele vai solicitar instalar alguns drives e se tudo correr bem ver√° uma mensagem como no print. Clique em finish e na lateral agora voc√™ deve estar vendo nosso banco j√° configurado!! ‚ú® Apesar de estarmos com nosso banco configurado, ainda n√£o temos nenhuma tabela para interagir e tamb√©m n√£o temos nenhuma intera√ß√£o entre a API e o banco. SQLAlchemy ao resgate!! ü©∫ ","date":"2021-07-31","objectID":"/posts/econowallet-fastapi-pt4/:3:2","tags":["RESTFull API","Web Development","Backend","Asynchronous","FastAPI","Docker","SQLAlchemy"],"title":"Criando uma API Pronta para Produ√ß√£o com FastAPI - PT.4","uri":"/posts/econowallet-fastapi-pt4/"},{"categories":["Tutorials"],"content":"SQLAlchemy SQLAlchemy faz parte de um conjunto de frameworkers que s√£o utilizados para abstrair a complexidade de lidar com o banco de dados diretamente. Desenvolver com um ORM n√£o te prende √† um banco de dados espec√≠fico, onde posteriormente √© poss√≠vel plugar a aplica√ß√£o ao Postgres, MySQL, Oracle, MariaDB ou outros (contanto que o framework tenha suporte). Escolhi o SQLAlchemy por alguns motivos: pela robustes: o SQLAlchemy possui diferentes m√≥dulos (Core e ORM) que te permite fazer muita coisa legal. muito tempo presente na comunidade: o que torna mais f√°cil lidar com problemas e bugs caso eles apare√ßam (e ir√£o aparecer =D). ter sido bastante utilizado e testado em produ√ß√£o: isso atribui maior confiabilidade ao software. Saiba que os desenvolvedores do SQLAlchemy est√£o trabalhando numa nova vers√£o da lib e parte da sintaxe de como realizar queries com o ORM ir√° mudar. Apesar dessa nova sintaxe estar dispon√≠vel na vers√£o atual da lib ela ainda n√£o est√° oficialmente lan√ßada, para mais detalhes acesse a documenta√ß√£o. Aqui iremos manter a sintaxe mais tradicional. ","date":"2021-07-31","objectID":"/posts/econowallet-fastapi-pt4/:4:0","tags":["RESTFull API","Web Development","Backend","Asynchronous","FastAPI","Docker","SQLAlchemy"],"title":"Criando uma API Pronta para Produ√ß√£o com FastAPI - PT.4","uri":"/posts/econowallet-fastapi-pt4/"},{"categories":["Tutorials"],"content":"Como funciona o SQLAlchemy OMR? √â importante deixar claro que aqui n√≥s estamos utilizando o componente ORM do SQLAlchemy e n√£o o Core. A diferen√ßa √© que o ORM adiciona uma camada de abstra√ß√£o que torna a intera√ß√£o com o banco mais pythonica e menos SQL raiz. Minha inten√ß√£o n√£o √© explicar todos os detalhes do SQLAlchemy, mas sim o de ajudar a compreender os componentes que iremos utilizar aqui. Existe um livro muito bom com v√°rias informa√ß√µes super relevantes chamado Essential SQLAlchemy. Dito isso, eu costumo pensar o SQLAlchemy como duas sess√µes diferentes: A primeira onde especificamos quais os models (traduzidos em tabelas do banco) da nossa aplica√ß√£o. E o segundo onde especificamos a session (conex√£o com o banco). No diagrama abaixo eu tento ilustrar esses dois cen√°rios: Cen√°rio Um Veja que partimos de uma classe m√£e chamada de declarative_base pelo SQLAlchemy e que a mesma √© utilizada na cria√ß√£o de cada um dos models. Al√©m disso, ap√≥s configurados os models, n√≥s utilizamos o .metadata.create_all(engine) para de fato materializar esses models em tabelas no banco. Essa engine √© justamente onde est√° a informa√ß√£o de qual banco de dados utilizar. Cen√°rio Dois Aqui n√≥s vemos como √© feita a conex√£o com o banco e √© por meio da session que faremos transa√ß√µes com as nossas tabelas criadas, o famoso Create/Read/Update/Delete. Um ponto para ficar atento √© que ao configurar as sessions na nossa aplica√ß√£o precisamos cuidar para que as sessions sejam sempre finalizadas, mesmo quando algum problema aconte√ßa. ","date":"2021-07-31","objectID":"/posts/econowallet-fastapi-pt4/:4:1","tags":["RESTFull API","Web Development","Backend","Asynchronous","FastAPI","Docker","SQLAlchemy"],"title":"Criando uma API Pronta para Produ√ß√£o com FastAPI - PT.4","uri":"/posts/econowallet-fastapi-pt4/"},{"categories":["Tutorials"],"content":"Pr√≥ximo Cap√≠tulo‚Ä¶ üéâ Na pr√≥xima etapa, iremos configurar o SQLAlchemy e fazer com que nosso app comece a interagir com o banco!! Voc√™ pode acompanhar o reposit√≥rio do projeto no link abaixo: GitHub - Econowallet ü•Ç ","date":"2021-07-31","objectID":"/posts/econowallet-fastapi-pt4/:5:0","tags":["RESTFull API","Web Development","Backend","Asynchronous","FastAPI","Docker","SQLAlchemy"],"title":"Criando uma API Pronta para Produ√ß√£o com FastAPI - PT.4","uri":"/posts/econowallet-fastapi-pt4/"},{"categories":["Tutorials"],"content":"Nos Cap√≠tulos Anteriores ‚Ä¶ Essa √© a parte 3 do nosso projeto do EconoWallet, se quiser verificar o que j√° fizemos at√© o momento, verifique os links abaixo: Parte 1 Parte 2 Parte 3 Parte 4 Parte 5 Dando continuidade a nossa aplica√ß√£o, hoje vamos iniciar a configura√ß√£o do Docker em nosso ambiente. E antes de mais nada, precisaremos instalar o Docker e o Docker Compose na nossa m√°quina. A documenta√ß√£o do Docker para instala√ß√£o √© muito bem explicada. Ao acessar o link voc√™ encontrar√° um passo-a-passo para qualquer SO que esteja usando (macOS, Linux ou Windows). Ao final da instala√ß√£o, o pr√≥prio tutorial indica como verificar se est√° tudo funcionando corretamente, a vers√£o que estarei utilizando aqui √©: $ docker -v Docker version 20.10.5, build 55c4c88 $ docker-compose -v docker-compose version 1.27.4, build 40524192 O workflow que vamos seguir aqui √© o de containerizar cada parte da aplica√ß√£o e depois orquestrar isso de alguma forma, para que cada parte se comunique de forma harm√¥nica e a aplica√ß√£o final funcione atrav√©s dos containers e imagens criadas. Nesse processo, cada ‚Äúbloco‚Äù da aplica√ß√£o vai ser constru√≠do utilizando um template de instru√ß√µes, nossos Dockerfiles, e a orquestra√ß√£o aqui vai ser feita por um arquivo chamado docker-compose.yml. Ao final desse workflow voc√™ vai ter um bundle onde cada container cont√™m parte da aplica√ß√£o, mas tudo funciona em conjunto. Veja na imagem (de forma simplificada) o que acontece. Imagem retirada do Google Uma ressalva que gostaria de fazer √© que existem outras formas de orquestrar os containers que n√£o seja utilizando um arquivo docker-compose.yml, como por exemplo utilizando Kubernetes (k8s). Criando o primeiro Dockerfile Uma regra b√°sica na hora de criar seus Dockerfiles √© considerar que suas instru√ß√µes ao longo do arquivo representam camadas, e as camadas que exigem mais processamento e mais tempo para ‚Äúconstruir‚Äù devem estar posicionadas no in√≠cio. Dessa forma, voc√™ otimiza o tempo de processamento, e caso seja precise rebuildar a imagem com alguma modifica√ß√£o em camadas mais superiores, pois as que n√£o sofrem modifica√ß√£o s√£o recarregadas a partir de um cache. No caso de Dockerfiles para aplica√ß√µes web em python eu costumo seguir um pipeline b√°sico do que √© preciso/interessante ter na imagem, e ao longo do projeto vou modificando de acordo a necessidade: Ainda assim, √© f√°cil encontrar modelos de Dockerfile para diversos tipos de aplica√ß√£o no github, apenas busque entender o que de fato est√° acontecendo antes de copiar e colar. FROM python:3.9.0-slim-buster WORKDIR /usr/src/app ENV PYTHONDONTWRITEBYTECODE 1 ENV PYTHONUNBUFFERED 1 RUN apt-get update \\ \u0026\u0026 apt-get -y install netcat gcc \\ \u0026\u0026 apt-get clean RUN pip install --upgrade pip \\ \u0026\u0026 pip install pipenv COPY /Pipenv Pipfile.lock ./ RUN pipenv install --deploy --system COPY . . Como visto no fluxograma, primeiramente vemos uma imagem base do python, que aqui estamos utilizando o python:3.9.0-slim-buster, uma esp√©cie de imagem m√≠nima apenas com o essencial para podermos rodar uma aplica√ß√£o python, al√©m da slim-buster temos outras varia√ß√µes que buscam aplicar o mesmo conceito de economia de espa√ßo. Este artigo do Medium cont√©m uma boa explica√ß√£o sobre a diferen√ßa entre os tipos: Slim, Alpine, Strech, Buster, Jessie e Bullseye. Como s√£o imagens m√≠nimas e de tamanho reduzido, pode ser que a depender de sua aplica√ß√£o, voc√™ precise completar a imagem com alguma depend√™ncia. √â dif√≠cil saber de antem√£o qual a imagem que vai funcionar corretamente com seu projeto, mas gra√ßas a praticidade de trabalhar com Docker, podemos facilmente trocar e buscar a que melhor se adapte. Voltando ao c√≥digo que escrevemos, nas vari√°veis de ambiente (ENV), √© uma boa pr√°tica ao trabalhar com projetos em python que adicionemos as seguintes vari√°veis: PYTHONDONTWRITEBYTECODE: Impede que o Python grave arquivos pyc no disco (equivalente a op√ß√£o -B) PYTHONUNBUFFERED: Evita que o Python armazene um buffer stdout e stderr (equivalente a op√ß√£o -u) Criando o primeiro d","date":"2021-04-08","objectID":"/posts/econowallet-fastapi-pt3/:0:0","tags":["RESTFull API","Web Development","Backend","Asynchronous","FastAPI","Docker","SQLAlchemy"],"title":"Criando uma API Pronta para Produ√ß√£o com FastAPI - PT.3","uri":"/posts/econowallet-fastapi-pt3/"},{"categories":["Tutorials"],"content":" Neste tutorial iremos iniciar o Econowallet, essa aplica√ß√£o que vai contar com uma API para controle financeiro de suas despesas e investimentos, se voc√™ n√£o viu o post passado (onde explico mais sobre meu objetivo com esse projeto) clique aqui. T√≥picos que ser√£o abordados nesse post: Setup Inicial: main.py e config.py Rotas async Setup Como todo projeto python, √© uma boa pr√°tica que voc√™ crie um ambiente virtual isolando as depend√™ncias do projeto, isso evita que voc√™ possa ter conflito entre diferentes libs de outros projetos que esteja trabalhando. Para este fiom temos v√°rias op√ß√µes: Pip + virtualenv Poetry Pipenv Conda Estarei utilizando aqui o Pipenv, sinta-se a vontade para usar o que tenha mais familiaridade. Como IDE estarei usando o Pycharm 2020.3.5 (minha preferida no momento para desenvolver em python) e utilizaremos python 3.9.0. Ao clicar em ‚ÄúNew Project‚Äù temos a seguinte imagem: Veja na lateral esquerda como o Pycharm j√° traz um template para diversos tipos de projeto, por√©m no momento ainda n√£o temos a op√ß√£o para o FastAPI. Ap√≥s iniciado o projeto, voc√™ deve encontrar um arquivo chamado Pipfile (se est√° usando o Pipenv) ou apenas uma pasta vazia. Ent√£o iremos executar os seguintes comandos no terminal: $ mkdir project $ mkdir project/app $ touch project/app/main.py $ touch project/app/__init__.py $ mv Pipfile project/ $ cd project $ pipenv install fastapi==0.63.0 $ pipenv install uvicorn==0.13.4 Aqui vale uma ressalva, n√£o sou nenhum expert em arquitetura de software e talvez a estrutura de projeto utilizada aqui n√£o seja a melhor poss√≠vel, mas busco sempre dividir o c√≥digo em blocos que contenham uma l√≥gica ou fun√ß√£o semelhante. Isso permite que tenh√°mos um c√≥digo bom para manter, bom para ser entendido por outros desenvolvedores e tamb√©m para colaborar. Ap√≥s os comandos anteriores voc√™ deveria ter uma estrutura de diret√≥rios semelhante √† mostrada abaixo: . ‚îî‚îÄ‚îÄ project ‚îú‚îÄ‚îÄ app ‚îÇ¬†‚îú‚îÄ‚îÄ __init__.py ‚îÇ¬†‚îî‚îÄ‚îÄ main.py ‚îú‚îÄ‚îÄ Pipfile ‚îî‚îÄ‚îÄ Pipfile.lock Veja que al√©m do FastAPI, instalamos o Uvicorn. O papel dessa lib √© permitir que consigamos interagir com o servi√ßo que estamos contru√≠ndo (ser√° nosso web server), al√©m disso, ela utiliza o protocolo ASGI que permite assincronosidade com suporte aos verbos async e await do python. Outros frames como o Django e Flask j√° possuem um server built-in que √© muito √∫til para fases de desenvolvimento mas que pode gerar confus√£o nas etapas de deploy pois o mesmo precisa ser substitu√≠do. Aqui, iremos ficar desde o in√≠cio com o Uvicorn, saiba mais na documenta√ß√£o clicando no link. ","date":"2021-04-03","objectID":"/posts/econowallet-fastapi-pt2/:0:0","tags":["RESTFull API","Backend","FastAPI","Docker","SQLAlchemy"],"title":"Criando uma API Pronta para Produ√ß√£o com FastAPI - PT.2","uri":"/posts/econowallet-fastapi-pt2/"},{"categories":["Tutorials"],"content":"Main Vamos configurar apenas uma rota inicial com o m√≠nimo de c√≥digo poss√≠vel para ver se nosso web server est√° funcionando. # project/app/main.py from fastapi import FastAPI app = FastAPI() @app.get(\"/ping\") def ping(): return { \"ping\": \"pong!\" } E ent√£o iremos rodar o uvicorn buscando a inst√¢ncia que acabamos de criar do FastAPI (linha 5). Observe que o comando depende de qual diret√≥rio estiver: uvicorn project.app.main:app (do diret√≥rio root) uvicorn app.main:app (do diret√≥rio de projeto) e assim em diante‚Ä¶ $ uvicorn project.app.main:app INFO: Started server process [654351] INFO: Waiting for application startup. INFO: Application startup complete. INFO: Uvicorn running on http://127.0.0.1:8000 (Press CTRL+C to quit) Agora, se navegarmos at√© http://127.0.0.1:8000/ping iremos ver a seguinte resposta: { \"ping\": \"pong!\" } Al√©m disso o FastAPI autom√°gicamente gerou um esquema baseado no padr√£o OpenAPI e juntamente com o Swagger UI criou uma documenta√ß√£o inicial (bem crua) para sua API. Podendo ser acessada em http://localhost:8000/docs ou ainda se n√£o gostou dessa interface do Swager voc√™ pode acessar http://localhost:8000/redoc para uma interface diferente. Swagger UI redoc ‚Ä¶al√©m disso, essa interface pode ser personalizada de acordo com seu projeto e necessidades, voc√™ pode encontrar detalhes na documenta√ß√£o clicando aqui. Agora iremos reiniciar nosso web server e adicionar um comando bem √∫til que iremos utilizar daqui em diante com o uvicorn, o --reload: $ uvicorn project.app.main:app --reload INFO: Uvicorn running on http://127.0.0.1:8000 (Press CTRL+C to quit) INFO: Started reloader process [663530] using statreload INFO: Started server process [663532] INFO: Waiting for application startup. INFO: Application startup complete. Dessa forma, sempre que fizermos alguma altera√ß√£o no c√≥digo, o Uvicorn vai reiniciar o server para n√≥s. Experimente remover a exclama√ß√£o do ‚Äúpong!‚Äù e veja o que acontece. ","date":"2021-04-03","objectID":"/posts/econowallet-fastapi-pt2/:1:0","tags":["RESTFull API","Backend","FastAPI","Docker","SQLAlchemy"],"title":"Criando uma API Pronta para Produ√ß√£o com FastAPI - PT.2","uri":"/posts/econowallet-fastapi-pt2/"},{"categories":["Tutorials"],"content":"Config Agora iremos adicionar um arquivo muito importante para nossa aplica√ß√£o e que vai lidar com nossas vari√°veis de ambiente tanto localmente quanto quando estivermos no ambiente do Docker. # project/app/config.py import logging import os from pydantic import BaseSettings log = logging.getLogger(\"uvicorn\") class Settings(BaseSettings): environment: str = os.getenv(\"ENVIRONMENT\", \"dev\") testing: bool = os.getenv(\"TESTING\", 0) def get_settings() -\u003e BaseSettings: log.info(\"Loading config settings from the environment...\") return Settings() O que est√° acontecendo aqui? Primeiramente, declaramos uma classe Settings com dois atributos: environment: que define nosso ambiente (prod, stage, dev). testing: que define se estamos ou n√£o em um modo de teste. Outro ponto importante a ressaltar √© a sintaxe da classe Settings (testing: bool), essa √© uma forma de explicitar o tipo da vari√°vel que a nossa imputa√ß√£o retornar√°, iremos utilizar muito ao longo do projeto. Pontos positivos de adotar essa sintaxe ao longo do projeto s√£o: c√≥digo mais leg√≠vel. auxilia a IDE a fornecer melhores formas de auto-complete e sugest√µes de m√©todos. previne alguns bugs, pois a IDE tem maior entendimento do que acontece com as fun√ß√µes, classes‚Ä¶ Agora iremos atualizar nosso arquivo main.py: # project/app/main.py from fastapi import FastAPI, Depends from project.app.config import Settings, get_settings app = FastAPI() @app.get(\"/ping\") def ping(settings: Settings = Depends(get_settings)): return { \"ping\": \"pong!\", \"environment\": settings.environment, \"testing\": settings.testing } Com essa altera√ß√£o no c√≥digo estamos setando depend√™ncias √† aplica√ß√£o sempre que acessamos a rota http://127.0.0.1:8000/ping. De uma forma mais intuitiva, o que estamos dizendo √© o seguinte: Agora, se rodarmos novamente nosso server, iremos ver uma resposta da seguinte forma: { \"ping\": \"pong!\", \"environment\": \"dev\", \"testing\": false } Legal, agora sabemos, por exemplo, que estamos no ambiente de desenvolvimento e n√£o de teste. Outro ponto importante √© que da forma como nosso config.py est√°, sempre que batemos na rota http://127.0.0.1:8000/ping ele reconfigura as settings. Esse comportamento de reconfigurar as settings n√£o √© muito atrativo, pois futuramente ao migrarmos o carregamento das vari√°veis de ambiente a partir de um arquivo (.env por exemplo) esse comportamento ir√° diminuir a velocidade com que nosso app responde aos requests. Usaremos ent√£o o decorator @lru_cache para cachiar as settings, de forma que get_settings √© chamada apenas uma vez e reusar√° valores de chamadas recentes para atribuir √†s vari√°veis. Tenha em mente que isso √© indicado apenas para casos onde voc√™ n√£o tem a necessidade de recomputar esses valores v√°rias vezes. # project/app/config.py import logging import os from functools import lru_cache from pydantic import BaseSettings log = logging.getLogger(\"uvicorn\") class Settings(BaseSettings): environment: str = os.getenv(\"ENVIRONMENT\", \"dev\") testing: bool = os.getenv(\"TESTING\", 0) @lru_cache() def get_settings() -\u003e BaseSettings: log.info(\"Loading config settings from the environment...\") return Settings() Agora se atualizarmos v√°rias vezes o http://127.0.0.1:8000/ping n√£o veremos o recarregamento do ‚ÄúLoading config settings from the environment‚Ä¶‚Äù, indicando que nosso cache funcionou üéâ. ","date":"2021-04-03","objectID":"/posts/econowallet-fastapi-pt2/:2:0","tags":["RESTFull API","Backend","FastAPI","Docker","SQLAlchemy"],"title":"Criando uma API Pronta para Produ√ß√£o com FastAPI - PT.2","uri":"/posts/econowallet-fastapi-pt2/"},{"categories":["Tutorials"],"content":"Rotas Async Como dito no in√≠cio do post, estamos usando um web server ASGI e al√©m disso um frame com suporte ao async e await (FastAPI). Portanto, podemos facilmente converter nosso endpoint para uma rota ass√≠ncrona simplesmente adicionando o verbo async. from fastapi import FastAPI, Depends from project.app.config import Settings, get_settings app = FastAPI() @app.get(\"/ping\") async def ping(settings: Settings = Depends(get_settings)): return { \"ping\": \"pong!\", \"environment\": settings.environment, \"testing\": settings.testing } Teste novamente a rota http://127.0.0.1:8000/ping para ver se tudo continua functionando. ATEN√á√ÉO!: O async e await do python n√£o est√° relacionado ao uso de mais threads ou de processamento paralelo. O conceito aqui √© permitir que o c√≥digo execute outras funcionalidades enquanto aguarda resposta de um outro servi√ßo. Irei adicionar um .gitignore ao projeto para irmos adicionando coisinhas que n√£o queremos expor no reposit√≥rio: __pycache__ env √â um bom momento para inicializar nosso reposit√≥rio e comitarmos nosso c√≥digo at√© o momento. Como irei atualizando o repo do projeto de acordo for postando aqui no blog, √© poss√≠vel que quando voc√™ esteja lendo esse conte√∫do o reposit√≥rio esteja com o projeto completo. Mas, qualquer coisa, segue o link do reposit√≥rio: GitHub - Econowallet At√© a pr√≥xima! üëã ","date":"2021-04-03","objectID":"/posts/econowallet-fastapi-pt2/:3:0","tags":["RESTFull API","Backend","FastAPI","Docker","SQLAlchemy"],"title":"Criando uma API Pronta para Produ√ß√£o com FastAPI - PT.2","uri":"/posts/econowallet-fastapi-pt2/"},{"categories":["Tutorials"],"content":" Motiva√ß√£o Atualmente tenho tido bastante dor de cabe√ßa em voltar a utilizar o Excel para fazer uma planilha de controle de gastos, apesar da simplicidade do software, constantemente meus registros de compras feitas ou alguma outra movimenta√ß√£o financeira ficam uma completa bagun√ßa. O que pode ser justificado pela minha falta de destreza com a ferramenta, mas fato √© que a experi√™ncia estava deixando a desejar e consequentemente acabei perdendo a disciplina de organizar meus gastos, o que n√£o recomendo a ningu√©m. Foi ent√£o que me veio a ideia de largar as planilhas e armazenar essa informa√ß√£o em um banco de dados, o que me facilitaria muita coisa, como por exemplo: facilidade para gera√ß√£o de um relat√≥rio com o n√≠vel de personaliza√ß√£o muito superior ao Excel (utilizando R ou Python para consumir os dados e fazer a parte anal√≠tica), manter a integridade dos dados de forma confi√°vel e robusta, criar modelos de machine learning para prever pr√≥ximos gastos e muito mais. Coincidentemente a tudo isso, tenho brincado bastante com o FastAPI, um framework fant√°stico para cria√ß√£o de RESTFull APIs utilizando linguagem python. E, por fim, um √∫ltimo quesito foi que vejo poucos tutoriais em portugu√™s que trazem um material interessante sobre como criar APIs com o FastAPI em um n√≠vel pronto (ou mais pr√≥ximo poss√≠vel) para produ√ß√£o. Overview do app Diante do que foi dito acima, decidi criar um projeto com v√°rias tecnologias que eu considero super interessante e que v√£o auxiliar na entrega de um produto bem robusto e escal√°vel (n√£o que fosse necess√°rio para essa situa√ß√£o). Ent√£o, juntamente com o FastAPI, vamos desenvolver usando um ambiente Docker e torn√°-lo o mais pr√≥ximo poss√≠vel de um ambiente de deploy. Utilizaremos o SQLAlchemy como nosso ORM (Object Relational Mapper) para ‚Äúconversar‚Äù com o banco de dados (Postgres). Ainda iremos tentar desenvolver utilizando o TDD (Test Driven Development) com o pytest. Irei tamb√©m adicionar o Celery, que vai permitir que tenh√°mos um produto altamente escal√°vel ao final do projeto. Por fim, configuraremos GitHub Actions para implementar entrega e integra√ß√£o cont√≠nua (CI/CD) entre o desenvolvimento e o deploy no Heroku. Muito provavelmente existem solu√ß√µes prontas e aplicativos para controle financeiro, mas minha experi√™ncia com eles √© justamente a falta de liberdade em criar algo personalizado para as minhas necessidades. Tecnologias Nesse projeto vou trazer muitas ferramentas como citado acima e com c√≥digo que voc√™ poder√° reutilizar nos mais variados projetos. A seguir eu trouxe um pouco sobre cada tecnologia que ser√° utilizada. ","date":"2021-03-30","objectID":"/posts/finance_control_app/:0:0","tags":["RESTFull API","Web Development","Backend","Asynchronous","FastAPI","Docker","SQLAlchemy"],"title":"Criando uma API Pronta para Produ√ß√£o com FastAPI - PT.1","uri":"/posts/finance_control_app/"},{"categories":["Tutorials"],"content":"FastAPI Como disse antes, tenho tido muito contato com o FastAPI e √© incr√≠vel como √© intuitivo codar com este framework. Abaixo eu trouxe alguns motivos que fazem o FastAPI diferenciado dos outros frameworks em python (retirados da pr√≥pria documenta√ß√£o). T√£o r√°pido quanto NodeJS e GO (FastAPI √© constru√≠do em cima do Starlette e Pydantic) Muito r√°pido para desenvolver APIs (200% a 300% em compara√ß√£o a concorrentes) Redu√ß√£o de bugs Intuitivo F√°cil de aprender Robusto Automagicamente gera documenta√ß√£o para a API e JSON Schemas (Muito Legal) Acesse a documenta√ß√£o do FastAPI para mais detalhes, que por sinal √© muito bem feita. ","date":"2021-03-30","objectID":"/posts/finance_control_app/:1:0","tags":["RESTFull API","Web Development","Backend","Asynchronous","FastAPI","Docker","SQLAlchemy"],"title":"Criando uma API Pronta para Produ√ß√£o com FastAPI - PT.1","uri":"/posts/finance_control_app/"},{"categories":["Tutorials"],"content":"Docker Docker √© uma plataforma de constru√ß√£o de containers utilizada para simular ambientes de desenvolvimento e deploy de forma isolada do seu sistema, ou seja, independentemente do seu SO (sistema operacional) voc√™ consegue ter o mesmo comportamento da aplica√ß√£o pois cada container cont√©m todas as depend√™ncias necess√°rias para sua execu√ß√£o. ","date":"2021-03-30","objectID":"/posts/finance_control_app/:2:0","tags":["RESTFull API","Web Development","Backend","Asynchronous","FastAPI","Docker","SQLAlchemy"],"title":"Criando uma API Pronta para Produ√ß√£o com FastAPI - PT.1","uri":"/posts/finance_control_app/"},{"categories":["Tutorials"],"content":"Pytest pytest √© um framework para escrever tests em Python, ele torna f√°cil e divertido escrever, organizar e rodar tests, quando comparado com o unittest (outro framework bastante conhecido). Abaixo cito algumas caracter√≠sticas do pytest: Requer menos c√≥digo boilerplate Suporta declara√ß√£o assert (built-in do Python) ao inv√©s de assertSomething (como no unittest) Atualizado frequentemente Possui as famosas fixtures que auxiliam na inicializa√ß√£o e finaliza√ß√£o de configura√ß√µes para determinados testes. Utiliza uma abordagem funcional ","date":"2021-03-30","objectID":"/posts/finance_control_app/:3:0","tags":["RESTFull API","Web Development","Backend","Asynchronous","FastAPI","Docker","SQLAlchemy"],"title":"Criando uma API Pronta para Produ√ß√£o com FastAPI - PT.1","uri":"/posts/finance_control_app/"},{"categories":["Tutorials"],"content":"SQLAlchemy SQLAlchemy faz parte de um conjunto de frameworkers que s√£o utilizados para abstrair a complexidade de lidar com o banco de dados diretamente. Desenvolver com um ORM n√£o te prende √† um banco de dados espec√≠fico, onde posteriormente √© poss√≠vel plugar a aplica√ß√£o ao Postgres, MySQL, Oracle, MariaDB ou outros (contanto que o framework tenha suporte). Escolhi o SQLAlchemy por alguns motivos: pela robustes: o SQLAlchemy possui diferentes m√≥dulos (Core e ORM) que te permite fazer muita coisa legal. muito tempo presente na comunidade: o que torna mais f√°cil lidar com problemas e bugs caso eles apare√ßam (e ir√£o aparecer =D). ter sido bastante utilizado e testado em produ√ß√£o: isso atribui maior confiabilidade ao software. ","date":"2021-03-30","objectID":"/posts/finance_control_app/:4:0","tags":["RESTFull API","Web Development","Backend","Asynchronous","FastAPI","Docker","SQLAlchemy"],"title":"Criando uma API Pronta para Produ√ß√£o com FastAPI - PT.1","uri":"/posts/finance_control_app/"},{"categories":["Tutorials"],"content":"Celery O Celery √© um task queue manager utilizado para implementar processamento ass√≠ncrono ao seu c√≥digo. Fique atento que n√£o estamos falando aqui de utiliza√ß√£o de mais threads para realiza√ß√£o de uma task, e sim de desiginar as tasks √† diferentes ‚Äúworkers‚Äù, o que possibilita um escalonamento horizontal e por isso acaba sendo muito atrativo ao design de microservi√ßos. O Celery √© utilizado na arquitetura de v√°rias tecnologias de Big Data como Apache SuperSet e Apache Airflow, justamente por ser um framework muito √∫til para escalabilidade. ","date":"2021-03-30","objectID":"/posts/finance_control_app/:5:0","tags":["RESTFull API","Web Development","Backend","Asynchronous","FastAPI","Docker","SQLAlchemy"],"title":"Criando uma API Pronta para Produ√ß√£o com FastAPI - PT.1","uri":"/posts/finance_control_app/"},{"categories":["Tutorials"],"content":"GitHub O GitHub Actions √© uma ferramente de integra√ß√£o cont√≠nua e entrega cont√≠nua (CI/CD), totalmente integrada com o GitHub. Temos outras plataformas que oferecem o mesmo servi√ßo como: GitLab AWS (CodeCommit, CodeBuild, CodeDeploy, ECR) GCP (Cloud Source Repositories, Cloud Build, Container Registry) Azure (Repos, Pipelines, Container Registry) Mas como j√° utilizo o GitHub para outros projetos, resolvi seguir com a mesma plataforma. ","date":"2021-03-30","objectID":"/posts/finance_control_app/:6:0","tags":["RESTFull API","Web Development","Backend","Asynchronous","FastAPI","Docker","SQLAlchemy"],"title":"Criando uma API Pronta para Produ√ß√£o com FastAPI - PT.1","uri":"/posts/finance_control_app/"},{"categories":["Tutorials"],"content":"Heroku Heroku √© uma cloud Platform as a Service (PaaS) que prov√™ um local para deployar aplica√ß√µes web. Eles abstraem muita da complixidade de infraestrutura e torna f√°cil hospedar aplica√ß√µes com seguran√ßa e escalabilidade. Para esse projeto irei utilizar planos gratuitos, para que todos possam seguir o passo-a-passo. Por√©m, como nossa aplica√ß√£o vai estar conteinerizada voc√™ pode deployar em basicamente em qualquer plataforma cloud que existe com poucas modifica√ß√µes no c√≥digo. Mas aqui iremos ficar com o Heroku. Pr√≥ximos tutoriais iremos partir para o hands-on!!! At√© l√°! ","date":"2021-03-30","objectID":"/posts/finance_control_app/:7:0","tags":["RESTFull API","Web Development","Backend","Asynchronous","FastAPI","Docker","SQLAlchemy"],"title":"Criando uma API Pronta para Produ√ß√£o com FastAPI - PT.1","uri":"/posts/finance_control_app/"},{"categories":null,"content":"Demand Forecasting of Brazilian Commodities Soybean, Corn, Sugar, Soybean Meal, Soybean Oil and Wheat (left to right). Demand Forecasting is a technique for estimation of probable demand for a product or services. It is based on the analysis of past demand for that product or service in the present market condition. Demand forecasting should be done on a scientific basis and facts and events related to forecasting should be considered. Therefore, in simple words, we can say that after gathering information about various aspects of the market and demand based on the past, is possible to estimate future demand. What we call forecasting of demand. For example, suppose we sold 200, 250, 300 units of product X in January, February, and March respectively. Now we can say that there will be a demand for Y units approximately of product X in April. And you can ask, Why should I matter with demand forecasting? Good question, down below I point to you some key advantages: More effective production scheduling Inventory management and reduction Cost reduction Optimized transport logistics Increased customer satisfaction Those are just some benefits in forecasting demand. As this is a task applicable in almost any business area, the concepts and approaches used here can be extrapolate for your problem too, with specific adjustments. This project has the aim of forecast the next 3 years of exports of top 3 Brazilian commodities: soybean, corn and sugar. But not just that, I‚Äôll cover also the following steps of a Data Science project: exploratory data analysis, data preparation, data cleaning, feature engineering and modeling. The dataset used here came from a public source available by clicking here. ","date":"2020-10-25","objectID":"/posts/demand_forecasting_brazilian_commodities/:0:0","tags":["Project","Data Science","Machine Learning","Time Series"],"title":"Demand Forecasting of Brazilian Commodities","uri":"/posts/demand_forecasting_brazilian_commodities/"},{"categories":null,"content":"Software Requirement If you want to reproduce the project in your environment, I suggest you to install the following packages first, before load them. # Data Exploration library(tidyverse) library(skimr) library(lubridate) library(tidytext) library(timetk) library(gt) # color pallete library(tidyquant) # modeling library(tidymodels) library(modeltime) ","date":"2020-10-25","objectID":"/posts/demand_forecasting_brazilian_commodities/:1:0","tags":["Project","Data Science","Machine Learning","Time Series"],"title":"Demand Forecasting of Brazilian Commodities","uri":"/posts/demand_forecasting_brazilian_commodities/"},{"categories":null,"content":"Exploratory Data Analysis As mentioned before, the dataset came from a public source called COMEX STAT. This website provides free access to Brazilian foreign trade statistics. # reading the data and converting categorical features to factor exp_imp_tbl \u003c- read_csv(\"data/data_comexstat.csv\") %\u003e% mutate_if(is.character, as_factor) As usual, I always create a dictionary of the dataset I‚Äôm working just to keep in mind the meaning of each variable, see the following list: date: date where occurred the transaction of export or import (our time series information). product: commodities (sugar, soybean meal, soybean oil, soybean, corn and wheat). state: State responsible for the production. country: Country responsible for the transaction type: if there is export or import. route: route used to transport the commodity. tons: quantity export/import. usd: commercial currency. ","date":"2020-10-25","objectID":"/posts/demand_forecasting_brazilian_commodities/:2:0","tags":["Project","Data Science","Machine Learning","Time Series"],"title":"Demand Forecasting of Brazilian Commodities","uri":"/posts/demand_forecasting_brazilian_commodities/"},{"categories":null,"content":"Overview The data contains all tracking information of monthly imports and exports of a range of products, by brazilian states, by routes (air, sea, ground, etc) and from/to which country. At the beginning of the process is a good idea to take a general overview of the data, and for that I love the skimr::skim() function, very handy to understand a big picture of your data. skim(exp_imp_tbl) ‚îÄ‚îÄ Data Summary ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ Values Name exp_imp_tbl Number of rows 117965 Number of columns 8 _______________________ Column type frequency: Date 1 factor 5 numeric 2 ________________________ Group variables None ‚îÄ‚îÄ Variable type: Date ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ skim_variable n_missing complete_rate min max median n_unique 1 date 0 1 1997-01-01 2019-12-01 2012-10-01 276 ‚îÄ‚îÄ Variable type: factor ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ skim_variable n_missing complete_rate ordered n_unique top_counts 1 product 0 1 FALSE 6 sug: 35202, soy: 22914, cor: 21872, soy: 18215 2 state 0 1 FALSE 27 SP: 28713, PR: 17155, MT: 16837, GO: 10981 3 country 0 1 FALSE 212 Chi: 7437, Par: 7160, Net: 7158, Arg: 4842 4 type 0 1 FALSE 2 Exp: 105861, Imp: 12104 5 route 0 1 FALSE 5 Sea: 93870, Gro: 13038, Oth: 6374, Air: 2918 ‚îÄ‚îÄ Variable type: numeric ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ skim_variable n_missing complete_rate mean sd p0 p25 p50 p75 p100 hist 1 tons 0 1 14537. 49779. 0 125. 2000 13534. 1798446. ‚ñá‚ñÅ‚ñÅ‚ñÅ‚ñÅ 2 usd 0 1 4813150. 19494116. 0 71552 725000 3895943 903930411 ‚ñá‚ñÅ‚ñÅ‚ñÅ‚ñÅ As we can see, our dataset have: 1 date variable 5 categorical variables 2 numerical variables For our luck there is no missing data in any of these columns. Two things pop up when we look at type and route variables. type: Brazil is a country that export more than import (more than 100k of observations on export category). route: The route that Brazil less use (considering exports and imports) is air. And the route more used is sea. ","date":"2020-10-25","objectID":"/posts/demand_forecasting_brazilian_commodities/:2:1","tags":["Project","Data Science","Machine Learning","Time Series"],"title":"Demand Forecasting of Brazilian Commodities","uri":"/posts/demand_forecasting_brazilian_commodities/"},{"categories":null,"content":"Production over time We also saw the date range of this date feature, and it‚Äôs from 1997/01/01 to 2019/01/12. Looking more closely at this feature we can investigate how was the exports from Brazil, considering all states and to everywhere, throughout the time. # data wrangling before plot # adding two columns: year and month exp_imp_year_month_tbl \u003c- exp_imp_tbl %\u003e% mutate(year = year(date), month = month(date, abbr = FALSE, label = TRUE, locale = Sys.setlocale(\"LC_COLLATE\", \"C\"))) # total of tons by year (considering just exports) expt_year_total_tbl \u003c- exp_imp_year_month_tbl %\u003e% filter(type == \"Export\") %\u003e% group_by(year) %\u003e% summarise(total_tons = sum(tons)) %\u003e% ungroup() # total of tons for each month of the year (considering just exports) expt_year_month_total_tbl \u003c- exp_imp_year_month_tbl %\u003e% filter(type == \"Export\") %\u003e% group_by(year, month) %\u003e% summarise(total_tons = sum(tons)) %\u003e% ungroup() # vizualizing the monthly total of tons (1997 - 2020) expt_year_month_total_tbl %\u003e% mutate(month = month %\u003e% str_to_title() %\u003e% as_factor()) %\u003e% ggplot(aes(x = year, y = total_tons)) + geom_point(size = .8) + geom_line() + facet_wrap(~month) + theme_tq() + scale_y_continuous(labels = scales::number_format(scale = 1e-6, suffix = \"M\")) + scale_x_continuous(breaks = c(1997, 2000, 2005, 2010, 2015, 2019)) + labs( title = \"Total of Tons for the Monthly Exports Between 1997 and 2020\", subtitle = \"Considering all Brazilian States, and Soybean, Soybeans Meal, Soybean Oil, Sugar, Corn and Wheat commodities\", x = \"\", y = \"Millions of Tons\", caption = \"linkedin.com/in/lucianobatistads/\" ) + theme(axis.text.x = element_text(size = 7)) On this monthly chart, we see that there is a higher pronounced growth trend in exports during the months from March to August. # building vizualizations # vizualizing the annual total of tons export (1997 - 2020) expt_year_total_tbl %\u003e% ggplot(aes(x = year, y = total_tons)) + geom_point() + geom_line() + theme_tq() + scale_y_continuous(labels = scales::number_format(scale = 1e-6, suffix = \"M\")) + scale_x_continuous(breaks = c(1997, 2000, 2005, 2010, 2015, 2019)) + labs( title = \"Total of Tons for the Annual Exports Between 1997 and 2020\", subtitle = \"Considering all Brazilian States, and Soybean, Soybeans Meal, Soybean Oil, Sugar, Corn and Wheat commodities\", x = \"\", y = \"Millions of Tons\", caption = \"linkedin.com/in/lucianobatistads/\" ) + theme(axis.text.x = element_text(size = 10)) As expected, over the years, the brazilian exports follow a growing trend, even if 2020 bring us a terrible result because of COVID-19, it is likely to go back to the initial trend in the next year. ","date":"2020-10-25","objectID":"/posts/demand_forecasting_brazilian_commodities/:2:2","tags":["Project","Data Science","Machine Learning","Time Series"],"title":"Demand Forecasting of Brazilian Commodities","uri":"/posts/demand_forecasting_brazilian_commodities/"},{"categories":null,"content":"Most Important Commodities We saw before our data have 6 different commodities: Soybean, Sugar, Soybeans Meal, Corn, Soybean Oil and Wheat. Let‚Äôs look at them and see which have been more export in the last 5 years. # data wrangling before plot # filtering for recent years and type == \"Export\" # total of tons for these groups top_3_product_exp_tbl \u003c- exp_imp_year_month_tbl %\u003e% filter(year %in% c(2019:2015)) %\u003e% filter(type == \"Export\") %\u003e% group_by(product) %\u003e% summarise(total_tons_exp = sum(tons)) %\u003e% ungroup() %\u003e% slice_max(total_tons_exp, n = 3) top_3_product_exp_tbl %\u003e% mutate(product_str = case_when( product == \"soybeans\" ~ \"Soybean\", product == \"corn\" ~ \"Corn\", TRUE ~ \"Sugar\" )) %\u003e% ggplot(aes(x = total_tons_exp, y = fct_reorder(product_str, total_tons_exp), fill = product_str)) + geom_col() + scale_fill_manual(values = c(\"#7EBEF7\", \"#2595F5\", \"#BBD7F0\")) + scale_x_continuous(labels = scales::number_format(scale = 1e-6, suffix = \"M\")) + guides(fill = FALSE) + theme_tq() + labs( title = \"Top 3 - Brazilian Commodities Exports\", subtitle = \"Considering the Last 5 Years\", caption = \"linkedin.com/in/lucianobatistads/\", x = \"Millions of Tons\", y = \"Commodities\" ) # good table to plot top_3_product_exp_tbl %\u003e% rename(Commodities = product, `Total of Tons` = total_tons_exp) %\u003e% mutate(`Total of Tons` = `Total of Tons` %\u003e% scales::number(scale = 1e-6, suffix = \"M\")) %\u003e% gt() The plot above is showing the top 3 commodities exported in Brazil by the last 5 years: soybean, corn and sugar. With the more important being soybean. If we compare with the others, soybeans have 55.5% more than the second (Corn) and 63.2% more than the third (Sugar), it is an enormous difference. ","date":"2020-10-25","objectID":"/posts/demand_forecasting_brazilian_commodities/:2:3","tags":["Project","Data Science","Machine Learning","Time Series"],"title":"Demand Forecasting of Brazilian Commodities","uri":"/posts/demand_forecasting_brazilian_commodities/"},{"categories":null,"content":"Routes These commodities we are seeing until now are exported by different routes: sea, ground, air, river and others. Let‚Äôs investigate if there is some preference to choose the route and product. Before building those visualizations, sounds a good idea to keep in mind the most chosen routes, considering all products, to establish a big picture of the situation. Look at the table below: # data wrangling before plot # top exports routes on recent years tops_routes_exp \u003c- exp_imp_year_month_tbl %\u003e% filter(type == \"Export\") %\u003e% filter(year \u003e 2000) %\u003e% # selecting most recent years count(route) %\u003e% mutate(prop = n / sum(n)) # table tops_routes_exp %\u003e% rename(Route = route, Percent = prop) %\u003e% mutate(Percent = Percent %\u003e% scales::number(scale = 100, suffix = \"%\", accuracy = .1)) %\u003e% select(-n) %\u003e% gt() %\u003e% tab_header( title = \"Participation of Routes of Exports\" ) Now let‚Äôs see by product and routes what it‚Äôs happening. # data wrangling before plot exp_by_route_product_tbl \u003c- exp_imp_year_month_tbl %\u003e% mutate(product = case_when( product == \"corn\" ~ \"Corn\", product == \"soybean_meal\" ~ \"Soybeans Meal\", product == \"soybean_oil\" ~ \"Soybean Oil\", product == \"sugar\" ~ \"Sugar\", product == \"soybeans\" ~ \"Soybean\", TRUE ~ \"Wheat\" )) %\u003e% filter(type == \"Export\") %\u003e% group_by(route, product) %\u003e% summarise(n = n()) %\u003e% mutate(prop_by_route = n / sum(n)) %\u003e% ungroup() exp_by_route_product_tbl %\u003e% ggplot(aes(x = tidytext::reorder_within(product, prop_by_route, route), y = prop_by_route)) + geom_col(aes(fill = prop_by_route)) + tidytext::scale_x_reordered() + coord_flip() + facet_wrap(~route, scales = \"free\") + scale_fill_gradient(high = \"#144582\", low = \"#D4E8FF\") + scale_y_continuous(labels = scales::percent_format(scale = 100, suffix = \"%\")) + labs( title = \"Proportion of Commodities Exports for all Different Routes\", caption = \"linkedin.com/in/lucianobatistads/\", y = \"\", x = \"Commodities\" ) + theme_tq() + labs(fill = \"Proportion by Routes\") Although most products are transported by sea (table above), we observe that depending on the route there is a preference for the product that will be exported. Considering three major products exported in each route, we have: Sea: sugar, soybean and soybeans meal. Ground: soybean oil, sugar and corn. Air: corn (much more), soybean and sugar. Other: sugar, soybean oil and corn. River: soybean, corn and sugar. And a closer look at soybean, give us the following chart: # data wrangling before plot tops_routes_sugar_exp \u003c- exp_imp_year_month_tbl %\u003e% filter(product == \"soybeans\") %\u003e% filter(type == \"Export\") %\u003e% filter(year \u003e 2000) %\u003e% # selecting most recent years count(route) %\u003e% mutate(prop = n / sum(n)) tops_routes_sugar_exp %\u003e% ggplot(aes(x = prop, y = fct_reorder(route, prop), fill = route)) + geom_col() + scale_x_continuous(labels = scales::percent_format(scale = 100, suffix = \"%\")) + scale_fill_manual(values = c( \"#2595F5\", \"#BBD7F0\", \"#BBD7F0\",\"#BBD7F0\",\"#7EBEF7\")) + theme_tq() + labs( title = \"Participation in Each Route of Brazilian Soybean Exports (1997 - 2020)\", caption = \"linkedin.com/in/lucianobatistads/\", x = \"Proportions\", y = \"Exportation Routes\" ) + guides(fill = F) Yeah, a very high concentration in sea transportation route. ","date":"2020-10-25","objectID":"/posts/demand_forecasting_brazilian_commodities/:2:4","tags":["Project","Data Science","Machine Learning","Time Series"],"title":"Demand Forecasting of Brazilian Commodities","uri":"/posts/demand_forecasting_brazilian_commodities/"},{"categories":null,"content":"Trade Partners Let‚Äôs look at our data by another perspective, trade partners. Brazil has a lot of trade partners, these are countries which with Brazil export and import more, and sounds a good idea to know which countries Brazil has been doing business. # data wrangling before plot # filtering by the last 5 years # first look on the exportation exp_trade_partners_corn_sugar_tbl \u003c- exp_imp_tbl %\u003e% mutate(year = year(date)) %\u003e% filter(year %in% c(2019:2014)) %\u003e% filter(type == \"Export\") %\u003e% # removing special characters mutate(country2 = country %\u003e% str_replace_all(\"[[:punct:]]\", \"\") %\u003e% str_trim(side = \"both\")) %\u003e% group_by(year, country2) %\u003e% summarise(total_usd = sum(usd)) %\u003e% ungroup() %\u003e% mutate(year_fc = as.factor(year), name = reorder_within(country2, total_usd, year_fc)) exp_trade_partners_corn_sugar_tbl %\u003e% # threshold to selecting some countries filter(total_usd \u003e 100000000) %\u003e% ggplot(aes(x = name, y = total_usd, fill = year)) + geom_col(show.legend = FALSE) + coord_flip() + scale_x_reordered() + scale_y_continuous(labels = scales::dollar_format(scale = 1e-6, suffix = \"M\")) + facet_wrap(~year, scales = \"free\", nrow = 3) + labs( title = \"Trade Partners Evaluation for the last 5 years of Exportation\", subtitle = \"Considering all Brazilian States, and Soybean, Soybeans Meal, Soybean Oil, Sugar, Corn and Wheat Commodities\", x = \"\", y = \"Millions of U.S. Dollars\", caption = \"linkedin.com/in/lucianobatistads/\" ) + theme_tq() It‚Äôs clear that China is our most important export trade partner. The others positions have been rotating between Netherlands, Spain and Iran. Now, by imports perspective. # this is the same code, but filtering by importation imp_trade_partners_corn_sugar_tbl \u003c- exp_imp_tbl %\u003e% mutate(year = year(date)) %\u003e% filter(year %in% c(2019:2014)) %\u003e% filter(type == \"Import\") %\u003e% mutate(country2 = country %\u003e% str_replace_all(\"[[:punct:]]\", \"\") %\u003e% str_trim(side = \"both\")) %\u003e% group_by(year, country2) %\u003e% summarise(total_usd = sum(usd)) %\u003e% ungroup() %\u003e% mutate(year_fc = as.factor(year), name = reorder_within(country2, total_usd, year_fc)) imp_trade_partners_corn_sugar_tbl %\u003e% group_by(year) %\u003e% slice_max(total_usd, n = 3) %\u003e% ungroup() %\u003e% ggplot(aes(x = name, y = total_usd, fill = year)) + geom_col(show.legend = FALSE) + coord_flip() + scale_x_reordered() + scale_y_continuous(labels = scales::dollar_format(scale = 1e-6, suffix = \"M\")) + facet_wrap(~year, scales = \"free\") + labs( title = \"Top 3 - Trade Partners Evaluation for the last 5 years of Imports\", subtitle = \"Considering Soybean, Soybeans Meal, Soybean Oil, Sugar, Corn and Wheat Commodities\", caption = \"linkedin.com/in/lucianobatistads/\", y = \"Millions of U.S. Dollars\", x = \"\" ) + theme_tq() Brazil‚Äôs major import trade partners alternate between Argentina, Paraguay, and USA. Curiously, seems that the participation of USA has been decreasing through the time, as opposed to Argentina. ","date":"2020-10-25","objectID":"/posts/demand_forecasting_brazilian_commodities/:2:5","tags":["Project","Data Science","Machine Learning","Time Series"],"title":"Demand Forecasting of Brazilian Commodities","uri":"/posts/demand_forecasting_brazilian_commodities/"},{"categories":null,"content":"States and Commodities Brazil is a huge country, the five largest country in the world, and this gives us different temperature ranges depends on each area you‚Äôre looking at. This geographical aspect lead to cultures of food been produce in specific regions them others. Let‚Äôs see from which region comes the production of ours commodities, in terms of exports. quest_5_tbl \u003c- exp_imp_tbl %\u003e% filter(type == \"Export\") %\u003e% group_by(state, product) %\u003e% summarise(total_usd = sum(usd)) %\u003e% ungroup() %\u003e% group_by(product) %\u003e% top_n(5) quest_5_tbl %\u003e% mutate(product = case_when( product == \"corn\" ~ \"Corn\", product == \"soybean_meal\" ~ \"Soybeans Meal\", product == \"soybean_oil\" ~ \"Soybean Oil\", product == \"sugar\" ~ \"Sugar\", product == \"soybeans\" ~ \"Soybeans\", TRUE ~ \"Wheat\" )) %\u003e% ggplot(aes(x = total_usd, y = reorder_within(state, total_usd, product), fill = product)) + geom_col() + facet_wrap(~product, scales = \"free\") + scale_fill_manual(values = c( \"#1B8BB5\", \"#695905\", \"#1B60B5\", \"#B55C24\", \"#B59C12\",\"#69381A\" )) + scale_y_reordered() + scale_x_continuous(labels = scales::number_format(scale = 1e-9, suffix = \"Bi\", prefix = \"$\")) + guides(fill = F) + theme_tq() + labs( x = \"Billions of U.S. Dollars\", y = \"\", title = \"Top 5 - Most Important Brazilian States by Commodities\", subtitle = \"Considering Exports\", caption = \"linkedin.com/in/lucianobatistads/\" ) Mato Grosso concentrates most of the exports of soybean oil, soybeans and soybean meal, along with Rio Grande do Sul and Paran√°. S√£o Paulo, on the other hand, takes part strongly in sugar exports. And very few states export wheat, the most expressive values comes from Rio Grande do Sul, Paran√° and Santa Catarina. Okay, we cover a lot until now. Take a minute, breathe, because now is modeling time!! ","date":"2020-10-25","objectID":"/posts/demand_forecasting_brazilian_commodities/:2:6","tags":["Project","Data Science","Machine Learning","Time Series"],"title":"Demand Forecasting of Brazilian Commodities","uri":"/posts/demand_forecasting_brazilian_commodities/"},{"categories":null,"content":"Data Modeling As we know, there is 3 time-series to predict the next 3 years of demand in tons: soybean, corn and sugar. I‚Äôll model each separately, because by this way is better to understand the underlying rationale behind the data. Something important to say is that I already tried different approaches of feature engineering and here I‚Äôll show you what had the better performance. Another point to clarify is I‚Äôll be using a modeltime framework workflow, integrated with tidymodels principles (quick review down below). As this is a first part of this project, we‚Äôll be using just ARIMA family of models, be aware that more advanced topics on modeling and feature engineering will covered in the second part. ","date":"2020-10-25","objectID":"/posts/demand_forecasting_brazilian_commodities/:3:0","tags":["Project","Data Science","Machine Learning","Time Series"],"title":"Demand Forecasting of Brazilian Commodities","uri":"/posts/demand_forecasting_brazilian_commodities/"},{"categories":null,"content":"A quick review of modeltime For those that aren‚Äôt familiar with modeltime framework, it‚Äôs a R package that set up a time series analysis workflow in a very optimized way. The package works as an extension of tidymodels but applied to time series problems. I‚Äôll summarize here the principals verbs used: Collect data and split into training and test sets Create \u0026 Fit Multiple Models Add fitted models to a Model Table Calibrate the models to a testing set. Perform Testing Set Forecast \u0026 Accuracy Evaluation Refit the models to Full Dataset \u0026 Forecast Forward To get more details you can access the documantation here. ","date":"2020-10-25","objectID":"/posts/demand_forecasting_brazilian_commodities/:4:0","tags":["Project","Data Science","Machine Learning","Time Series"],"title":"Demand Forecasting of Brazilian Commodities","uri":"/posts/demand_forecasting_brazilian_commodities/"},{"categories":null,"content":"Soybean The first thing to do is set up our tibble with the right timestamp. And as we already know, the dataset has a monthly periodicity equally spaced (regular time series). # data wrangling ts_soybean_tbl \u003c- exp_imp_tbl %\u003e% select(date, product, tons) %\u003e% filter(product == \"soybeans\") %\u003e% group_by(date, product) %\u003e% summarise(total_tons = sum(tons)) %\u003e% ungroup() # visualizing ts_soybean_tbl %\u003e% plot_time_series(.date_var = date, .value = total_tons, .interactive = F, .smooth = F) + scale_y_continuous(labels = scales::number_format(scale = 1e-6, suffix = \"M\")) + labs( y = \"Millions of Tons\", title = \"Soybean Time Series\", caption = \"linkedin.com/in/lucianobatistads/\" ) Just by analyzing this visualization, we‚Äôre seeing that there is a clear annual seasonality with a multiplicative behavior (values are growing throughout the time). We can verify these assumptions with ACF/PACF charts. # High annual correlation ts_soybean_tbl %\u003e% plot_acf_diagnostics(.date_var = date, total_tons, .interactive = F, .show_white_noise_bars = T) + labs( title = \"Soybean Lag Diagnostics\", caption = \"linkedin.com/in/lucianobatistads/\" ) Here we‚Äôre confirm the high correlation with annual lags and also one high partial correlation considering 9, 10 and 11 lags. Is possible to use those features to improve performance, but here we‚Äôll be working with the forecast::auto_arima model that automatic look for lags during the training. ts_soybean_tbl %\u003e% plot_seasonal_diagnostics(.date_var = date, log(total_tons), .interactive = F) + labs( title = \"Soybean Seasonal Diagnostics\", y = \"Log scale\", caption = \"linkedin.com/in/lucianobatistads/\" ) Here we‚Äôre seeing that there is quarterly seasonality, every second and third quarters occur an increase in exports. ts_soybean_tbl %\u003e% mutate(year = year(date) %\u003e% as_factor(), month = month(date, label = TRUE, locale = Sys.setlocale(\"LC_COLLATE\", \"C\")) %\u003e% as_factor()) %\u003e% ggplot(aes(x = year, y = fct_rev(month), fill = total_tons)) + scale_fill_distiller(labels = scales::number_format(scale = 1e-6, suffix = \"M\"), direction = 1) + geom_tile(color = \"grey40\") + labs( title = \"Totals of Exports of Soybeans Across the Years and Months\", y = \"Months\", x = \"\", fill = \"Millions of\\nTons\", caption = \"linkedin.com/in/lucianobatistads/\" ) Now we have a big picture of what is happening. The second and third quarters of practically all months are darker, indicating a higher amount of exports in those periods. Modeling soybean time series First thing will be standardize our data, applying a box-cox transformation. This is a method used to variance reduction applying a power transformation. As we‚Äôll be using ARIMA family, is interesting work that way. We also will keep track of the lambda value, important to back-transform our data after modeling phase. ts_boxcox_soybean_tbl \u003c- ts_soybean_tbl %\u003e% select(-product) %\u003e% mutate(total_tons = box_cox_vec(total_tons)) boxcox_soybean_lambda \u003c- 0.382376781152543 So, we don‚Äôt have so much data to work, actually our time series has 265 observations. That way, I split the data in 5 years of assessment and choose the rest to training. soybean_boxcox_splits \u003c- time_series_split(ts_boxcox_soybean_tbl, assess = \"5 years\", cumulative = TRUE) train_soybean_boxcox_tbl \u003c- training(soybean_boxcox_splits) test_soybean_boxcox_tbl \u003c- testing(soybean_boxcox_splits) soybean_boxcox_splits %\u003e% tk_time_series_cv_plan() %\u003e% plot_time_series_cv_plan(date, total_tons, .interactive = F) + labs( title = \"Soybean Training and Testing Splits\", y = \"BoxCox transformed values\" ) Now, we can start work with the modeltime workflow showed before. auto_arima_formula \u003c- formula(total_tons ~ .) # training auto_arima_boxcox_fit \u003c- arima_reg() %\u003e% set_engine(\"auto_arima\") %\u003e% fit(auto_arima_formula, train_soybean_boxcox_tbl) # testing calibration_boxcox_tbl \u003c- modeltime_table( auto_arima_boxcox_fit ) %\u003e% modeltime_calibrate( new_data = test_soybean_boxcox_tbl) # accuracy on testing data soybean_boxcox_accuracy \u003c- calibratio","date":"2020-10-25","objectID":"/posts/demand_forecasting_brazilian_commodities/:4:1","tags":["Project","Data Science","Machine Learning","Time Series"],"title":"Demand Forecasting of Brazilian Commodities","uri":"/posts/demand_forecasting_brazilian_commodities/"},{"categories":null,"content":"Corn Here we‚Äôll follow the same workflow as soybean demand forecast showed before. ts_corn_tbl \u003c- exp_imp_tbl %\u003e% select(date, product, tons) %\u003e% filter(product == \"corn\") %\u003e% group_by(date, product) %\u003e% summarise(total_tons = sum(tons)) %\u003e% ungroup() ts_corn_tbl %\u003e% plot_time_series(.date_var = date, .value = total_tons, .interactive = F, .smooth = F) + scale_y_continuous(labels = scales::number_format(scale = 1e-6, suffix = \"M\")) + labs( y = \"Millions of Tons\", title = \"Corn Time Series\", caption = \"linkedin.com/in/lucianobatistads/\" ) We also have annual seasonality with a multiplicative behavior. Let‚Äôs look the lag diagnostic. # High annual correlation ts_corn_tbl %\u003e% plot_acf_diagnostics(.date_var = date, total_tons, .interactive = F, .show_white_noise_bars = T) + labs( title = \"Corn Lag Diagnostics\", caption = \"linkedin.com/in/lucianobatistads/\" ) Confirm our assumption of annual seasonality. ts_corn_tbl %\u003e% plot_seasonal_diagnostics(.date_var = date, total_tons, .interactive = F) + labs( title = \"Corn Seasonal Diagnostics\", y = \"Log scale\", caption = \"linkedin.com/in/lucianobatistads/\" ) The interesting of this chart is that we can see a quarterly seasonality too (similiar to soybean seasonal diagnostics), this time with third and fourth quarters. ts_corn_tbl %\u003e% mutate(year = year(date) %\u003e% as_factor(), month = month(date, label = TRUE, locale = Sys.setlocale(\"LC_COLLATE\", \"C\")) %\u003e% as_factor()) %\u003e% ggplot(aes(x = year, y = fct_rev(month), fill = total_tons)) + scale_fill_distiller(labels = scales::number_format(scale = 1e-6, suffix = \"M\"), direction = 1) + geom_tile(color = \"grey40\") + labs( title = \"Totals of Corn Exports Across the Years and Months\", y = \"Months\", x = \"\", fill = \"Millions of\\nTons\", caption = \"linkedin.com/in/lucianobatistads/\" ) Looking at this heatmap is visible that through the years the exports are growing and the period of the year that has more exports (3rd and 4rd quarters). Modeling corn time series # transforming target ---- # boxcox ts_boxcox_corn_tbl \u003c- ts_corn_tbl %\u003e% select(-product) %\u003e% mutate(total_tons = box_cox_vec(total_tons)) boxcox_corn_lambda \u003c- 0.0676121372845911 # visualizing the transformations ---- ts_boxcox_corn_tbl %\u003e% plot_time_series(date, total_tons) # splits ---- # boxcox corn_boxcox_splits \u003c- time_series_split(ts_boxcox_corn_tbl, assess = \"5 years\", cumulative = TRUE) train_corn_boxcox_tbl \u003c- training(corn_boxcox_splits) test_corn_boxcox_tbl \u003c- testing(corn_boxcox_splits) corn_boxcox_splits %\u003e% tk_time_series_cv_plan() %\u003e% plot_time_series_cv_plan(date, total_tons, .interactive = F) + labs( title = \"Corn Training and Testing Splits\", y = \"BoxCox transformed values\", caption = \"linkedin.com/in/lucianobatistads/\" ) Our formula here will be different, by including this features our model could better capture the seasonality. # modeling with modeltime ---- # formula auto_arima_formula \u003c- formula(total_tons ~ . + year(date) + month(date, label = TRUE)) # training auto_arima_fit \u003c- arima_reg() %\u003e% set_engine(\"auto_arima\") %\u003e% fit(auto_arima_formula, train_corn_boxcox_tbl) # testing calibration_boxcox_tbl \u003c- modeltime_table( auto_arima_fit ) %\u003e% modeltime_calibrate( new_data = test_corn_boxcox_tbl ) # accuracy on testing data corn_accuracy \u003c- calibration_boxcox_tbl %\u003e% modeltime_accuracy() # vizualing forecasting calibration_boxcox_tbl %\u003e% modeltime_forecast(new_data = test_corn_boxcox_tbl, actual_data = ts_boxcox_corn_tbl) %\u003e% plot_modeltime_forecast(.interactive = F) + labs( title = \"Corn - Model Performance on Assessment Data\", y = \"BoxCox transformed values\", caption = \"linkedin.com/in/lucianobatistads/\" ) The R-Squared here is about 0.643 with good understanding of the seasonality, but the model could not capture the depressions of the time series data. We‚Äôll stick with this model for now. Now let‚Äôs refit the data. # refiting ---- # boxcox refit_boxcox_tbl \u003c- calibration_boxcox_tbl %\u003e% modeltime_refit(data = ts_boxcox_corn_tbl) refit_boxcox_tbl %\u003e% modeltime_forecast(h ","date":"2020-10-25","objectID":"/posts/demand_forecasting_brazilian_commodities/:4:2","tags":["Project","Data Science","Machine Learning","Time Series"],"title":"Demand Forecasting of Brazilian Commodities","uri":"/posts/demand_forecasting_brazilian_commodities/"},{"categories":null,"content":"Sugar Let‚Äôs investigate the final one. ts_sugar_tbl \u003c- exp_imp_tbl %\u003e% select(date, product, tons) %\u003e% filter(product == \"sugar\") %\u003e% group_by(date, product) %\u003e% summarise(total_tons = sum(tons)) %\u003e% ungroup() ts_sugar_tbl %\u003e% tk_summary_diagnostics() ts_sugar_tbl %\u003e% plot_time_series(.date_var = date, .value = total_tons, .interactive = F, .smooth = F) + scale_y_continuous(labels = scales::number_format(scale = 1e-6, suffix = \"M\")) + labs( y = \"Millions of Tons\", title = \"Sugar Time Series\", caption = \"linkedin.com/in/lucianobatistads/\" ) This time series seems to have a change in behavior after the year of 2012, with a high spike and a significant increase in quantity of exports. What ACF and PACF tell us? # High annual correlation # lag 11 and 23 also have good indicate of correlation ts_sugar_tbl %\u003e% plot_acf_diagnostics(.date_var = date, total_tons, .interactive = F, .show_white_noise_bars = T) + labs( title = \"Sugar Lag Diagnostics\", caption = \"linkedin.com/in/lucianobatistads/\" ) Here we‚Äôre seeing a high correlation mostly with recent 70 lags, and negative correlation with older lags. Then in PACF plot, lag 2 and 9 seems important to our model. ts_sugar_tbl %\u003e% plot_seasonal_diagnostics(.date_var = date, log(total_tons), .interactive = F) + labs( title = \"Sugar Seasonal Diagnostics\", y = \"Log scale\", caption = \"linkedin.com/in/lucianobatistads/\" ) As we confirmed, since 2012 we have higher exports. But looking at this plot, we don‚Äôt see any seasonality throughout the time. So let filter the data and analyse the seasonality after 2012. ts_sugar_tbl %\u003e% filter_by_time(date, .start_date = \"2012-01-01\") %\u003e% plot_seasonal_diagnostics(.date_var = date, total_tons, .interactive = F) + scale_y_continuous(labels = scales::number_format(scale = 1e-6, suffix = \"M\")) + labs( title = \"Sugar Seasonal Diagnostics\", subtitle = \"Seasonal diagnostics considering data since 2012\", y = \"Millions of Tons\", caption = \"linkedin.com/in/lucianobatistads/\" ) Now we can capture an interesting bahavior, seems that the third and first quarter have an increase in exports. Searching the why of happened this change in 2012, I found some events that probably are correlated to our problem. 2012 was the year that Brazil increases the production of ethanol To produce more ethanol was needed to plant more sugar cane (the base of ethanol production). Sugar also came from sugar cane, so, with more sugar cane cultivation, we saw an increase of sugar production, hence reflected on its exports. So, there is a huge probability of our time series have really changed its behavior. Another point is that there is a quarterly seasonality that matches exactly with the period of sugar production: 90 days in the summer and 100 days in the winter. With this context in mind, I‚Äôll use just the data after 2012 for now. ts_sugar_tbl %\u003e% mutate(year = year(date) %\u003e% as_factor(), month = month(date, label = TRUE, locale = Sys.setlocale(\"LC_COLLATE\", \"C\")) %\u003e% as_factor()) %\u003e% ggplot(aes(x = year, y = fct_rev(month), fill = total_tons)) + scale_fill_distiller(labels = scales::number_format(scale = 1e-6, suffix = \"M\"), direction = 1) + geom_tile(color = \"grey40\") + labs( title = \"Totals of Sugar Exports Across the Years and Months\", y = \"Months\", x = \"\", fill = \"Millions of\\nTons\", caption = \"linkedin.com/in/lucianobatistads/\" ) Looking at this heatmap, it‚Äôs visible that through the years the exports intensified by a huge quantity since 2012. I‚Äôll choose look just for the years after 2012 to modeling our time series. Modeling sugar time series # transforming target ---- # log ts_boxcox_sugar_tbl \u003c- ts_sugar_tbl %\u003e% select(-product) %\u003e% filter_by_time(date, .start_date = \"2012-01-01\") %\u003e% mutate(total_tons = box_cox_vec(total_tons)) boxcox_sugar_lambda \u003c- 0.645806609678906 # visualizing the transformations ---- ts_boxcox_sugar_tbl %\u003e% plot_time_series(date, total_tons) # splits ---- # boxcox sugar_boxcox_splits \u003c- time_series_split(ts_boxcox_sugar_tbl, assess = \"4 years\", cumulat","date":"2020-10-25","objectID":"/posts/demand_forecasting_brazilian_commodities/:4:3","tags":["Project","Data Science","Machine Learning","Time Series"],"title":"Demand Forecasting of Brazilian Commodities","uri":"/posts/demand_forecasting_brazilian_commodities/"},{"categories":null,"content":"Next Steps! (Important) Throughout the project, we could see that ARIMA family of models is a very powerfull method. But, there is so much machine learning and deep learning algorithms available to work with time series forecasting that this article would be too big if I putt all that here. Now, that we have a great understanding about our dataset and also we have a really nice baseline model for all three commodities (soybean, corn and sugar), we can go deeper in modeling and cover advanced topics. As a spoiler to the next part of this project, check this list: Much more different models (modeltime) Lot more feature engineering (recipes) Hyperparameter Tunning (tune) Resampling tecniques (modeltime.resample) Stacking and ensembles models (modeltime.ensemble) Thanks for your attention, any question please let me know, Bye! source: https://www.toppr.com/guides/business-economics/theory-of-demand/demand-forecasting/ source: https://blog.flexis.com/the-benefits-of-forecasting-in-planning-and-production ","date":"2020-10-25","objectID":"/posts/demand_forecasting_brazilian_commodities/:4:4","tags":["Project","Data Science","Machine Learning","Time Series"],"title":"Demand Forecasting of Brazilian Commodities","uri":"/posts/demand_forecasting_brazilian_commodities/"},{"categories":["Tutorials"],"content":" Correlation analysis is a key task when you‚Äôre exploring any dataset. The principal objective is to find linear relationships between features that can help to understanding the big picture. Probably, the best way to see correlations between variables is to use scatterplots, but in most of time you‚Äôre working with a high dimensional dataset with a high number of variables, in these situations you have two major problems: It‚Äôs a high computational task to plot lots of scatterplot, specially if you have a big dataset. Even if you can plot all scatterplots at once, the readability of the charts will be horrible, and you‚Äôll not see nothing useful. One great approach to solve this problem is calculating the coefficient of correlation and instead of having lots of scatterplots, you‚Äôll have a matrix showing how much it correlates your variables, assuming a range from -1 to 1, high negatively correlated to high positively correlated, respectively. This is definitely much faster to plot and easy to interpret. ","date":"2020-10-13","objectID":"/posts/correlation_analysis_in_r/:0:0","tags":["Data exploration","Data visualization","ggplot2","corrr","corplot","R"],"title":"Effective approach to analyze correlation coefficients","uri":"/posts/correlation_analysis_in_r/"},{"categories":["Tutorials"],"content":"Misinterpretation of the coefficient of correlation Before starting to code, it‚Äôs important to understand two topics about correlation analysis to drift reliable conclusions. First Sometimes, we misinterpret the value of coefficient of correlation and establish the cause-and-effect relationship, i.e. one variable causing the variation in the other variable. Actually, we cannot interpret in this way unless we have a powerful motive beside just the coefficient value. Correlation coefficient gives us a quantitative determination of a relationship between two variables X and Y, not information about the association between the two variables. Causation implies an invariable sequence ‚Äî A always leads to B ‚Äî whereas correlation is a measure of mutual association between two variables. Second Another aspect that we need to be aware of is the factors that influencing the size of the correlation coefficient and can also lead to misinterpretation, like: The size of the coefficient is very much dependent upon the variability of measured values in the correlated sample. The greater the variability, the higher will be the correlation, everything else being equal. The size of the coefficient is altered when an investigator selects an extreme group of subjects to compare these groups regarding certain behavior. The coefficient got from the combined data of extreme groups would be larger than the coefficient got from a random sample of the same group. Addition or dropping the extreme cases from the group can lead to a change in the coefficient‚Äôs size. Addition of the extreme case may increase the size of correlation, while dropping the extreme cases will lower the value of the coefficient. With that in mind, it‚Äôs a good idea to do a standardized step before looking for correlations, to minimize variability and extreme values. ","date":"2020-10-13","objectID":"/posts/correlation_analysis_in_r/:0:1","tags":["Data exploration","Data visualization","ggplot2","corrr","corplot","R"],"title":"Effective approach to analyze correlation coefficients","uri":"/posts/correlation_analysis_in_r/"},{"categories":["Tutorials"],"content":"How to do a correlation analysis in R? As everything in R, here there are also plenty packages to calculate and plot coefficients of correlation. It‚Äôs up to you to choose which package is better for your analysis. The aim of this article is to help you structure a optimized workflow for correlation analysis, creating great charts and consistent functions. For that will be necessary, especially two specific packages, corrplot and corrr. ","date":"2020-10-13","objectID":"/posts/correlation_analysis_in_r/:0:2","tags":["Data exploration","Data visualization","ggplot2","corrr","corplot","R"],"title":"Effective approach to analyze correlation coefficients","uri":"/posts/correlation_analysis_in_r/"},{"categories":["Tutorials"],"content":"Data As a dataset for this tutorial, I‚Äôll use some data from a personal project in development. I collected the data from a public API and from an electronic game called League of Legends. League of Legends is one of the most popular video games in the world. It is played by over 100 million active users every single month. Each team has a base they must guard from their opponents while simultaneously attacking their opponent‚Äôs base, there is the Blue team, whose base is in the lower left part of the map, and the Red team, whose base is in the upper right part of the map, at the back of each team‚Äôs base there is a building called The Nexus. You win the game by destroying the enemy team‚Äôs Nexus. During the match, there are a lot of statistics about performing each player (5 in each team) and also from the match itself, so, I created a script to collect all this information. The game also has a ranking by country and according to your performance you get a specific tier, that can be: iron, bronze, silver, gold, platinum, diamond, master, grandmaster and challenger, following the batter to worse order, respectively. Just to simplify the example, I‚Äôll show just the data for one particular tier of users, diamond players. I choose this dataset because we have a lot of variables, high dimensionality, and look for linear correlations can be useful to build our model in the next stage of the project. Let‚Äôs begin! # libraries used in this tutorial library(tidyverse) library(corrr) library(corrplot) library(tidyquant) # color pallete library(plotly) # dataset cleaned # download from kaggle: https://www.kaggle.com/dsluciano/league-of-legends-match-statistics/download lol_diamond_tbl \u003c- read_csv(\"data/lol_diamond_numeric_final.csv\") lol_diamond_tbl %\u003e% glimpse() Rows: 99,727 Columns: 68 $ totalDamageDealt \u003cdbl\u003e -0.2930711279, 2.5773485960, 0.2419916679, 0.7503652702, 0.9952605522, -0.04‚Ä¶ $ kills \u003cdbl\u003e 0.0153183, -0.2410509, 1.9523597, 0.9610752, 0.6312293, -0.5386355, 0.961075‚Ä¶ $ deaths \u003cdbl\u003e -0.1644351, -0.5187119, -1.8349225, -0.9050923, -0.1644351, -0.9050923, -1.3‚Ä¶ $ assists \u003cdbl\u003e -1.81311786, -0.06615988, -0.49817594, 0.46319384, 0.12308021, 0.61792164, 0‚Ä¶ $ largestKillingSpree \u003cdbl\u003e 0.32609942, -0.07849633, 2.84784752, 0.94628600, 0.32609942, -0.07849633, 0.‚Ä¶ $ largestMultiKill \u003cdbl\u003e -0.472330, -0.472330, 0.865404, 0.865404, -0.472330, -0.472330, 0.865404, 0.‚Ä¶ $ killingSprees \u003cdbl\u003e 0.68996109, -0.08011612, 0.68996109, 1.24585393, 0.68996109, -0.08011612, 1.‚Ä¶ $ longestTimeSpentLiving \u003cdbl\u003e 0.04987467, 1.71337317, -0.42769388, 1.73915199, 0.89870412, 1.01482638, 1.2‚Ä¶ $ doubleKills \u003cdbl\u003e -0.7575416, -0.7575416, 1.6941396, 1.1340473, -0.7575416, -0.7575416, 1.5003‚Ä¶ $ tripleKills \u003cdbl\u003e -0.2604183, -0.2604183, -0.2604183, -0.2604183, -0.2604183, -0.2604183, -0.2‚Ä¶ $ quadraKills \u003cdbl\u003e -0.1065788, -0.1065788, -0.1065788, -0.1065788, -0.1065788, -0.1065788, -0.1‚Ä¶ $ pentaKills \u003cdbl\u003e -0.04710454, -0.04710454, -0.04710454, -0.04710454, -0.04710454, -0.04710454‚Ä¶ (...) I cleaned the dataset before, and the script used for that is in my GitHub repo. Besides that, you can find the description of the variables used here on this kaggle kernel created by me. If you have any questions, please let me know. ","date":"2020-10-13","objectID":"/posts/correlation_analysis_in_r/:0:3","tags":["Data exploration","Data visualization","ggplot2","corrr","corplot","R"],"title":"Effective approach to analyze correlation coefficients","uri":"/posts/correlation_analysis_in_r/"},{"categories":["Tutorials"],"content":"Using corplot package This is a well-known library, that R users normally choose. The downside from corrplot package is we don‚Äôt have a ggplot object at the output, and because of that is difficult to customize the plot with something that you want. But the procedure to plot the correlation coefficients using corrplot is fairly simple: calculate the correlation matrix apply the corrplot() function customize Although the simplicity of this process, the final result it‚Äôs not so beautiful and it‚Äôs not the best chart to put in your report. # basic procedure corr_matrix_train_mtx \u003c- lol_diamond_tbl %\u003e% cor() corrplot(corr_matrix_train_mtx) One feature that most people don‚Äôt know is the possibility to group variables, creating clusters regarding the correlation coefficient. Definitely this can improve the visualization and interpretability of the result. In the following sequence of steps I‚Äôll show you how to improve the final result: Change the method parameter to put a square shape in the circle place (more easy to read). This way we can get the first plot. But as a personal taste I don‚Äôt like the grid, and so I change from square to color (second plot). Here we have a problem, as a correlation matrix is mirrored, you end up with too much information to analyze. To solve that, we take just the inferior triangle and get the plot 3. We can agree that red text is not the better color to put in your chart, for that we can change tl.col to black and also decrease the font size with tl.cex (plot 4). And the most important, we can rearrange the variables using clustering methods, the corrplot function accept 5 different ways: AOE, FPC, hclust and alphabet (look the documentation to see more). # first plot corrplot(corr_matrix_train_mtx, method = \"square\") # second plot corrplot(corr_matrix_train_mtx, method = \"color\") # third plot corrplot(corr_matrix_train_mtx, type = \"lower\", method = \"color\") # fourth plot corrplot(corr_matrix_train_mtx, method = \"color\", type = \"lower\", tl.col = \"black\", tl.cex = 0.5) # final plot corrplot(corr_matrix_train_mtx, method = \"color\", type = \"lower\", tl.col = \"black\", tl.cex = 0.5, order = \"hclust\",) Look that the final result (figure below) really helps you see distinct groups of variables that have similar correlations, and can be a start point to investigate specific groups later. ","date":"2020-10-13","objectID":"/posts/correlation_analysis_in_r/:0:4","tags":["Data exploration","Data visualization","ggplot2","corrr","corplot","R"],"title":"Effective approach to analyze correlation coefficients","uri":"/posts/correlation_analysis_in_r/"},{"categories":["Tutorials"],"content":"Using corrr package This second approach is the most tidy way to perform a correlation analysis. To facilitate the readability of this workflow, I drew this flowchart below: # plot 1 lol_diamond_tbl %\u003e% correlate() %\u003e% rearrange() %\u003e% shave() %\u003e% # rplot need to receive a correlation matrix rplot() It‚Äôs incredibly straightforward, and you just need to tune specific parameters to achieve a astonish chart. The standard configuration is shown in the next figure: Yes, it‚Äôs a mess and difficult to read. Let‚Äôs improve this plot following some customizations: Set the PCA method for rearrange the variables. Setting shape = 15, will put squares shape in circle places. You can also try different number to different shapes. I‚Äôll choose better colors. And at the end, I will set the x-axis label angle to 45 and hjust = 1, to put the axis text in right place. lol_diamond_tbl %\u003e% correlate(use = \"pairwise.complete.obs\") %\u003e% rearrange(method = \"PCA\") %\u003e% shave() %\u003e% # rplot need to receive a correlation matrix rplot(shape = 15, colours = c(\"darkorange\", \"white\", \"darkcyan\")) + theme_minimal() + theme( axis.text.x = element_text(angle = 45, hjust = 1) ) This is a much better chart! =) The corrr library can also use a lot of different clustering methods for rearranging the variables, and you can get the full list from the seriate package documentation. One more convenience in use corrr is that you can create plotly interactive plots and hover through specific squares to investigate the values of correlation coefficients and the variable in x and y axis play the video. g \u003c- lol_diamond_tbl %\u003e% correlate(use = \"pairwise.complete.obs\") %\u003e% rearrange(method = \"PCA\") %\u003e% corrr::shave() %\u003e% # rplot need to receive a correlation matrix rplot(shape = 15, colours = c(\"darkorange\", \"white\", \"darkcyan\")) + theme_minimal() + theme( axis.text.x = element_text(angle = 45, hjust = 1) ) plotly::ggplotly(g) Now that you already have a fully understanding in how to plot a good chart to investigate the correlation between your variables, I‚Äôll show you one specific approach to a more applicable task in your modeling workflow. See, the correlation between all variables is useful, but as we are investigating a correlation against one specific variable (target), a better way is to extract the correlation coefficients to that specific variable and plot it. ","date":"2020-10-13","objectID":"/posts/correlation_analysis_in_r/:0:5","tags":["Data exploration","Data visualization","ggplot2","corrr","corplot","R"],"title":"Effective approach to analyze correlation coefficients","uri":"/posts/correlation_analysis_in_r/"},{"categories":["Tutorials"],"content":"Creating the plot_cor() function In this last section, I‚Äôll show you two functions you can use to create beautiful plots and see which variable can contribute more to your correlation analysis. First, we need the get_cor() function, that will return the correlation matrix. After, we plug into plot_cor() function and get our awesome chart. # getting the correlation matrix get_cor \u003c- function(data, target, use = \"pairwise.complete.obs\", fct_reorder = FALSE, fct_rev = FALSE) { # meta programming to capture the variables # like tidy functions feature_expr \u003c- enquo(target) feature_name \u003c- quo_name(feature_expr) # get the corralating matrix # and also ensuring that the data is in the # right format data_cor \u003c- data %\u003e% mutate(across(where(is.character), as_factor)) %\u003e% mutate(across(where(is.factor), as.numeric)) %\u003e% cor(use = use) %\u003e% as_tibble() %\u003e% mutate(feature = names(.)) %\u003e% select(feature, !! feature_expr) %\u003e% filter(!(feature == feature_name)) # conditionals to sort the variables # very usefull to plot if (fct_reorder) { data_cor \u003c- data_cor %\u003e% mutate(feature = fct_reorder(feature, !! feature_expr)) %\u003e% arrange(feature) } if (fct_rev) { data_cor \u003c- data_cor %\u003e% mutate(feature = fct_rev(feature)) %\u003e% arrange(feature) } return(data_cor) } Second, we need the plot function. # this function plot the correlation scores in order of values # and have a lot of parameters that can be used to # fully customize your plot as you want plot_cor \u003c- function(data, target, fct_reorder = FALSE, fct_rev = FALSE, include_lbl = TRUE, lbl_precision = 2, lbl_position = \"outward\", size = 2, line_size = 1, vert_size = 1, color_pos = palette_light()[[1]], color_neg = palette_light()[[2]]) { # meta programming to capture the variables # like tidy functions feature_expr \u003c- enquo(target) feature_name \u003c- quo_name(feature_expr) data_cor \u003c- data %\u003e% get_cor(!! feature_expr, fct_reorder = fct_reorder, fct_rev = fct_rev) %\u003e% # used as label, and also putting the precision of the numbers mutate(feature_name_text = round(!! feature_expr, lbl_precision)) %\u003e% # labeling the correlation as negative and positive mutate(Correlation = case_when( (!! feature_expr) \u003e= 0 ~ \"Positive\", TRUE ~ \"Negative\") %\u003e% as.factor()) g \u003c- data_cor %\u003e% ggplot(aes_string(x = feature_name, y = \"feature\", group = \"feature\")) + geom_point(aes(color = Correlation), size = size) + geom_segment(aes(xend = 0, yend = feature, color = Correlation), size = line_size) + geom_vline(xintercept = 0, color = palette_light()[[1]], size = vert_size) + expand_limits(x = c(-1, 1)) + theme_tq() + scale_color_manual(values = c(color_neg, color_pos)) if (include_lbl) g \u003c- g + geom_label(aes(label = feature_name_text), hjust = lbl_position) return(g) } plot_cor(lol_diamond_tbl, totalDamageDealt, fct_reorder = T) And that is the final result. As we can see, goldEarned and goldSpent is more related to totalDamageDealt (target variable), and make sense because you earn gold when you kill/destroy players/minions/objectives in the game, hence, with more gold you spent more gold. Look, you can change the target variable and see the correlation between other variables too, just need to change the target parameter. I encourage you to explore different customization and change the default values for this function, and if you find any question, please let me know. I hope you enjoyed this tutorial, bye! ^^ LinkedIn: https://www.linkedin.com/in/lucianobatistads/ GitHub: https://github.com/LucianoBatista source: https://www.yourarticlelibrary.com/statistics-2/correlation-meaning-types-and-its-computation-statistics/92001 ","date":"2020-10-13","objectID":"/posts/correlation_analysis_in_r/:0:6","tags":["Data exploration","Data visualization","ggplot2","corrr","corplot","R"],"title":"Effective approach to analyze correlation coefficients","uri":"/posts/correlation_analysis_in_r/"},{"categories":["Tutorials"],"content":"Introduction This is the first of two articles that we‚Äôll talk about two different approaches to perform hypotheses tests, covering the classical and computational methodologies. In the end I‚Äôll show you one R package (Infer) capable to execute any of these methods in an easy, flexible, and less error-prone way. In the second article, we‚Äôll go deeper in a hands-on experiment using the Infer package, if you already know the package and want to see more code than text, click here. What is Hypothesis Test? Before start to talk about the hypothesis test I think that it‚Äôs important to know where we are in the ‚ÄòStatsVerse‚Äô. One didactic way is to start looking for the two major areas of Statistics: Descriptive Statistics: correspond to methods for collecting, describing, and analyzing a set of data without drawing any conclusions about a large group. Inferential Statistics: correspond to methods for the analysis of a subset of data leading to predictions or inferences about the entire set of data. That been said, hypothesis tests are part of Inferential Statistics and allow us to take a sample of data from a population and infer about the plausibility of competing hypotheses. Sadly, here we have a lot of terminologies, notations, and definitions, and all that is important to understand the following topics of the article, because of this I prepare the table bellow with some short explanations you can use, if needed, to remember some of these concepts. Now you already refresh your mind about the hypothesis test, we can think about how to perform them. In that case, there is a classical methodology, based on very well known tests (t, Z, Chi-squared‚Ä¶), and there is the other one I didn‚Äôt know until coming across with this incredible article ‚ÄúThere is Only one Test‚Äù, based on a general framework and resampling techniques (permutation and bootstrap), called computational methodology. So, you may ask me ‚ÄúWhy should I care about this computational approach?‚Äù. And that is the answer I intend to give you at the end of the article =D. Let‚Äôs see now how we can proceed in order to perform a hypothesis test using these different tests. Classical Methodology The way that you do a hypothesis test by the classical approach is based on these following steps: State the hypotheses. Identify the test statistic and its probability distribution. Specify the significance level. State the decision rule. Collect the data and perform the calculations. Make the statistical decision. Make the business decision. You probably noticed that the second step is bold, and that is because the most difficult part is identifying how the test should be used based on the data you have, it‚Äôs not a trivial task, and you may probably end up googling and find images like this: or this: or even worse‚Ä¶ All these tests are based on assumptions that you need to follow in order to get a significative response. Otherwise, your answer will be misleading, that is, you may find a relationship between variables that is not true and vice-versa. So, if existed one way to generalize this second step? Would be great, right? In fact, there is and we go see now! Computational Methodology This computational methodology is based on a framework developed by Allen Downey, a computer scientist, author of Think Stats, and other books. Allen is a Professor of Computer Science at Olin College of Engineering. He has a Ph.D. in Computer Science from U.C. Berkeley and Master‚Äôs and Bachelor‚Äôs degrees from MIT. He observed a pattern in the way that hypotheses tests are applied and came up with one more generalized approach to perform these tests. The author believes that when computation was expensive, the shortcuts offered by all kinds of statistical tests were very important and necessary to calculate p-values, but now that computation is in a completely different level, they lost your weight. The solution brought by him is to use simulation. That is, if you can construct a model of the null hypothesis","date":"2020-09-20","objectID":"/posts/hype_test/:0:0","tags":["Hypothesis Testing","Data Science","Statistics","R","Infer"],"title":"Hypothesis Testing by Computational Methodology - Part 1","uri":"/posts/hype_test/"},{"categories":["Tutorials"],"content":"Framework The figure below it‚Äôs showing the framework proposed by Allen Downey. In order to describe the elements of this framework, I‚Äôll put here what was wrote by the author: Given a dataset, you compute a test statistic that measures the size of the apparent effect. For example, if you are describing a difference between the two groups, the test statistic might be the absolute difference in means. He calls the test statistic from the observed data Œ¥.* Next, you define a null hypothesis, which is a model of the world under the assumption that the effect is not real. For example, if you think there might be a difference between the two groups, the null hypothesis would assume that there is no difference. Your model of the null hypothesis should be stochastic. That is, capable of generating random datasets similar to the original dataset. Now, the goal of classical hypothesis testing is to compute a p-value, which is the probability of seeing an effect as big as Œ¥ under the null hypothesis. You can estimate the p-value by using your model of the null hypothesis to generate many simulated datasets. For each simulated dataset, compute the same test statistic you used on the actual data. Finally, count the fraction of times the test statistic from simulated data exceeds Œ¥. This fraction approximates the p-value. If it‚Äôs sufficiently small, you can conclude that the apparent effect is unlikely to be due to chance. That‚Äôs it, all hypothesis tests fit into this framework. ","date":"2020-09-20","objectID":"/posts/hype_test/:1:0","tags":["Hypothesis Testing","Data Science","Statistics","R","Infer"],"title":"Hypothesis Testing by Computational Methodology - Part 1","uri":"/posts/hype_test/"},{"categories":["Tutorials"],"content":"Pros \u0026 Cons Like everything we always have pros and cons, and here there is no difference. Pros Cons Easy Easy just for programmers Less error-prone Big data can take longer to run Flexible Fast enough Look that these two cons can be easily managed if you want: if you are not comfortable with programming, the Infer package makes it easier to execute this framework. although be fast, if you are working with big data, tasks like permutation can be very computationally expensive. But, nothing stops you to get a sample from your big dataset to perform hypotheses tests. I‚Äôm not saying that the classical methodology it‚Äôs not necessary anymore, but just showing another way (less error-prone) we can take advantage of all computational power to optimize this task. ","date":"2020-09-20","objectID":"/posts/hype_test/:2:0","tags":["Hypothesis Testing","Data Science","Statistics","R","Infer"],"title":"Hypothesis Testing by Computational Methodology - Part 1","uri":"/posts/hype_test/"},{"categories":["Tutorials"],"content":"Infer Package This package was developed by Andrew Bray and collaborators, and the objective is exactly to perform statistical inference using an expressive statistical grammar that coheres with the design framework showed before. The package is centered around 4 main verbs, supplemented with many utilities to visualize and extract value from their outputs. specify(): allows you to specify the variable or relationship between variables, that you‚Äôre interested in. hypothesize(): allows you to declare the null hypothesis. generate(): allows you to generate data reflecting the null hypothesis. calculate(): allows you to calculate a distribution of statistics from the generated data to form the null distribution. The flow is straightforward, like the image below: To learn more about the principles underlying the package, stay tuning to the next article! Sources Allen Downey article Hypothesis testing theory Andrew Bray RStudio talk ","date":"2020-09-20","objectID":"/posts/hype_test/:3:0","tags":["Hypothesis Testing","Data Science","Statistics","R","Infer"],"title":"Hypothesis Testing by Computational Methodology - Part 1","uri":"/posts/hype_test/"},{"categories":["Tutorials"],"content":"What is it correlation analysis? The concept of correlation is the same used in non-time series data: identify and quantify the relationship between two variables. Due to the continuous and chronologically ordered nature of time series data, there is a likelihood that there will be some degree of correlation between the series observations. Measuring and analyzing the correlation between two variables, in the context of time series analysis, can be understood by two different aspects: Analyzing the correlation between a series and its lags, as some of the past lags may contain predictive information, which can be utilized to forecast events of the series. One of the most popular methods for measuring the level of correlation between a series and its lags is the autocorrelation function and partial autocorrelation function. Analyzing the correlation between two series in order to identify exogenous factors or predictors, which can explain the variation of the series over time. In this case, the measurement of correlation is typically done with the cross-correlation function. Looking at these characteristics can be very useful to find new features to use in the modeling step, also to understand patterns of behavior throughout the time. How to Perform Correlation Analysis in Time Series Data Using R? Finding a way to do this in R can be overwhelming, because of the vast quantity of packages, and each package use one different kind of object. The objective of this article is to walk you through three different ways of doing the correlation analysis, which the last one is a general (tidy) way, and also what I prefer. A short disclaimer before start, this article is not meant to explain all the theory behind the correlation analysis and for a more complete explanation, you can access this great reference: Forecasting: Principles and Pratice. The Different Approaches Let‚Äôs set our environment: # packages for this tutorial # first approach library(feasts) library(tsibble) library(lubridate) # second approach library(TSstudio) library(plotly) # third approach library(tidyverse) library(timetk) library(lubridate) ","date":"2020-09-15","objectID":"/posts/r_correlation/:0:0","tags":["Time Series","Data Science","Autocorrelation","R"],"title":"How to Perform Correlation Analysis in Time Series data using R?","uri":"/posts/r_correlation/"},{"categories":["Tutorials"],"content":"Data We‚Äôll use a dataset from Stack Overflow, that have the numbers of questions for each month from 2009 to 2019, in different topics. You can access the data using this link. The dataset contains 9 different features regarding keywords used in Stack Overflow questions, but here, we‚Äôll use just r and python columns. # reading the dataset stackoverflow_raw \u003c- read_csv(\"00_data/MLTollsStackOverflow.csv\") # selecting columns to use in this tutorial stackoverflow_tbl \u003c- stackoverflow_raw %\u003e% select(month, r, python) I‚Äôll do one basic transformation in this dataset that must help-us during the tutorial and in all methods. # convert to date stackoverflow_prep_tbl \u003c- stackoverflow_tbl %\u003e% mutate(date = str_glue(\"20{month}\"), date = yearmonth(date) %\u003e% as_date()) %\u003e% select(date, r, python, -month) # A tibble: 132 x 3 date r python \u003cdate\u003e \u003cdbl\u003e \u003cdbl\u003e 1 2009-01-01 8 631 2 2009-02-01 9 633 3 2009-03-01 4 766 4 2009-04-01 12 768 5 2009-05-01 2 1003 6 2009-06-01 5 1046 7 2009-07-01 51 1165 8 2009-08-01 47 1143 9 2009-09-01 139 1169 10 2009-10-01 73 1424 # ‚Ä¶ with 122 more rows Now we can start!! ","date":"2020-09-15","objectID":"/posts/r_correlation/:1:0","tags":["Time Series","Data Science","Autocorrelation","R"],"title":"How to Perform Correlation Analysis in Time Series data using R?","uri":"/posts/r_correlation/"},{"categories":["Tutorials"],"content":"First Approach The first method that I want to show you use the ts object, a built-in format in R for time series. I choose this approach because with the ts object we have a good integration with TSstudio package and nice interactive functions to use. # ts function is responsible to convert to ts object stackOverflow_ts \u003c- ts(data = stackoverflow_prep_tbl[, c(\"python\", \"r\")], # selecting 2 variables start = c(2009, 01), # start date end = c(2019, 12), # end date frequency = 12) Now you can see all information about the time series that was created with the ts function. ts_info(stackOverflow_ts) The stackOverflow_ts series is a mts object with 2 variables and 132 observations Frequency: 12 Start time: 2009 1 ` End time: 2019 12 With this summary, it‚Äôs possible to see that we have two variables, that represent two different time series. Each time series represent one feature (r and python). Lets visualize the time series. ts_plot(stackOverflow_ts, title = \"Monthly questions on Stack Overflow platform\", Ytitle = \"# Questions\", Xtitle = \"Year\") With this graph, we see that python has a more pronounced trend than R regarding the number of questions made in the stack overflow platform. The time series doesn‚Äôt have a seasonal pattern, but it‚Äôs possible to see some cyclicality in both time series. Let‚Äôs look at the correlation plots! ","date":"2020-09-15","objectID":"/posts/r_correlation/:2:0","tags":["Time Series","Data Science","Autocorrelation","R"],"title":"How to Perform Correlation Analysis in Time Series data using R?","uri":"/posts/r_correlation/"},{"categories":["Tutorials"],"content":"Lags Analysis I The TSstudio package doesn‚Äôt have any function to plot the ACF and PACF measures, and for that, we use the built-in functions acf and pacf. The downside here, for me, is that the object generated by these functions is not a ggplot2 object, and we lose all beautiful features from the grammar of graphics. Obviously, you can go through the data and try to plot in the ggplot2 by yourself but will be a little burdensome. For R time series: par(mfrow = c(1, 2)) # acf R time series stackOverflow_ts[, c(\"r\")] %\u003e% acf(lag.max = 300, main = \"Autocorrelation Plot - R\") # pacf R time series stackOverflow_ts[, c(\"r\")] %\u003e% pacf(lag.max = 300, main = \"Partial Autocorrelation Plot - R\") For Python time series: # acf Python time series stackOverflow_ts[, c(\"python\")] %\u003e% acf(lag.max = 300, main = \"Autocorrelation Plot - Python\") # pacf Python time series stackOverflow_ts[, c(\"python\")] %\u003e% pacf(lag.max = 300, main = \"Python Autocorrelation Plot - Python\") In all graphics we have similar characteristics that need explanation: blue lines: These lines are referent to a significance interval, the bars that run through these lines have statistical meaning. y-axis: Correlation scores. x-axis: Here we have the indication of the lags. Each lag corresponds 12 months, so, almost 11 lags. Note that looking at ACF plots, both for R and Python time series, we have a greater correlation with more recent lags, which is lost over time. With more distant lags it is possible to see that we have a negative correlation with recent data. The Partial Autocorrelation is a little different, this ‚Äúpartial‚Äù correlation between two variables is the amount of correlation between them which is not explained by their mutual correlations with a specified set of other variables. For example, if we are regressing a variable Y on other variables X1, X2, and X3, the partial correlation between Y and X3 is the amount of correlation between Y and X3 that is not explained by their common correlations with X1 and X2. To summarize, when we look at the PACF plot, we want to know each lag that has relevant information to use as a predictor in a future forecast. How much greater the PACF score, the better. And, in our plots, we see that both time series have one lag (closer to lag 1) that may be useful. Just know which correlation score has the higher score is not enough, it‚Äôs important to see visually how these points are distributed. In that, TSstudio has a great interactive function, called ts_lags. # Looking at lag plots ts_lags(stackOverflow_ts, lags = c(2, 20, 30, 40, 50, 80)) %\u003e% # choosing what lags to plot layout(title = \"Series vs Lags\") ","date":"2020-09-15","objectID":"/posts/r_correlation/:2:1","tags":["Time Series","Data Science","Autocorrelation","R"],"title":"How to Perform Correlation Analysis in Time Series data using R?","uri":"/posts/r_correlation/"},{"categories":["Tutorials"],"content":"Causality analysis I Just look at the past pattern within the series is not always a good idea. The main pitfall of this method is that it will fail whenever the changes in the series derive from the exogenous factors. The goal of causality analysis, in the context of time series analysis, is to identify whether a causality relationship exists between the series we wish to forecast other potential exogenous factors. Be careful about the fact that correlation doesn‚Äôt imply causation, we just looking for something that could help the forecast model. In our current case, we just can see if exist some lag in R time series correlated to Python time series. # ccf time series par(mfrow=c(1,1)) ccf(stackOverflow_ts[, c(\"r\")], stackOverflow_ts[, c(\"python\")], lag.max = 300, main = \"Cros-Correlation Plot\", ylab = \"CCF\") We see that the higher score is in the recent lags, and some scores between 5 and 10 have a negative relationship. The graphic is saying to us that currently (at the date of the dataset) the growth of R questions is highly correlated to Python questions. ","date":"2020-09-15","objectID":"/posts/r_correlation/:2:2","tags":["Time Series","Data Science","Autocorrelation","R"],"title":"How to Perform Correlation Analysis in Time Series data using R?","uri":"/posts/r_correlation/"},{"categories":["Tutorials"],"content":"Second Approach This second approach uses what is called tsibble, like a tibble with an implicit date index. So, let‚Äôs convert our tibble to a tsibble. stackoverflow_prep_tsbl \u003c- stackoverflow_prep_tbl %\u003e% mutate(date = yearmonth(date)) %\u003e% pivot_longer(cols = c(r, python), names_to = \"names\", values_to = \"value\") %\u003e% as_tsibble(key = names,index = date) # specifying the time series in the object Differently of what we did before, here the data was converted to a long format, better to plot. Looking at the object created, we see the indication of an interval of these observations [1M] (monthly). And also, we see the key variable, that it‚Äôs a way to indicate how many time series exist in this object. It‚Äôs possible to have different combinations of features to do a new time series in the object. As we have the objects ready, we can now perform the correlation analysis by this second approach. Attempt that the graphic interpretation was already made, and here I‚Äôll just explain the difference between the methods. ","date":"2020-09-15","objectID":"/posts/r_correlation/:3:0","tags":["Time Series","Data Science","Autocorrelation","R"],"title":"How to Perform Correlation Analysis in Time Series data using R?","uri":"/posts/r_correlation/"},{"categories":["Tutorials"],"content":"Lag Analysis II Unlike TSstudio, we do not have interactivity plots, all plots are static, but they are objects of ggplot2 and allow us to use the concepts of the grammar of graphics. A problem with these functions is that they may behave differently than expected. Plugging the data to the acf and pacf functions, we are able to automatically see the faceted plots between all time series within the object. But if we try to simply visualize the series over time, it will not be possible to facet. It‚Äôs not a big problem, and you can plot using ggplot normally (see the code). # looking at the data stackoverflow_prep_tsbl %\u003e% feasts::autoplot() # looking at the data facet way stackoverflow_prep_tsbl %\u003e% ggplot(aes(x = date, y = value, color = names)) + geom_line() + facet_wrap(~names, scales = \"free_y\") # lag plots stackoverflow_prep_tsbl %\u003e% filter(names == \"python\") %\u003e% gg_lag(value, geom=\"point\") # acf stackoverflow_prep_tsbl %\u003e% ACF(value, lag_max = 300) %\u003e% autoplot() # pacf stackoverflow_prep_tsbl %\u003e% PACF(value, lag_max = 300) %\u003e% autoplot() ","date":"2020-09-15","objectID":"/posts/r_correlation/:3:1","tags":["Time Series","Data Science","Autocorrelation","R"],"title":"How to Perform Correlation Analysis in Time Series data using R?","uri":"/posts/r_correlation/"},{"categories":["Tutorials"],"content":"Casuality Analysis II # ccf stackoverflow_prep_tsbl %\u003e% pivot_wider(names_from = names, values_from = value) %\u003e% CCF(python, r, lag_max = 300) %\u003e% autoplot() ","date":"2020-09-15","objectID":"/posts/r_correlation/:3:2","tags":["Time Series","Data Science","Autocorrelation","R"],"title":"How to Perform Correlation Analysis in Time Series data using R?","uri":"/posts/r_correlation/"},{"categories":["Tutorials"],"content":"Third Approach (Tidy Approach) Until now we have changed our original object several times, but using the timetk package you can perform all these analyzes using just the original object, a tibble data frame. Making analysis much more intuitive, and also integrating with all tidyverse packages. ","date":"2020-09-15","objectID":"/posts/r_correlation/:4:0","tags":["Time Series","Data Science","Autocorrelation","R"],"title":"How to Perform Correlation Analysis in Time Series data using R?","uri":"/posts/r_correlation/"},{"categories":["Tutorials"],"content":"Lag Analysis III You basically use 2 functions to perform all the correlation analysis with much less code and more flexibility. The only thing that you need to do is put the dataset in a long format. # passing to a long format stackoverflow_prep_long_tbl \u003c- stackoverflow_prep_tbl %\u003e% pivot_longer(cols = c(r, python), names_to = \"names\", values_to = \"value\") # the series stackoverflow_prep_long_tbl %\u003e% plot_time_series(.date_var = date, .value = value, .facet_vars = names) # acf/pacf plots stackoverflow_prep_long_tbl %\u003e% group_by(names) %\u003e% plot_acf_diagnostics(.date_var = date, .value = value, .show_white_noise_bars = T) ","date":"2020-09-15","objectID":"/posts/r_correlation/:4:1","tags":["Time Series","Data Science","Autocorrelation","R"],"title":"How to Perform Correlation Analysis in Time Series data using R?","uri":"/posts/r_correlation/"},{"categories":["Tutorials"],"content":"Causality Analysis III In the real world, the time series you want to compare will probably not be in the same range as here, and you will probably need to do some transformation to put in the same interval and only then calculate the ccf score. # CCF plot stackoverflow_prep_tbl %\u003e% plot_acf_diagnostics(.date_var = date, .value = r, .ccf_vars = python, .show_ccf_vars_only = T) So, the main takeaways in this tutorial are that the timetk package is a great boost for your exploratory data analysis in time series context. The functions are consistent and we have interactivity in all plots if we want. timetk is able to do also, other different analysis: Seasonality Anomaly STL Decompostion Regression Plots I hope you find something useful with this tutorial, if you want to contact me, check the links: LinkedIn gitHub =] ","date":"2020-09-15","objectID":"/posts/r_correlation/:4:2","tags":["Time Series","Data Science","Autocorrelation","R"],"title":"How to Perform Correlation Analysis in Time Series data using R?","uri":"/posts/r_correlation/"},{"categories":["Projetos"],"content":"Segmenta√ß√£o de clientes de Food Delivery Neste projeto o objetivo √© segmentar clientes de um food delivery. A segmenta√ß√£o permite que profissionais de marketing e product managers possam identificar subconjuntos de p√∫blico-alvo para melhor adaptar suas estrat√©gias. O dataset utilizado foi cedido pela Data Science Academy, e pode ser encontrado no meu github juntamente com todo o c√≥digo apresentado aqui. Um breve overview do que ser√° abordado no projeto: Prepara√ß√£o de dados Visualiza√ß√£o de dados Padroniza√ß√£o de vari√°veis Clusteriza√ß√£o com K-Means Avalia√ß√£o do melhor n√∫mero de cluster com o √≠ndice de Calinski-Harabasz Bootstrap para avaliar consist√™ncia do cluster Visualiza√ß√£o dos clusters Sem mais delongas, vamos ao c√≥digo! ","date":"2020-06-19","objectID":"/posts/segment-client/:0:0","tags":["Project","Data Science","Machine Learning","K-Means"],"title":"Segmenta√ß√£o de clientes de Food Delivery","uri":"/posts/segment-client/"},{"categories":["Projetos"],"content":"Libs library(tidyverse) library(scales) library(lubridate) library(ggridges) library(gghalves) library(tidytext) library(tidymodels) library(wrapr) library(fpc) library(wesanderson) library(skimr) ","date":"2020-06-19","objectID":"/posts/segment-client/:1:0","tags":["Project","Data Science","Machine Learning","K-Means"],"title":"Segmenta√ß√£o de clientes de Food Delivery","uri":"/posts/segment-client/"},{"categories":["Projetos"],"content":"An√°lise Explorat√≥ria dos Dados Vamos importar os dados e visualizar quais s√£o as vari√°veis que iremos trabalhar ao longo desse projeto. orders_client_raw \u003c- read_csv(\"data/dataset.csv\") glimpse(orders_client_raw) Rows: 260,645 Columns: 8 $ id_transacao \u003cchr\u003e \"0x7901ee\", \"0x7901ee\", \"0x7901ee\", \"0x12b47f\", \"0x12b47f\", \"0x6d6979\", \"0x6d6979\", \"0x78dd1e\", \"0x78dd1e\", \"0x78dd1e\", \"0x4df8ab\", \"‚Ä¶ $ horario_pedido \u003cdttm\u003e 2019-01-16 18:33:00, 2019-01-16 18:33:00, 2019-01-16 18:33:00, 2019-09-04 12:36:00, 2019-09-04 12:36:00, 2019-03-18 00:27:00, 2019-0‚Ä¶ $ localidade \u003cdbl\u003e 7, 7, 7, 3, 3, 6, 6, 2, 2, 2, 8, 8, 6, 6, 7, 7, 7, 7, 6, 6, 6, 2, 2, 2, 7, 7, 7, 7, 2, 2, 6, 6, 6, 6, 2, 2, 2, 9, 9, 9, 9, 9, 9, 5, 5‚Ä¶ $ nome_item \u003cchr\u003e \"bebida\", \"pizza\", \"sobremesa\", \"salada\", \"sobremesa\", \"pizza\", \"sobremesa\", \"bebida\", \"pizza\", \"sobremesa\", \"salada\", \"sobremesa\", \"‚Ä¶ $ quantidade_item \u003cdbl\u003e 2, 2, 2, 1, 1, 2, 2, 2, 2, 2, 3, 3, 1, 1, 3, 3, 1, 4, 1, 4, 4, 1, 2, 2, 2, 2, 1, 3, 1, 1, 1, 1, 3, 4, 1, 2, 2, 4, 4, 4, 4, 4, 4, 1, 1‚Ä¶ $ latitude \u003cdbl\u003e 41.79413, 41.79413, 41.79413, 41.88449, 41.88449, 41.78458, 41.78458, 42.04931, 42.04931, 42.04931, 41.89420, 41.89420, 41.78458, 41.‚Ä¶ $ longitude \u003cdbl\u003e -88.01014, -88.01014, -88.01014, -87.62706, -87.62706, -87.60756, -87.60756, -87.67761, -87.67761, -87.67761, -87.62096, -87.62096, -‚Ä¶ $ nome_item_fac \u003cfct\u003e bebida, pizza, sobremesa, salada, sobremesa, pizza, sobremesa, bebida, pizza, sobremesa, salada, sobremesa, pizza, sobremesa, bebida,‚Ä¶ A n√≠vel de melhor compreender os dados podemos estabelecer um dicion√°rio com o que cada preditor representa: id_transacao: ID da transa√ß√£o. Um mesmo ID pode ter v√°rios itens de um pedido. horario_pedido: Hor√°rio exato do pedido. localidade: Localidade que processou o pedido (unidade do restaurante). nome_item: Nome do item (pizza, salada, bebida, sobremesa). quantidade_item: Quantidade de itens no pedido. latitude: Latitude da localidade onde o pedido foi gerado. longitude: Longitude da localidade onde o pedido foi gerado. Observamos que a vari√°vel nome_item possui um tipo de character, por√©m √© mais indicado trabalhar com a mesma como fator. orders_client_raw$nome_item_fac \u003c- as.factor(orders_client_raw$nome_item) No R temos um pacote muito interessante que permite ter um panorama geral sobre os dados, com v√°rias estat√≠sticas mais b√°sicas, chamado skimr. skim(orders_client_raw) ‚îÄ‚îÄ Data Summary ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ Values Name orders_client_raw Number of rows 260645 Number of columns 8 _______________________ Column type frequency: character 2 factor 1 numeric 4 POSIXct 1 ________________________ Group variables None ‚îÄ‚îÄ Variable type: character ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ skim_variable n_missing complete_rate min max empty n_unique whitespace 1 id_transacao 0 1 7 8 0 100000 0 2 nome_item 0 1 5 9 0 4 0 ‚îÄ‚îÄ Variable type: factor ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ skim_variable n_missing complete_rate ordered n_unique top_counts 1 nome_item_fac 0 1 FALSE 4 sob: 100000, piz: 76122, beb: 46156, sal: 38367 ‚îÄ‚îÄ Variable type: numeric ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ skim_variable n_missing complete_rate mean sd p0 p25 p50 p75 p100 hist 1 localidade 0 1 5.13 2.55 1 3 5 7 9 ‚ñÜ‚ñÜ‚ñÇ‚ñá‚ñÜ 2 quantidade_item 0 1 2.45 1.33 1 1 2 4 5 ‚ñá‚ñÜ‚ñÖ‚ñÖ‚ñÇ 3 latitude 0 1 41.8 0.144 41.5 41.8 41.9 41.9 42.0 ‚ñÇ‚ñÅ‚ñÖ‚ñá‚ñÇ 4 longitude 0 1 -87.7 0.136 -88.0 -87.8 -87.7 -87.6 -87.6 ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñá ‚îÄ‚îÄ Variable type: POSIXct ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ skim_variable n_missing complete_rate min max median n_unique 1 horario_pedido 0 1 2019-01-01 00:00:00 2019-12-30 23:59:00 2019-07-01 11:49:00 76799 De mais importante no resumo acima, podemos ver que os dados n√£o possuem valores missing (=D), temos 100.000 valores √∫nicos no id_transacao e temos 4 produtos de diferentes categorias. Localidade poss√≠velmente pr","date":"2020-06-19","objectID":"/posts/segment-client/:2:0","tags":["Project","Data Science","Machine Learning","K-Means"],"title":"Segmenta√ß√£o de clientes de Food Delivery","uri":"/posts/segment-client/"},{"categories":["Projetos"],"content":"Pivotando order_client_clustering \u003c- orders_client_raw %\u003e% select(id_transacao, nome_item_fac, quantidade_item) %\u003e% pivot_wider(id_transacao, names_from = nome_item_fac, values_from = quantidade_item, values_fill = 0) order_client_clustering # A tibble: 100,000 x 5 id_transacao bebida pizza sobremesa salada \u003cchr\u003e \u003cdbl\u003e \u003cdbl\u003e \u003cdbl\u003e \u003cdbl\u003e 1 0x7901ee 2 2 2 0 2 0x12b47f 0 0 1 1 3 0x6d6979 0 2 2 0 4 0x78dd1e 2 2 2 0 5 0x4df8ab 0 0 3 3 6 0x3be6d3 0 1 1 0 7 0x755a0b 3 3 4 1 8 0x653685 1 4 4 0 9 0x44fe1b 1 2 2 0 10 0x8ade21 2 2 3 1 # ‚Ä¶ with 99,990 more rows Como falamos antes, o k-means e outros algor√≠tmos dessa classe de modelos, trabalham com dist√¢ncias, por isso √© extremamente interessante que tenhamos os dados numa mesma escala e unidade. Aqui aplicaremos uma padroniza√ß√£o (m√©dia = 0 e desvio = 1), assumindo assim que nossa distribui√ß√£o √© uma distribui√ß√£o normal, o que de fato vimos que na maioria dos casos essa premissa se fez verdade. ","date":"2020-06-19","objectID":"/posts/segment-client/:3:0","tags":["Project","Data Science","Machine Learning","K-Means"],"title":"Segmenta√ß√£o de clientes de Food Delivery","uri":"/posts/segment-client/"},{"categories":["Projetos"],"content":"Padroniza√ß√£o # vari√°veis para padronizar vars_to_use \u003c- colnames(order_client_clustering)[-1] pmatrix \u003c- scale(order_client_clustering[, vars_to_use]) pcenter \u003c- attr(pmatrix, \"scaled:center\") # par√¢metro que foi utilizado para centralizar pscale \u003c- attr(pmatrix, \"scaled:scale\") # par√¢metro que foi utilizado para escalonar rm_scales \u003c- function(scaled_matrix) { attr(scaled_matrix, \"scaled:center\") \u003c- NULL attr(scaled_matrix, \"scaled:scale\") \u003c- NULL scaled_matrix } # matriz que utilizaremos para modelagem pmatrix \u003c- rm_scales(pmatrix) ","date":"2020-06-19","objectID":"/posts/segment-client/:4:0","tags":["Project","Data Science","Machine Learning","K-Means"],"title":"Segmenta√ß√£o de clientes de Food Delivery","uri":"/posts/segment-client/"},{"categories":["Projetos"],"content":"K-means Antes de iniciar a implementa√ß√£o do k-means, vale pontuar algumas quest√µes importantes sobre o algor√≠timo e que levaram a adotar o workflow abaixo. O k-means agrupa os dados para um n√∫mero pr√©-determinado de k-clusters. Ent√£o √© interessante ecolher um n√∫mero de clusters anteriomente. O k-means inicia o processo a partir de um ponto aleat√≥rio do seu espa√ßo de dados, por isso uma boa pr√°tica √© rodar o modelo v√°rias vezes para observar o comportamento dos dados nos clusters. Os grupos (clusters) formados pelo k-means n√£o necessariamente ser√£o ‚Äúclusters reais‚Äù (explico abaixo). E para isso teremos que avaliar a estabilidade desses clusters. O √∫ltimo ponto tr√°s o conceito de ‚Äúclusters reais‚Äù, que de forma simplificada nada mais √© do que um cluster de pontos que n√£o s√£o bem explicados por nenhum outro cluster. Para verificar isso vamos utilizar uma reamostragem bootstrap e rodar o kmeans() v√°rias vezes e em diferentes por√ß√µes dos dados e medir isso. E por fim, iremos utilizar o index Calinski-Harabasz para identificar o melhor cluster. Existem v√°aarias formas de fazer isso, mas eu preferi escolher essa e aqui segue um link caso voc√™ queira entender melhor sobres as defini√ß√µes matem√°ticos por tr√°s desse m√©todo, muito bem explicado por sinal. # 1 - kmeansruns para encontrar o melhor n√∫mero de cluster e visualizar o calinski-harabasz index # 4 - clusterboot() para analisar a estabilidade do cluster # 2 - col_to_print # 3 - print_cluster() # 5 - visualizar cada grupo # kmeansruns nos nossos dados padronizados, com um range em k de 1 a 10, e crit√©rio do calinski clustering_ch \u003c- kmeansruns(pmatrix, krange = 1:10, criterion = \"ch\") # best k retornado pelo modelo clustering_ch$bestk # pequeno tratamento nos dados para poder visualizar o CH-index em fun√ß√£o do cluster coef \u003c- clustering_ch$crit k \u003c- 1:10 data \u003c- tibble(coef, k) ggplot(data, aes(k, coef)) + geom_point() + geom_line(linetype=\"dashed\") + scale_x_continuous(breaks = 0:10) + theme_minimal() + labs(y = \"Calinski-Harabasz Index\") Quanto maior o √≠ndice, melhor. Ent√£o eu optei por escolher dois dos maiores pontos (3 e 8 clusters) e visualizar como se d√° o comportamento deles quando realizado o bootstrap. # essa c√©lula pode demorar um pouco para rodar dependendo da sua m√°quina # running o clusterboot pra melhor escolher o k # k = 3 cboot_3 \u003c- clusterboot(pmatrix, clustermethod = kmeansCBI, runs = 100, iter.max = 100, krange = 3, seed = 2020) # k = 8 cboot_8 \u003c- clusterboot(pmatrix, clustermethod = kmeansCBI, runs = 100, iter.max = 100, krange = 8, seed = 2020) No objeto cboot_x retornado pela fun√ß√£o clusterboot() podemos chamar o camapo $bootmean que indica justamente qual foi a estabilidade do cluster ao longo das 100 itera√ß√µes do bootstrap. \u003e cboot_3$bootmean [1] 1 1 1 \u003e cboot_8$bootmean [1] 0.6846583 0.9253406 0.8652381 0.7604600 0.6980723 0.9149958 0.8921955 0.7588892 O que vemos aqui √© que estranhamente o modelo com 3 clusters tem uma estabilidade de 1, o que seria perfeito em se tratando que o range de valores poss√≠veis dessa t√©cnica √© de 0 a 1, provavelmente tem alguma coisa errada com esse n√∫mero de clusters j√° que nada na vida √© t√£o perfeito assim. Por isso, vamos optar de seguir com 8 clusters, utilizando ent√£o o objeto cboot_8. O que os n√∫meros do $bootmean nos dizem? clusters com valores menores que 0.6 s√£o considerados inst√°veis. valores entre 0.6 e 0.75 indicam que o cluster encontrou algum n√≠vel de padr√£o nos dados. valores acima disso possui maior estabilidade, sendo os mais est√°veis os valores acima de 0.85. # groups recebe os datasets com os cluster groups \u003c- cboot_8$result$partition # colunas que ser√£o utilizadas dataset final com os clusters cols_to_print \u003c- wrapr::qc(id_transacao, bebida, pizza, sobremesa, salada) print_clusters \u003c- function(data, groups, columns) { groupedID \u003c- split(data, groups) lapply(groupedID, function(df) df[, columns]) } clusters \u003c- print_clusters(order_client_clustering, groups, cols_to_print) Nesse ponto, o que temos √© uma lista co","date":"2020-06-19","objectID":"/posts/segment-client/:5:0","tags":["Project","Data Science","Machine Learning","K-Means"],"title":"Segmenta√ß√£o de clientes de Food Delivery","uri":"/posts/segment-client/"},{"categories":["Projetos"],"content":"Clusters Visualization Nas pr√≥ximas visualiza√ß√µes teremos basicamente o mesmo c√≥digo de plotagem, onde cada gr√°fico tr√°s a informa√ß√£o de um cliente (id_transacao), e qual o n√∫mero de pedidos (1, 2, 3, 4 ou 5) que o mesmo fez em rela√ß√£o a cada produto (bebida, pizza, salada, sobremesa). Uma linha horizontal foi tra√ßada para representar um cliente dentre os milhares presentes nos plots. ","date":"2020-06-19","objectID":"/posts/segment-client/:6:0","tags":["Project","Data Science","Machine Learning","K-Means"],"title":"Segmenta√ß√£o de clientes de Food Delivery","uri":"/posts/segment-client/"},{"categories":["Projetos"],"content":"Cluster 1 O cluster 1 tr√°s os clientes que possuem um maior consumo de sobremesa e salada, e que consomem pouco, ou n√£o consomem nem bebida nem pizza. Aqueles que querem ser saud√°veis mas n√£o dispensam um docinho. cluster_1 \u003c- clusters[[1]] %\u003e% pivot_longer(cols = c(\"bebida\", \"pizza\", \"sobremesa\", \"salada\"), names_to = \"pedidos\", values_to = \"values\") cluster_1 %\u003e% ggplot(aes(x = values, y = id_transacao, color = pedidos)) + geom_point(position = position_jitter(height = 0, width = .3)) + scale_x_continuous(breaks = 0:4) + annotate(geom = \"rect\", xmin = -0.5, xmax = 4.5, ymax = 16500, ymin = 16400, alpha = .3) + labs(x = \"N√∫mero de pedidos realizado pelo cliente\", title = \"Pedidos feitos por cada cliente\", subtitle = \"A linha horizontal indica a informa√ß√£o de um cliente e seus pedidos\") + theme(axis.text.y.left = element_blank(), axis.title.y = element_blank(), axis.ticks.y = element_blank()) ","date":"2020-06-19","objectID":"/posts/segment-client/:6:1","tags":["Project","Data Science","Machine Learning","K-Means"],"title":"Segmenta√ß√£o de clientes de Food Delivery","uri":"/posts/segment-client/"},{"categories":["Projetos"],"content":"Cluster 2 O cluster 2 √© um p√∫blico que consome bem pouco, e quando consome, consome mais pizza, sobremesa e alguns ainda consomem bebida. Esses s√£o realmente aversos a saladas. cluster_2 \u003c- clusters[[2]] %\u003e% pivot_longer(cols = c(\"bebida\", \"pizza\", \"sobremesa\", \"salada\"), names_to = \"pedidos\", values_to = \"values\") cluster_2 %\u003e% ggplot(aes(x = values, y = id_transacao, color = pedidos)) + geom_point(position = position_jitter(height = 0, width = .3)) + scale_x_continuous(breaks = 0:1) + annotate(geom = \"rect\", xmin = -0.5, xmax = 1.5, ymax = 16500, ymin = 16400, alpha = .3) + labs(x = \"N√∫mero de pedidos realizado pelo cliente\", title = \"Pedidos feitos por cada cliente\", subtitle = \"A linha horizontal indica a informa√ß√£o de um cliente e seus pedidos\") + theme(axis.text.y.left = element_blank(), axis.title.y = element_blank(), axis.ticks.y = element_blank()) ","date":"2020-06-19","objectID":"/posts/segment-client/:6:2","tags":["Project","Data Science","Machine Learning","K-Means"],"title":"Segmenta√ß√£o de clientes de Food Delivery","uri":"/posts/segment-client/"},{"categories":["Projetos"],"content":"Cluster 3 No cluster 3 n√≥s vemos os clientes que pedem em maior quantidade pizza e sobremesa e optam por n√£o pedir ou pedir pouco salada e bebida. cluster_3 \u003c- clusters[[3]] %\u003e% pivot_longer(cols = c(\"bebida\", \"pizza\", \"sobremesa\", \"salada\"), names_to = \"pedidos\", values_to = \"values\") cluster_3 %\u003e% ggplot(aes(x = values, y = id_transacao, color = pedidos)) + geom_point(position = position_jitter(height = 0, width = .3)) + scale_x_continuous(breaks = 0:5) + annotate(geom = \"rect\", xmin = -0.5, xmax = 5.5, ymax = 3500, ymin = 3470, alpha = .3) + labs(x = \"N√∫mero de pedidos realizado pelo cliente\", title = \"Pedidos feitos por cada cliente\", subtitle = \"A linha horizontal indica a informa√ß√£o de um cliente e seus pedidos\") + theme(axis.text.y.left = element_blank(), axis.title.y = element_blank(), axis.ticks.y = element_blank()) ","date":"2020-06-19","objectID":"/posts/segment-client/:6:3","tags":["Project","Data Science","Machine Learning","K-Means"],"title":"Segmenta√ß√£o de clientes de Food Delivery","uri":"/posts/segment-client/"},{"categories":["Projetos"],"content":"Cluster 4 O cluster 4 tr√°s as pessoas que gostam de sobremesa mas que pedem um pouco de pizza e salada. Al√©m disso n√£o costuma pedir bebidas. cluster_4 \u003c- clusters[[4]] %\u003e% pivot_longer(cols = c(\"bebida\", \"pizza\", \"sobremesa\", \"salada\"), names_to = \"pedidos\", values_to = \"values\") cluster_4 %\u003e% ggplot(aes(x = values, y = id_transacao, color = pedidos)) + geom_point(position = position_jitter(height = 0, width = .3)) + scale_x_continuous(breaks = 0:2) + annotate(geom = \"rect\", xmin = -0.5, xmax = 2.5, ymax = 11500, ymin = 11430, alpha = .3) + labs(x = \"N√∫mero de pedidos realizado pelo cliente\", title = \"Pedidos feitos por cada cliente\", subtitle = \"A linha horizontal indica a informa√ß√£o de um cliente e seus pedidos\") + theme(axis.text.y.left = element_blank(), axis.title.y = element_blank(), axis.ticks.y = element_blank()) ","date":"2020-06-19","objectID":"/posts/segment-client/:6:4","tags":["Project","Data Science","Machine Learning","K-Means"],"title":"Segmenta√ß√£o de clientes de Food Delivery","uri":"/posts/segment-client/"},{"categories":["Projetos"],"content":"Cluster 5 Aqui vemos mais um cluster onde os clientes gostam de pedir muita salda com sobremesa, mas n√£o dispensam uma pizza em algumas situa√ß√µes. cluster_5 \u003c- clusters[[5]] %\u003e% pivot_longer(cols = c(\"bebida\", \"pizza\", \"sobremesa\", \"salada\"), names_to = \"pedidos\", values_to = \"values\") cluster_5 %\u003e% ggplot(aes(x = values, y = id_transacao, color = pedidos)) + geom_point(position = position_jitter(height = 0, width = .3)) + scale_x_continuous(breaks = 0:5) + annotate(geom = \"rect\", xmin = -0.5, xmax = 5.5, ymax = 2800, ymin = 2770, alpha = .3) + labs(x = \"N√∫mero de pedidos realizado pelo cliente\", title = \"Pedidos feitos por cada cliente\", subtitle = \"A linha horizontal indica a informa√ß√£o de um cliente e seus pedidos\") + theme(axis.text.y.left = element_blank(), axis.title.y = element_blank(), axis.ticks.y = element_blank()) ","date":"2020-06-19","objectID":"/posts/segment-client/:6:5","tags":["Project","Data Science","Machine Learning","K-Means"],"title":"Segmenta√ß√£o de clientes de Food Delivery","uri":"/posts/segment-client/"},{"categories":["Projetos"],"content":"Cluster 6 Os clientes do grupo 6 s√£o tamb√©m bem aversos a salada, optando por uma combina√ß√£o perfeita, sobremesa e pizza. cluster_6 \u003c- clusters[[6]] %\u003e% pivot_longer(cols = c(\"bebida\", \"pizza\", \"sobremesa\", \"salada\"), names_to = \"pedidos\", values_to = \"values\") cluster_6 %\u003e% ggplot(aes(x = values, y = id_transacao, color = pedidos)) + geom_point(position = position_jitter(height = 0, width = .3)) + scale_x_continuous(breaks = 0:3) + annotate(geom = \"rect\", xmin = -0.5, xmax = 3.5, ymax = 15500, ymin = 15400, alpha = .3) + labs(x = \"N√∫mero de pedidos realizado pelo cliente\", title = \"Pedidos feitos por cada cliente\", subtitle = \"A linha horizontal indica a informa√ß√£o de um cliente e seus pedidos\") + theme(axis.text.y.left = element_blank(), axis.title.y = element_blank(), axis.ticks.y = element_blank()) ","date":"2020-06-19","objectID":"/posts/segment-client/:6:6","tags":["Project","Data Science","Machine Learning","K-Means"],"title":"Segmenta√ß√£o de clientes de Food Delivery","uri":"/posts/segment-client/"},{"categories":["Projetos"],"content":"Cluster 7 No cluster 7 temos aqueles clientes que juntamente a pizza com sobremesa, adicionam uma bebida. cluster_7 \u003c- clusters[[7]] %\u003e% pivot_longer(cols = c(\"bebida\", \"pizza\", \"sobremesa\", \"salada\"), names_to = \"pedidos\", values_to = \"values\") cluster_7 %\u003e% ggplot(aes(x = values, y = id_transacao, color = pedidos)) + geom_point(position = position_jitter(height = 0, width = .3)) + scale_x_continuous(breaks = 0:5) + annotate(geom = \"rect\", xmin = -0.5, xmax = 5.5, ymax = 17500, ymin = 17400, alpha = .3) + labs(x = \"N√∫mero de pedidos realizado pelo cliente\", title = \"Pedidos feitos por cada cliente\", subtitle = \"A linha horizontal indica a informa√ß√£o de um cliente e seus pedidos\") + theme(axis.text.y.left = element_blank(), axis.title.y = element_blank(), axis.ticks.y = element_blank()) ","date":"2020-06-19","objectID":"/posts/segment-client/:6:7","tags":["Project","Data Science","Machine Learning","K-Means"],"title":"Segmenta√ß√£o de clientes de Food Delivery","uri":"/posts/segment-client/"},{"categories":["Projetos"],"content":"Cluster 8 E por fim, o cluster os loucos por sobremesa. cluster_8 \u003c- clusters[[8]] %\u003e% pivot_longer(cols = c(\"bebida\", \"pizza\", \"sobremesa\", \"salada\"), names_to = \"pedidos\", values_to = \"values\") cluster_8 %\u003e% ggplot(aes(x = values, y = id_transacao, color = pedidos)) + geom_point(position = position_jitter(height = 0, width = .3)) + scale_x_continuous(breaks = 0:5) + annotate(geom = \"rect\", xmin = -0.5, xmax = 5.5, ymax = 9500, ymin = 9420, alpha = .3) + labs(x = \"N√∫mero de pedidos realizado pelo cliente\", title = \"Pedidos feitos por cada cliente\", subtitle = \"A linha horizontal indica a informa√ß√£o de um cliente e seus pedidos\") + theme(axis.text.y.left = element_blank(), axis.title.y = element_blank(), axis.ticks.y = element_blank()) ","date":"2020-06-19","objectID":"/posts/segment-client/:6:8","tags":["Project","Data Science","Machine Learning","K-Means"],"title":"Segmenta√ß√£o de clientes de Food Delivery","uri":"/posts/segment-client/"},{"categories":["Projetos"],"content":"Is there a cat in your dat? Esse √© um playground do Kaggle, onde diferentes pessoas competem dentro de um determinado problema ‚Äòincomum‚Äô por√©m bastante relevante. Nesta competi√ß√£o foi disponibilizado um dataset repleto de vari√°veis categ√≥ricas. Sabemos que uma das tarefas do processo de machine learning √© realizar o encoding de vari√°veis categ√≥ricas de forma que os algoritmos possam executar seus c√°lculos e retornar um resultado. Dominar tal skill √© de grande import√¢ncia para o Cientista de Dados. Entre os tipos de vari√°veis categ√≥ricas, temos: Bin√°rias: vari√°veis com apenas 2 valores (S/N, F/V, 0/1‚Ä¶). Nominais: vari√°veis que s√£o apenas palavras. Ordinais: vari√°veis que apresentam certa ordem no seu dataset. C√≠clicas: vari√°veis que possuem algum tipo de periodiza√ß√£o (dia da semana, horas e minutos do dia, velocidade do vento, per√≠odo de mar√©s‚Ä¶). Esta foi o primeiro Categorical Feature Encoding Challenge, j√° est√° dispon√≠vel o segundo, com maior n√≠vel de dificuldade, mas vamos por partes. # Import import pandas as pd import numpy as np from scipy import sparse, stats from sklearn.preprocessing import LabelEncoder, OneHotEncoder from sklearn.model_selection import train_test_split, StratifiedKFold, GridSearchCV from sklearn.metrics import accuracy_score, roc_auc_score from sklearn.linear_model import LogisticRegressionCV from xgboost import XGBClassifier import matplotlib.gridspec as gridspec import matplotlib.pyplot as plt import seaborn as sns # Carregando os dados df_train = pd.read_csv('data/train.csv') df_test = pd.read_csv('data/test.csv') submission = pd.read_csv('sample_submission.csv') train = df_train.copy() test = df_test.copy() # removendo target dos dados labels = train.pop('target') y = labels.values # removendo e salvando o id dos datasets # ser√° √∫ltil mais tarde train_id = train.pop('id') test_id = test.pop('id') Antes de come√ßarmos a fazer qualquer tipo de tratamento no dataset √© importante que analisemos o comportamento das vari√°veis. E para isso entra em a√ß√£o a An√°lise Explorat√≥ria dos Ddaos. Vamos a ela! EDA (Exploratory Data Analysis) Primeiramente √© interessante ter uma fun√ß√£o que nos permita ter uma vis√£o geral dos dados, para dados num√©ricos poder√≠amos utilizar o describe, por√©m aqui n√£o seria nada √∫til. Portanto a seguinte fun√ß√£o foi desenvolvida. def resumetable(df): print(f\"Dataset Shape: {df.shape}\") summary = pd.DataFrame(df.dtypes,columns=['dtypes']) summary = summary.reset_index() summary['Name'] = summary['index'] summary = summary[['Name','dtypes']] summary['Missing'] = df.isnull().sum().values summary['Uniques'] = df.nunique().values # C√°lcula da Entropia dos dados for name in summary['Name'].value_counts().index: summary.loc[summary['Name'] == name, 'Entropy'] = round(stats.entropy(df[name].value_counts(normalize=True), base=2),2) return summary summary = resumetable(train) summary Dataset Shape: (300000, 23) .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Com esse simples resumo, j√° temos algumas informa√ß√µes bem √∫teis. Sabemos que algumas vari√°veis possuem um alto n√≠vel de cardinalidade, ou seja, apresentam um conjunto muito grande de valores √∫nicos, e que os casos mais cr√≠ticos est√£o nas vari√°veis nominais. A ord_5 (ordinal) tamb√©m exigir√° um tratamento mais cuidadoso. Veja tamb√©m que foi calculado a entropia, que corrobora com a informa√ß√£o de valores √∫nicos, possuindo um valor mais alto para vari√°veis de maior n√≠vel de cardinalidade. De forma simples, a Entropia (aqui baseada na Teoria da Informa√ß√£o), mede o grau de heterogeneidade dos dados. Por√©m, como estamos lidando com vari√°veis categ√≥ricas e ainda por cima com alto n√∫mero de valores √∫nicos, √© poss√≠vel que exista alguns desse valores que esteja no dataset de treino e n√£o esteja no dataset de teste e virse-versa. O impacto provocado por essa caracter√≠stica √© que nosso modelo ser√° treinado sobre um determinado conjunto de classes e avaliado com classes que ele nunca viu, o que pode ser bem cr√≠tico","date":"2019-11-04","objectID":"/posts/cat_in_the_dat/:0:0","tags":["Project","Challenge","Data Science","Machine Learning"],"title":"Categorical Feature Encoding Challenge","uri":"/posts/cat_in_the_dat/"},{"categories":["Projetos"],"content":"Visualiza√ß√µes: Target # Criando uma paleta de cores flatui = [\"#e74c3c\", \"#3498db\", \"#95a5a6\", \"#9b59b6\", \"#f1c40fff\", \"#34495e\", \"#2ecc71\"] flatui = sns.color_palette(flatui) total = len(df_train) plt.figure(figsize=(12,6)) sns.set(style=\"darkgrid\") g = sns.countplot(x='target', data=df_train, palette=flatui) g.set_title(\"Contagem das classes\", fontsize = 20) g.set_xlabel(\"Target Values\", fontsize = 15) g.set_ylabel(\"Count\", fontsize = 15) sizes=[] # c√≥digo para porcentagens em cada barra for p in g.patches: height = p.get_height() sizes.append(height) g.text(p.get_x()+p.get_width()/2., height + 3, '{:1.2f}%'.format(height/total*100), ha=\"center\", fontsize=14) g.set_ylim(0, max(sizes) * 1.15) plt.show() O que observamos √© que os dados n√£o s√£o t√£o desbalanceados: 69,41% pertence a classe 0 30,59% pertence a classe 1 ","date":"2019-11-04","objectID":"/posts/cat_in_the_dat/:1:0","tags":["Project","Challenge","Data Science","Machine Learning"],"title":"Categorical Feature Encoding Challenge","uri":"/posts/cat_in_the_dat/"},{"categories":["Projetos"],"content":"Visualiza√ß√µes: Bin√°rias # vamos fazer um grid # temos 5 vari√°veis bin√°rias, logo uma iremos # plotar em 2 linhas e 3 colunas. grid = gridspec.GridSpec(2, 3) plt.figure(figsize=(16,20)) # queremos um countplot, em cada vari√°vel bin√°ria # por isso o loop for bin_cols = ['bin_0', 'bin_1', 'bin_2', 'bin_3', 'bin_4'] for n, col in enumerate(df_train[bin_cols]): ax = plt.subplot(grid[n]) sns.countplot(x=col, data=df_train, hue='target', palette=flatui) ax.set_ylabel('Count', fontsize=15) ax.set_title(f'{col} Por classe', fontsize=18) ax.set_xlabel(f'{col} values', fontsize=15) sizes=[] # c√≥digo para porcentagens em cada barra for p in ax.patches: height = p.get_height() sizes.append(height) ax.text(p.get_x()+p.get_width()/2., height + 3, '{:1.2f}%'.format(height/total*100), ha=\"center\", fontsize=12) ax.set_ylim(0, max(sizes) * 1.15) plt.show() Vemos que as vari√°veis num√©ricas variam entre 3 tipos de classes: T/F Y/N 0/1 Novamente fica bem claro que os dados s√£o relativamente bem balanceados, apenas vari√°vel bin_1 apresenta uma maior diferen√ßa entre as classes da vari√°vel target. ","date":"2019-11-04","objectID":"/posts/cat_in_the_dat/:2:0","tags":["Project","Challenge","Data Science","Machine Learning"],"title":"Categorical Feature Encoding Challenge","uri":"/posts/cat_in_the_dat/"},{"categories":["Projetos"],"content":"Visualiza√ß√µes: Nominais # nom_cols = ['nom_0', 'nom_1', 'nom_2', 'nom_3', 'nom_4', 'nom_5', 'nom_6', 'nom_7', 'nom_8', 'nom_9'] nom_cols = ['nom_0', 'nom_1', 'nom_2', 'nom_3', 'nom_4'] def ploting_cat_fet(df, cols, vis_row=5, vis_col=2): sns.set(style=\"white\") grid = gridspec.GridSpec(vis_row,vis_col) # The grid of chart plt.figure(figsize=(17, 35)) # size of figure # loop to get column and the count of plots for n, col in enumerate(df_train[cols]): tmp = pd.crosstab(df_train[col], df_train['target'], normalize='index') * 100 tmp = tmp.reset_index() tmp.rename(columns={0:'No',1:'Yes'}, inplace=True) ax = plt.subplot(grid[n]) # feeding the figure of grid sns.countplot(x=col, data=df_train, order=list(tmp[col].values), palette='Spectral') ax.set_ylabel('Count', fontsize=15) # y axis label ax.set_title(f'{col} Por Classe', fontsize=18) # title label ax.set_xlabel(f'{col} values', fontsize=15) # x axis label # twinX - to build a second yaxis gt = ax.twinx() gt = sns.pointplot(x=col, y='Yes', data=tmp, order=list(tmp[col].values), color='black', legend=False) gt.set_ylim(tmp['Yes'].min()-5,tmp['Yes'].max()*1.1) gt.set_ylabel(\"Target %True(1)\", fontsize=16) sizes=[] # Get highest values in y for p in ax.patches: # loop to all objects height = p.get_height() sizes.append(height) ax.text(p.get_x()+p.get_width()/2., height + 3, '{:1.2f}%'.format(height/total*100), ha=\"center\", fontsize=14) ax.set_ylim(0, max(sizes) * 1.15) # set y limit based on highest heights plt.subplots_adjust(hspace = 0.5, wspace=.3) plt.show() ploting_cat_fet(df_train, nom_cols, vis_row=5, vis_col=2) Esse gr√°fico mescla 2 tipos diferentes de plots, o point plot com um gr√°fico de count plot, a inten√ß√£o √© mostrar o comportamento de algumas das vari√°veis nominais e a ocorr√™ncia de valores True (1) na aquela vari√°vel. √â observado que a ocorr√™ncia de valores True √© maior em classes de vari√°veis nominais que possuem menor ocorr√™ncia no grupo. Para vari√°veis nominais de alta cardinalidade (nom_5 a nom_9) se torna invi√°vel de realizar o mesmo tipo de plot ","date":"2019-11-04","objectID":"/posts/cat_in_the_dat/:3:0","tags":["Project","Challenge","Data Science","Machine Learning"],"title":"Categorical Feature Encoding Challenge","uri":"/posts/cat_in_the_dat/"},{"categories":["Projetos"],"content":"Visualiza√ß√£o: Ordinais ord_cols = ['ord_0', 'ord_1', 'ord_2', 'ord_3', 'ord_4'] ploting_cat_fet(df_train, ord_cols, vis_row=5, vis_col=2) Muito interessante, como estamos tratando de vari√°veis ordinais, significa que cada ordem ir√° possuir um n√≠vel de import√¢ncia. O que fica n√≠tido ao visualizar ord_4 e ord_5, que est√£o ordenados de A a Z. O mesmo padr√£o tamb√©m √© visto em ord_0. Mas por que n√£o vemos o mesmo comportamento em ord_1 e ord_2? O motivo √© que as vari√°veis n√£o est√£o ordenadas pelo que de fato elas representam, e sim por ordem alfab√©tica. Vamos visualizar ent√£o quando organizamos as vari√°veis ord_1 e ord_2 por uma ordem esp√©c√≠fica e para isso definimos algumas fun√ß√µes. # Vamos definir algumas fun√ß√µes para convers√£o def ord_to_fac(df, name, categories): ''' A fun√ß√£o converte para uma ordem espec√≠fica de fatores. ''' raw_cat = pd.Categorical(df[name], categories=categories, ordered=True) labels, unique = pd.factorize(raw_cat, sort=True) new_name = name + '_v2' df[new_name] = labels def ord_to_num(name, df, categories): ''' A fun√ß√£o converte para ordem num√©rica ou alfab√©tica. ''' raw_cat = pd.Categorical(df[name], categories=categories, ordered=True) encoder = LabelEncoder() encoded = encoder.fit_transform(raw_cat) new_name = name + '_v2' df[new_name] = encoded # Verificando as classes nas vari√°veis ordinais unique_ord0 = df_train['ord_0'].unique() unique_ord1 = df_train['ord_1'].unique() unique_ord2 = df_train['ord_2'].unique() unique_ord3 = df_train['ord_3'].unique() unique_ord4 = df_train['ord_4'].unique() unique_ord5 = df_train['ord_5'].unique() name = 'ord_1' categories = ['Novice', 'Contributor', 'Expert', 'Master', 'Grandmaster'] ord_to_fac(df_train, name, categories) df_train.columns Index(['id', 'bin_0', 'bin_1', 'bin_2', 'bin_3', 'bin_4', 'nom_0', 'nom_1', 'nom_2', 'nom_3', 'nom_4', 'nom_5', 'nom_6', 'nom_7', 'nom_8', 'nom_9', 'ord_0', 'ord_1', 'ord_2', 'ord_3', 'ord_4', 'ord_5', 'day', 'month', 'target', 'ord_1_v2'], dtype='object') name = 'ord_2' categories = ['Freezing', 'Cold', 'Warm', 'Hot', 'Boiling Hot', 'Lava Hot'] ord_to_fac(df_train, name, categories) ord_cols = ['ord_1_v2', 'ord_2_v2'] ploting_cat_fet(df_train, ord_cols, vis_row=2, vis_col=2) Muito bom, agora o padr√£o se torna n√≠tido. Por √∫ltimo vamos observar a vari√°vel ord_5. name = 'ord_5' categories = ['AP', 'Ai', 'Aj', 'BA', 'BE', 'Bb', 'Bd', 'Bn', 'CL', 'CM', 'CU', 'CZ', 'Cl', 'DH', 'DN', 'Dc', 'Dx', 'Ed', 'Eg', 'Er', 'FI', 'Fd', 'Fo', 'GD', 'GJ', 'Gb', 'Gx', 'Hj', 'IK', 'Id', 'JX', 'Jc', 'Jf', 'Jt', 'KR', 'KZ', 'Kf', 'Kq', 'LE', 'MC', 'MO', 'MV', 'Mf', 'Ml', 'Mx', 'NV', 'Nf', 'Nk', 'OR', 'Ob', 'Os', 'PA', 'PQ', 'PZ', 'Ps', 'QM', 'Qb', 'Qh', 'Qo', 'RG', 'RL', 'RP', 'Rm', 'Ry', 'SB', 'Sc', 'TR', 'TZ', 'To', 'UO', 'Uk', 'Uu', 'Vf', 'Vx', 'WE', 'Wc', 'Wv', 'XI', 'Xh', 'Xi', 'YC', 'Yb', 'Ye', 'ZR', 'ZS', 'Zc', 'Zq', 'aF', 'aM', 'aO', 'aP', 'ac', 'av', 'bF', 'bJ', 'be', 'cA', 'cG', 'cW', 'ck', 'cp', 'dB', 'dE', 'dN', 'dO', 'dP', 'dQ', 'dZ', 'dh', 'eG', 'eQ', 'eb', 'eg', 'ek', 'ex', 'fO', 'fh', 'gJ', 'gM', 'hL', 'hT', 'hh', 'hp', 'iT', 'ih', 'jS', 'jV', 'je', 'jp', 'kC', 'kE', 'kK', 'kL', 'kU', 'kW', 'ke', 'kr', 'kw', 'lF', 'lL', 'll', 'lx', 'mb', 'mc', 'mm', 'nX', 'nh', 'oC', 'oG', 'oH', 'oK', 'od', 'on', 'pa', 'ps', 'qA', 'qJ', 'qK', 'qP', 'qX', 'qo', 'qv', 'qw', 'rZ', 'ri', 'rp', 'sD', 'sV', 'sY', 'sn', 'su', 'tM', 'tP', 'tv', 'uJ', 'uS', 'ud', 'us', 'ut', 'ux', 'uy', 'vK', 'vq', 'vy', 'wu', 'wy', 'xP', 'xy', 'yN', 'yY', 'yc', 'zU'] ord_to_num(name, df_train, categories) ord_cols = ['ord_5_v2'] ploting_cat_fet(df_train, ord_cols, vis_row=1, vis_col=1) O que observamos √© que as classes da vari√°vel ord_5 n√£o possuem o comportamento t√£o linear quanto as outras vari√°veis ordinais. Nos levando a duas hip√≥teses: Ou a vari√°vel √© resultado de combina√ß√£o de vari√°veis, que acabou proporcionando esse ru√≠do observado. Ou ela realmente possui (intrinsecamente) esse ru√≠do, e n√£o ter√≠amos muito oq fazer. Podemos ent√£o testar a primeira suposi√ß√£o, e realizar a s","date":"2019-11-04","objectID":"/posts/cat_in_the_dat/:4:0","tags":["Project","Challenge","Data Science","Machine Learning"],"title":"Categorical Feature Encoding Challenge","uri":"/posts/cat_in_the_dat/"},{"categories":["Projetos"],"content":"Temos dia da semana e meses do ano Para os meses do ano, o mais indicado √© que se divida os dados em trimestres. Aos quais as novas vari√°veis ser√£o: Q1, Q2, Q3, Q4. df['quarters'] = pd.to_datetime(df['month'].values, format='%m').astype('period[Q]') df_month = pd.get_dummies(df['quarters'], prefix='v2', drop_first=False) df = pd.concat([df, df_month], axis=1) Drop de algumas colunas df.columns Index(['bin_0', 'bin_1', 'bin_2', 'bin_3', 'bin_4', 'nom_0', 'nom_1', 'nom_2', 'nom_3', 'nom_4', 'nom_5', 'nom_6', 'nom_7', 'nom_8', 'nom_9', 'ord_0', 'ord_1', 'ord_2', 'ord_3', 'ord_4', 'ord_5', 'day', 'month', 'ord_1_v2', 'ord_2_v2', 'ord_3_v2', 'ord_4_v2', 'ord_5_v3', 'ord_5_v3_v2', 'bin_3_v2', 'bin_4_v2', 'quarters', 'v2_1900Q1', 'v2_1900Q2', 'v2_1900Q3', 'v2_1900Q4'], dtype='object') # para n√£o perder o progresso no dataset original por algum comando new_df = df.copy() new_df.drop(['bin_3', 'bin_4', 'ord_1', 'ord_2', 'ord_3', 'ord_4', 'ord_5', 'month', 'quarters', 'ord_5_v3'], axis=1, inplace=True) new_df.columns Index(['bin_0', 'bin_1', 'bin_2', 'nom_0', 'nom_1', 'nom_2', 'nom_3', 'nom_4', 'nom_5', 'nom_6', 'nom_7', 'nom_8', 'nom_9', 'ord_0', 'day', 'ord_1_v2', 'ord_2_v2', 'ord_3_v2', 'ord_4_v2', 'ord_5_v3_v2', 'bin_3_v2', 'bin_4_v2', 'v2_1900Q1', 'v2_1900Q2', 'v2_1900Q3', 'v2_1900Q4'], dtype='object') Iniciando a cria√ß√£o da matriz esparsa Como iremos aplicar o one hot encoder em toda a matriz, a quantidade de mem√≥ria utilizada √© extremamente grande. Para minimizar isso podemos realizar a cria√ß√£o de uma matriz esparsa, que justamente √© especializada no armazenamento de uma matriz que possui grande quantidades de zeros com valores esparssados ao longo de sua dimens√£o. Para isso, o Pandas possui o get_dummies com o par√¢metro sparse, que precisa receber o valor True. columns = [i for i in new_df.columns] print(columns) ['bin_0', 'bin_1', 'bin_2', 'nom_0', 'nom_1', 'nom_2', 'nom_3', 'nom_4', 'nom_5', 'nom_6', 'nom_7', 'nom_8', 'nom_9', 'ord_0', 'day', 'ord_1_v2', 'ord_2_v2', 'ord_3_v2', 'ord_4_v2', 'ord_5_v3_v2', 'bin_3_v2', 'bin_4_v2', 'v2_1900Q1', 'v2_1900Q2', 'v2_1900Q3', 'v2_1900Q4'] dummies = pd.get_dummies(new_df, columns=columns, drop_first=True, sparse=True) dummies.shape (500000, 16382) train = dummies.iloc[:train.shape[0], :] test = dummies.iloc[train.shape[0]:, :] # preenchemos os NAs com 0, pois fica complicado verificar se existe valor nulo # devido a dimens train = train.fillna(0) train.head(5) .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } train = train.sparse.to_coo().tocsr() test = test.sparse.to_coo().tocsr() Modelagem Preditiva seed = 7 cv_size = 0.30 X_train, X_cv, y_train, y_cv = train_test_split(train, y, test_size=cv_size, random_state=seed) model = XGBClassifier() model.fit(X_train, y_train) prediction = model.predict_proba(X_cv)[:, 1] roc_score = roc_auc_score(y_cv, prediction) print(roc_score) 0.7631479991590155 lr_cv = LogisticRegressionCV(Cs=7, solver=\"lbfgs\", tol=0.0002, max_iter=10000, cv=5, n_jobs=6) lr_cv.fit(train, y) lr_cv_pred = lr_cv.predict_proba(train)[:, 1] score = roc_auc_score(y, lr_cv_pred) print(\"score: \", score) score: 0.8177916253269919 O melhor score encontrado foi do m√©todo utilizando a Regress√£o Log√≠stica, que teve um comportamento fant√°stico sendo que processou dados de 16 mil colunas com um score de 81,78% na m√©trica AUC utilizada para avalia√ß√£o do modelo. Gera√ß√£o do arquivo de Submiss√£o ao Kaggle submission[\"id\"] = test_id submission[\"target\"] = lr_cv.predict_proba(test)[:, 1] submission.head() .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } submission.to_csv(\"submission_verX.csv\", index=False) Conclus√µes Logicamente que esses dados possuem uma caracteristica bastante acentuada, que √© de ser composto apenas de dados categ√≥ricos e ainda com algumas colunas possu√≠ndo mais de 11 mil valores √∫nicos. Na realidade √© possivel que n√£o encontremos uma base de dados t√£o peculiar no dia a dia, mas as skills des","date":"2019-11-04","objectID":"/posts/cat_in_the_dat/:4:1","tags":["Project","Challenge","Data Science","Machine Learning"],"title":"Categorical Feature Encoding Challenge","uri":"/posts/cat_in_the_dat/"},{"categories":["Projetos"],"content":"Fontes: Algumas fun√ß√µes s√£o de dois kernels incr√≠veis do kaggle. https://www.kaggle.com/kabure/eda-feat-engineering-encode-conquer https://www.kaggle.com/adaubas/2nd-place-solution-categorical-fe-callenge ","date":"2019-11-04","objectID":"/posts/cat_in_the_dat/:5:0","tags":["Project","Challenge","Data Science","Machine Learning"],"title":"Categorical Feature Encoding Challenge","uri":"/posts/cat_in_the_dat/"},{"categories":["Projetos"],"content":"PrevisaÃÉo de Uso de Energia Quando falamos de revolu√ß√£o tecnol√≥gica, a no√ß√£o de Internet das Coisas, ou Internet of Things (IoT), √© um dos assuntos principais. O seu desenvolvimento est√° mudando a forma como o ser humando tem se conectado com diversos dispositivos tecnl√≥gicos. IoT vem gerando uma quantidade imensa de dados, um dos maiores respons√°veis pelo Big Data. Diante da gera√ß√£o de tanta informa√ß√£o, o papel do Cientista de Dados se faz cada vez mais presente, pois torna-se poss√≠vel a obte√ß√£o de grandes insights sobre diferentes √°rea at√© ent√£o n√£o analisadas. Este projeto de IoT tem como objetivo a cria√ß√£o de modelos preditivos para a previs√£o de consumo de energia de eletrodom√©sticos. Os dados utilizados incluem medi√ß√µes de sensores de temperatura e umidade de uma rede sem fio, previs√£o do tempo de uma esta√ß√£o de um aeroporto e uso de energia utilizada por lumin√°rias. A vers√£o completa do projeto pode ser encontrada no reposit√≥rio do meu GitHub, assim como os dados que foram utilizados. An√°lise Explorat√≥ria dos Dados ","date":"2019-04-04","objectID":"/posts/predictive_iot_energy/:0:0","tags":["Project","Data Science","Machine Learning"],"title":"Modelagem Preditiva em IoT","uri":"/posts/predictive_iot_energy/"},{"categories":["Projetos"],"content":"O que representa cada vari√°vel no dataset? date: Tempo de coleta dos dados pelos sensores Appliances: Uso de energia (em W) lights: Pot√™ncia de energia de eletrodom√©sticos na casa (em W) TX: Temperatura em um lugar da casa (em Celsius) RH_X: Umidade relativa em algum ponto da casa (em %) Windspeed: Velocidade do vento (em m/s) Visibility: Visibilidade (em Km) Tdewpoint: N√£o foi informado. rv1: Vari√°vel rand√¥mica adicional rv2: Vari√°vel rand√¥mica adicional WeekStatus: Indica se √© dia de semana ou final de semana Day_of_week: Dia da semana NSM: Medida de tempo (em s) Ao importar os dados foi visto que os dados possuem a mesma quantidade de colunas e que elas s√£o iguais, sendo assim, os dados de teste e treino foram concatenados para realizar uma an√°lise conjunta dos dados. df = pd.concat([train, test], axis=0) print(f'O novo dataset possui {df.shape[0]} linhas e {df.shape[1]} vari√°veis') print(f'O dataset possui {sum(df.isnull().sum())} valores nulos') O novo dataset possui 19735 linhas e 32 vari√°veis O dataset possui 0 valores nulos Optei por separar vari√°veis num√©ricas e categ√≥ricas para melhor observar algumas informa√ß√µes que s√£o caracter√≠sticas de cada tipo. num_vars = [name for name in df.columns if df[name].dtype != 'object'] cat_vars = [name for name in df.columns if df[name].dtype == 'object'] df_num = df[num_vars] df_cat = df[cat_vars] print(f'O data set possui {len(num_vars)} vari√°veis num√©ricas e {len(cat_vars)} de tipo objeto, sendo todas relacionadas √† data') O data set possui 29 vari√°veis num√©ricas e 3 de tipo objeto, sendo todas relacionadas √† data. Uma observa√ß√£o interessante √© que precisamos estar sempre atentos ao tipo de dado que est√° sendo manipulado, no atual conjunto de dados a vari√°vel ‚Äòdate‚Äô n√£o foi importada como tipo de data. Logo, foi preciso realizar essa convers√£o, al√©m disso, a data (agora convertida) foi adicionada ao index do DataFrame. A adi√ß√£o da vari√°vel de data ao index do objeto DataFrame do pacote Pandas permite uma manipula√ß√£o mais intuitiva desse tipo de dado. Mais √† frente tal funcionalidade ser√° observada. # converter a coluna 'date' para tipo data df_cat['date'] = pd.to_datetime(df_cat['date']) df_cat.dtypes date datetime64[ns] WeekStatus object Day_of_week object dtype: object # Transformando a coluna de data em index podemos realizar algumas an√°lises # de forma mais simples. df_cat.index = pd.DatetimeIndex(df_cat['date']) df_num.index = pd.DatetimeIndex(df_cat['date']) # Tamb√©m precisaremos de uma coluna com as datas (utilizada para alguns plots). df_num['date'] = df_cat['date'].copy() df_num.columns Index(['Appliances', 'lights', 'T1', 'RH_1', 'T2', 'RH_2', 'T3', 'RH_3', 'T4', 'RH_4', 'T5', 'RH_5', 'T6', 'RH_6', 'T7', 'RH_7', 'T8', 'RH_8', 'T9', 'RH_9', 'T_out', 'Press_mm_hg', 'RH_out', 'Windspeed', 'Visibility', 'Tdewpoint', 'rv1', 'rv2', 'NSM', 'date'], dtype='object') df_cat.columns Index(['date', 'WeekStatus', 'Day_of_week'], dtype='object') ","date":"2019-04-04","objectID":"/posts/predictive_iot_energy/:1:0","tags":["Project","Data Science","Machine Learning"],"title":"Modelagem Preditiva em IoT","uri":"/posts/predictive_iot_energy/"},{"categories":["Projetos"],"content":"Agora vamos observar um conjunto de plots que ajudaram a entender melhor os dados. ","date":"2019-04-04","objectID":"/posts/predictive_iot_energy/:2:0","tags":["Project","Data Science","Machine Learning"],"title":"Modelagem Preditiva em IoT","uri":"/posts/predictive_iot_energy/"},{"categories":["Projetos"],"content":"Pairplot O c√≥digo abaixo permite a visualiza√ß√£o de um pairplot filtrado por per√≠odos. √à poss√≠vel visualizar os dados em diferentes intervalos de dias, semanas, meses e ano (caso nosso dataset tivesse dados de um ano diferente √† 2016). # Precisa editar algumas linhas caso precise plotar um dia espec√≠fico # ou intervalo espec√≠fico. year_start = '2016' month_start = '05' day = int('01') # fim year_end = '2016' month_end = '05' if day \u003c 10: # date_start = year_start + '-' + month_start + '-' + '0' + str(day_range) + ' 00:00:00' date_start = year_start + '-' + month_start date_end = year_end + '-' + month_end + '-' + '0' + str(day) + ' 23:00:00' else: # date_start = year_start + '-' + month_start + '-' + str(day_range) + ' 00:00:00' date_start = year_start + '-' + month_start date_end = year_end + '-' + month_end + '-' + str(day) + ' 23:00:00' print(f'{date_start} at√© {date_end} com {len(df_num[date_start])} pontos') sns.set(style=\"ticks\", color_codes=True) df_filter = df_num[date_start] # date_start:date_end g = sns.pairplot(df_filter) plt.show() 2016-05 at√© 2016-05-01 23:00:00 com 3853 pontos Podemos visualizar que a maioria das distribui√ß√£o se assemelham √† uma distribui√ß√£o normal, sendo que algumas poucas possuem desvios. Vamos verificar agora se existe a presen√ßa de muitos outliers nos dados ","date":"2019-04-04","objectID":"/posts/predictive_iot_energy/:3:0","tags":["Project","Data Science","Machine Learning"],"title":"Modelagem Preditiva em IoT","uri":"/posts/predictive_iot_energy/"},{"categories":["Projetos"],"content":"Boxplot # Novamente, cada plot tem um c√≥digo que permite o filtro por data. year_start = '2016' month_start = '04' day = int('01') # fim year_end = '2016' month_end = '05' if day \u003c 10: date_start = year_start + '-' + month_start date_end = year_end + '-' + month_end + '-' + '0' + str(day) + ' 23:00:00' else: date_start = year_start + '-' + month_start date_end = year_end + '-' + month_end + '-' + str(day) + ' 23:00:00' print(f'{date_start} at√© {date_end} com {len(df_num[date_start])} pontos') plt.subplots(figsize=(30, 20)) sns.set(style=\"ticks\", color_codes=True) df_filter = df_num[date_start] df_filter = df_filter.drop(['date', 'NSM'], axis=1) df_filter.boxplot() plt.show() 2016-04 at√© 2016-05-01 23:00:00 com 4320 pontos Os dados que mais possuem outliers √© a vari√°vel target. Nesse caso √© essencial que esse pontos sejam tratados, o que ser√° feito mais a frente. ","date":"2019-04-04","objectID":"/posts/predictive_iot_energy/:4:0","tags":["Project","Data Science","Machine Learning"],"title":"Modelagem Preditiva em IoT","uri":"/posts/predictive_iot_energy/"},{"categories":["Projetos"],"content":"Plot Comparativo # Gr√°ficos comparativo de qualquer vari√°vel ao longo de um per√≠odo # Determine o per√≠odo de in√≠cio e fim que deseja visualizar # E determine a vari√°vel name com a vari√°vel a ser observada # in√≠cio year_start = '2016' month_start = '04' day = '01' # fim year_end = '2016' month_end = '04' # plot automatizado # n = n√∫mero de subplots (dias para visualizar) n = 10 name = 'Appliances' fig, ax = plt.subplots(n, figsize=(10, 70)) for i in range(0, n): day_range = int(day) + i if day_range \u003c 10: date_start = year_start + '-' + month_start + '-' + '0' + str(day_range) + ' 00:00:00' date_end = year_end + '-' + month_end + '-' + '0' + str(day_range) + ' 23:00:00' else: date_start = year_start + '-' + month_start + '-' + str(day_range) + ' 00:00:00' date_end = year_end + '-' + month_end + '-' + str(day_range) + ' 23:00:00' dates, values = zip(*sorted(zip(df_num['date'],df_num[date_start: date_end][name]))) ax[i].plot_date(dates,values, '-') print(f'{date_start} at√© {date_end} com {len(df_num[date_start: date_end][name])} pontos') Com o c√≥digo acima √© poss√≠vel visualizar o comportamento de qualquer vari√°vel num√©rica em rela√ß√£o ao tempo. Por conta de ser um post no blog, eu optei em apenas mostrar uma das figuras criadas pelo c√≥digo. Mas voc√™ pode acessar o link para visualizar a sa√≠da completa dessa c√©lula. Podemos observar o seguinte: temperatura interna: Possui valores de 20 a 25, acredito que graus Celsius, tendo picos durante a noite. Per√≠odo em que provavelmente o aquecedor est√° ligado. temperatura externa: Possui valores baixos, categorizando um pa√≠s frio que √© a B√©lgica. Tamb√©m apresenta varia√ß√£o de acordo o por e nascer do sol RH e RH_out: Apresenta uma variabilidade interessante para explicar o comportament do Appliances. Press√£o: Apresenta um comportamento bem padr√£o. rv_1, rv_2: Possuem um comportamento bem aleat√≥rio ao longo do tempo, j√° que se trata realmente de uma vari√°vel aleat√≥ria adicionada aos dados. ","date":"2019-04-04","objectID":"/posts/predictive_iot_energy/:5:0","tags":["Project","Data Science","Machine Learning"],"title":"Modelagem Preditiva em IoT","uri":"/posts/predictive_iot_energy/"},{"categories":["Projetos"],"content":"FacetGrid Este √© um tipo de gr√°fico que permite a visualiza√ß√£o do comportamento de mais de duas vari√°veis ao mesmo tempo. Na figura, optei por visualizar a vari√°vel ‚ÄòAppliances‚Äô em rela√ß√£o √† ‚Äòrv1‚Äô e ‚ÄòT_out‚Äô. # Aqui basta alterar o intervalo desejado e as vari√°veis desejadas g = sns.FacetGrid(df_num['2016-05'], col=\"Appliances\", col_wrap=8, height=2, ylim=(0, 50)) g.map(sns.scatterplot, \"rv1\", \"T_out\", ci=None); Vemos que a maior varialibilidade dos dados se d√° entre 20 a 130 do Appliance. Logo a modelagem pode ser favorecida a partir da remo√ß√£o de outliers. Feature Engineering ","date":"2019-04-04","objectID":"/posts/predictive_iot_energy/:6:0","tags":["Project","Data Science","Machine Learning"],"title":"Modelagem Preditiva em IoT","uri":"/posts/predictive_iot_energy/"},{"categories":["Projetos"],"content":"Tratamento de outliers Como vimos que o ‚ÄòAppliances‚Äô possui muito valores outliers. Podemos iniciar o tratamento pela elima√ß√£o desses valores. Para isso, identifiquei o Interquantile Range para remvover o whisker superior do boxplot. Como pode ser visto no c√≥digo a seguir. # Primeiro e terceiro quartil e c√°lculo da dist√¢ncia IQR Q1 = df_num['Appliances'].quantile(0.25) Q3 = df_num['Appliances'].quantile(0.75) IQR = Q3 - Q1 print(Q1) print(Q3) print(IQR) 50.0 100.0 50.0 # Determina√ß√£o dos ranges inferiores e superiores do boxplot Lower_Whisker = Q1 - 1.5*IQR Upper_Whisker = Q3 + 1.5*IQR print(Lower_Whisker, Upper_Whisker) -25.0 175.0 # Remo√ß√£o dos dados outiliers new_df = df_num[df_num['Appliances'] \u003c 175.0] year_start = '2016' month_start = '04' day = int('01') # fim year_end = '2016' month_end = '05' if day \u003c 10: date_start = year_start + '-' + month_start date_end = year_end + '-' + month_end + '-' + '0' + str(day) + ' 23:00:00' else: date_start = year_start + '-' + month_start date_end = year_end + '-' + month_end + '-' + str(day) + ' 23:00:00' print(f'{date_start} at√© {date_end} com {len(new_df[date_start])} pontos') plt.subplots(figsize=(30, 20)) sns.set(style=\"ticks\", color_codes=True) df_filter = new_df[date_start] df_filter = df_filter.drop(['date', 'NSM'], axis=1) df_filter.boxplot() plt.show() 2016-04 at√© 2016-05-01 23:00:00 com 3870 pontos # Removendo vari√°vel de data, adicionada para ajudar na constru√ß√£o dos gr√°ficos new_df_num = new_df.drop(['date'], axis=1) # Resetando o index new_df_num.reset_index(drop=True, inplace=True) new_df_num.index RangeIndex(start=0, stop=17597, step=1) ","date":"2019-04-04","objectID":"/posts/predictive_iot_energy/:7:0","tags":["Project","Data Science","Machine Learning"],"title":"Modelagem Preditiva em IoT","uri":"/posts/predictive_iot_energy/"},{"categories":["Projetos"],"content":"Correlation Aqui foi observado como se d√° a rela√ß√£o entre as vari√°veis. Particularmente se existe uma correla√ß√£o linear entre as vari√°veis. A correla√ß√£o √© uma medida que varia de 0 a 1, onde valores pr√≥ximos a 0 possuem uma correla√ß√£o negativa e pr√≥ximos a 1 uma correla√ß√£o positiva. A correla√ß√£o negativa entre duas vari√°veis representa que a cada vez que eu aumento uma a outra diminui e virse-versa. O contr√°rio tamb√©m acontece quando temos uma correla√ß√£o positiva, a medida que uma vari√°vel aumenta a outra tamb√©m aumenta. # Using Pearson Correlation # General correlation plt.figure(figsize=(25,15)) cor = new_df_num.corr() sns.heatmap(cor, annot=True, cmap=plt.cm.Reds) plt.show() #Correlation with output variable cor_target = abs(cor[\"Appliances\"]) #Selecting highly correlated features relevant_features = cor_target[cor_target\u003c0.2] relevant_features RH_1 0.045596 RH_2 0.109746 T3 0.180061 RH_3 0.088410 T4 0.195689 RH_4 0.036932 T5 0.191782 RH_5 0.072040 T7 0.175519 RH_7 0.128740 T9 0.154471 Press_mm_hg 0.089829 Windspeed 0.055363 Visibility 0.024974 Tdewpoint 0.081550 rv1 0.009986 rv2 0.009986 Name: Appliances, dtype: float64 Uma observa√ß√£o interessante √© que maioria das vari√°veis possuem uma correla√ß√£o negativa com a vari√°vel target, ou seja, o ‚ÄòAppliances‚Äô aumenta a medida que uma vari√°vel espec√≠fica diminui. ","date":"2019-04-04","objectID":"/posts/predictive_iot_energy/:8:0","tags":["Project","Data Science","Machine Learning"],"title":"Modelagem Preditiva em IoT","uri":"/posts/predictive_iot_energy/"},{"categories":["Projetos"],"content":"Normaliza√ß√£o Como estamos trabalhando com dados num√©ricos e que possuem uma distribui√ß√£o com intervalos diferentes, pode ser que o algoritmo de machine learning escolhido seja sens√≠vel a este tipo de comportamento. Outro ajuste que se faz necess√°rio √© a normaliza√ß√£o dos dados. # lembrando que a vari√°vel target n√£o tem necessidade de ser normalizada new_df_num_Y = new_df_num['Appliances'] new_df_num = new_df_num.drop(['Appliances'], axis=1) # normaliza√ß√£o min_max_scaler = MinMaxScaler() columns = list(new_df_num.columns) print(columns) for name in columns: # convertemos para np array pois √© a forma que o min_max_scaler # recebe os dados new_df_num[name] = min_max_scaler.fit_transform(np.array(new_df_num[name]).reshape(-1, 1)) ['lights', 'T1', 'RH_1', 'T2', 'RH_2', 'T3', 'RH_3', 'T4', 'RH_4', 'T5', 'RH_5', 'T6', 'RH_6', 'T7', 'RH_7', 'T8', 'RH_8', 'T9', 'RH_9', 'T_out', 'Press_mm_hg', 'RH_out', 'Windspeed', 'Visibility', 'Tdewpoint', 'rv1', 'rv2', 'NSM'] Algumas vari√°veis ainda possuem muitos outliers, mas ser√£o mantidas. Feature Selection Aqui apliquei 2 diferentes m√©todos de sele√ß√£o de vari√°veis. ","date":"2019-04-04","objectID":"/posts/predictive_iot_energy/:9:0","tags":["Project","Data Science","Machine Learning"],"title":"Modelagem Preditiva em IoT","uri":"/posts/predictive_iot_energy/"},{"categories":["Projetos"],"content":"Recursive Elimination Neste m√©todo, precisamos definir quantas vari√°veis queremos para rodar o modelo, e depois, quando obtido o n√∫mero √≥timo aplicamos a sele√ß√£o de vari√°veis. O m√©todo identifica, recursivamente, qual conjunto de vari√°veis possuem maior score. new_df_num.shape (17597, 28) #no of features nof_list=np.arange(1,28) high_score=0 #Variable to store the optimum features nof=0 score_list =[] for n in range(len(nof_list)): X_train, X_test, y_train, y_test = train_test_split(new_df_num, new_df_num_Y, test_size = 0.3, random_state = 0) model = LinearRegression() rfe = RFE(model,nof_list[n]) X_train_rfe = rfe.fit_transform(X_train,y_train) X_test_rfe = rfe.transform(X_test) model.fit(X_train_rfe,y_train) score = model.score(X_test_rfe,y_test) score_list.append(score) if(score\u003ehigh_score): high_score = score nof = nof_list[n] print(\"Optimum number of features: %d\" %nof) print(\"Score with %d features: %f\" % (nof, high_score)) Optimum number of features: 21 Score with 21 features: 0.353213 cols = list(new_df_num.columns) model = LinearRegression() #Initializing RFE model rfe = RFE(model, 21) #Transforming data using RFE X_train, X_test, y_train, y_test = train_test_split(new_df_num, new_df_num_Y, test_size = 0.3, random_state = 0) X_rfe = rfe.fit_transform(X_train,y_train) #Fitting the data to model model.fit(X_rfe,y_train) temp = pd.Series(rfe.support_,index = cols) selected_features_rfe = temp[temp==True].index print(selected_features_rfe) Index(['lights', 'T1', 'RH_1', 'T2', 'RH_2', 'T3', 'RH_3', 'T4', 'RH_4', 'T5', 'RH_5', 'T6', 'T7', 'RH_7', 'T8', 'RH_8', 'T9', 'RH_9', 'T_out', 'Windspeed', 'NSM'], dtype='object') ","date":"2019-04-04","objectID":"/posts/predictive_iot_energy/:10:0","tags":["Project","Data Science","Machine Learning"],"title":"Modelagem Preditiva em IoT","uri":"/posts/predictive_iot_energy/"},{"categories":["Projetos"],"content":"Embedded Method reg = LassoCV() reg.fit(X_train, y_train) print(\"Best alpha using built-in LassoCV: %f\" % reg.alpha_) print(\"Best score using built-in LassoCV: %f\" %reg.score(X_train,y_train)) coef = pd.Series(reg.coef_, index = new_df_num.columns) imp_coef = coef.sort_values() import matplotlib matplotlib.rcParams['figure.figsize'] = (8.0, 10.0) imp_coef.plot(kind = \"barh\") plt.title(\"Feature importance using Lasso Model\") Best alpha using built-in LassoCV: 0.003504 Best score using built-in LassoCV: 0.356319 Text(0.5, 1.0, 'Feature importance using Lasso Model') Machine Learning Na etapa de escolha do modelo, selecionei 3 m√©todos para problemas de regress√£o, problema que est√° sendo modelado, e avaliei a m√©tria de R¬≤. # removendo vari√°veis alet√≥rias antes de aplicar o modelo new_df_v = new_df_num.drop(['rv1', 'rv2'], axis=1) X_train, X_test, y_train, y_test = train_test_split(new_df_v, new_df_num_Y, train_size=0.7) # Regress√£o Linear M√∫ltipla modelo = LinearRegression() modelo.fit(X_train, y_train) y_pred = modelo.predict(X_test) score = r2_score(y_test, y_pred) print(score) 0.34058856776814994 # Support Vector Regression modelo = SVR() modelo.fit(X_train, y_train) y_pred = modelo.predict(X_test) score = r2_score(y_test, y_pred) print(score) 0.3594652532873541 # Gradient Boosting Regressor params = {'n_estimators': 500, 'max_depth': 8, 'min_samples_split': 2, 'learning_rate': 0.01, 'loss': 'ls'} clf = ensemble.GradientBoostingRegressor(**params) clf.fit(X_train, y_train) y_pred = clf.predict(X_test) mse = mean_squared_error(y_test, y_pred) r2score = r2_score(y_test, y_pred) print(f\"R2 SCORE: {r2score}\") R2 SCORE: 0.6598 Vemos ent√£o que o modelo que apresentou melhor resultado foi o Gradient Boosting Regressor, com 65,98% na m√©trica de R¬≤. Tal resultado pode ser tido como um resultado preliminar, e etapas de otimiza√ß√£o podem ser realizadas daqui em diante para melhorar o score do modelo. Como formas de otimiza√ß√£o que ainda n√£o foram testadas, temos: Utiliza√ß√£o da t√©cnica de An√°lise por Componentes Principais para diminui√ß√£o da dimensionalidade do modelo. Cria√ß√£o de novas vari√°veis. Utiliza√ß√£o das vari√°veis categ√≥ricas de data como; dia da semana, m√™s, feriado etc. Utiliza√ß√£o do gridsearch para buscar um refinamento do modelo de maior score. ","date":"2019-04-04","objectID":"/posts/predictive_iot_energy/:11:0","tags":["Project","Data Science","Machine Learning"],"title":"Modelagem Preditiva em IoT","uri":"/posts/predictive_iot_energy/"},{"categories":["Aplica√ß√µes"],"content":"Ol√°, como sabemos a ci√™ncia de dados √© uma tecnologia de grande potencial e vem sendo aplicada em diferentes √°reas de neg√≥cio. Estou iniciando uma s√©rie de artigos que visam aprensentar sua aplica√ß√£o em 5 √°reas distintas: Log√≠stica, Sa√∫de, Turismo, Esporte e E-Sports e Engenharia. Antes de iniciar propriamente os t√≥picos, √© interessante que estejamos na mesma p√°gina quando falamos de Data Science. Devido ao grande hype dessa tecnologia, ela pode acabar sendo mal interpretada, por isso eu gosto de utilizar uma defini√ß√£o dada pela DataRobot uma empresa pioneira em Machine Learning, localizada no Quadrante m√°gico do Gartner. ‚ÄúA ci√™ncia de dados √© o campo de estudo que combina habilidades de programa√ß√£o e conhecimento de matem√°tica e estat√≠stica para extrair informa√ß√µes significativas dos dados. Os profissionais de ci√™ncia de dados aplicam algoritmos de aprendizado de m√°quina √† n√∫meros, texto, imagens, v√≠deo, √°udio e muito mais para produzir sistemas de intelig√™ncia artificial (IA) para executar tarefas que normalmente exigem intelig√™ncia humana. Por sua vez, esses sistemas geram insights que analistas e empres√°rios podem traduzir em valor comercial tang√≠vel.‚Äù Como apresentado no conceito acima, o objetivo √© extrair informa√ß√µes a partir dos dados, sejam eles estruturados ou n√£o-estruturados. E tudo isso s√≥ √© poss√≠vel devido ao Big Data, que representa a chamada ‚Äútempestade perfeita‚Äù, pois o ser humano pela primeira vez na hist√≥ria est√° gerando uma quantidade massiva de dados e ao mesmo tempo possui poder computacional para processar toda essa informa√ß√£o. Vejamos ent√£o como isso se aplica √† diferentes √°reas de neg√≥cios ","date":"2019-03-16","objectID":"/posts/5_topics_data_science_application/:0:0","tags":["Log√≠stica","Logistic","Data Science","Machine Learning"],"title":"5 Diferentes Aplica√ß√µes de Ci√™ncia de Dados","uri":"/posts/5_topics_data_science_application/"},{"categories":["Aplica√ß√µes"],"content":"Log√≠stica Tem ocorrido grandes mudan√ßas no setor de log√≠stica e cadeias de abastecimento devido ao desenvolvimento das tecnologias de Data Science (DS). Os benef√≠cios observados s√£o diversos, e sua aplica√ß√£o vai desde an√°lises preditivas e rotas inteligentes at√© ve√≠culos aut√¥nomos. A melhor forma de vislumbrar esse desenvolvimento √© analisar as aplica√ß√µes da tecnologia. Ent√£o pontuei 3 t√≥picos de utiliza√ß√£o de DS na log√≠stica ","date":"2019-03-16","objectID":"/posts/5_topics_data_science_application/:1:0","tags":["Log√≠stica","Logistic","Data Science","Machine Learning"],"title":"5 Diferentes Aplica√ß√µes de Ci√™ncia de Dados","uri":"/posts/5_topics_data_science_application/"},{"categories":["Aplica√ß√µes"],"content":"1 ‚Äì Armazenamento Automatizado Atualmente, existe uma tend√™ncia para a IA transformar opera√ß√µes de armazenamento, como coleta e an√°lise de informa√ß√µes ou processamento de invent√°rio. Como resultado, a IA ajuda a aumentar a efici√™ncia e obter lucro. Como funciona? A intelig√™ncia artificial √© usada para prever a demanda por determinados produtos. Depois disso, a empresa entrega itens exigentes aos armaz√©ns regionais, reduzindo os custos de transporte. De acordo com a Vero Solutions, cerca de 30% das tarefas de armazenamento poder√£o ser automatizadas em um futuro pr√≥ximo. Um caso bastante interessante √© de um supermercado on-line localizado na Gr√£-Bretanha, Ocado. Esta empresa desenvolveu um armaz√©m automatizado. Existe um rob√¥ chamado ‚Äúhive-grid-machine‚Äù que realiza os pedidos muito mais rapidamente do que os trabalhadores. Durante uma semana, a ‚Äòhive-grid-machine‚Äô pode atender 65.000 pedidos ou substituir 3,5 milh√µes de itens de mercearia. Esses rob√¥s ajudam a mover, levantar e classificar as coisas. Depois disso, os funcion√°rios do Ocado embalam e enviam pedidos. Como resultado, o tempo necess√°rio para enviar um pedido √© reduzido. Armaz√©ns automatizados tendem a usar a vis√£o computacional. Essa tecnologia permite reconhecer e organizar os itens. ","date":"2019-03-16","objectID":"/posts/5_topics_data_science_application/:1:1","tags":["Log√≠stica","Logistic","Data Science","Machine Learning"],"title":"5 Diferentes Aplica√ß√µes de Ci√™ncia de Dados","uri":"/posts/5_topics_data_science_application/"},{"categories":["Aplica√ß√µes"],"content":"2 ‚Äì Detec√ß√£o de Padr√µes Visuais O aprendizado de m√°quina √© excelente no reconhecimento visual de padr√µes, abrindo muitas aplica√ß√µes em potencial na inspe√ß√£o f√≠sica e manuten√ß√£o de ativos f√≠sicos em toda uma rede da cadeia de suprimentos. Projetado usando algoritmos que buscam rapidamente colocar padr√µes compar√°veis em v√°rios conjuntos de dados, o aprendizado de m√°quina tamb√©m est√° se mostrando muito eficaz na automa√ß√£o da inspe√ß√£o de qualidade de entrada em todos os hubs de log√≠stica, isolando as remessas de produtos com danos e desgaste. Os algoritmos de aprendizado de m√°quina na plataforma Watson da IBM foram capazes de determinar se um cont√™iner de remessa e / ou produto foram danificados, classific√°-lo por tempo de dano e recomendar a melhor a√ß√£o corretiva para reparar os ativos. O Watson combina dados visuais e baseados em sistemas para rastrear, relatar e fazer recomenda√ß√µes em tempo real. Fonte: Intelig√™ncia Artificial em Log√≠stica. ","date":"2019-03-16","objectID":"/posts/5_topics_data_science_application/:1:2","tags":["Log√≠stica","Logistic","Data Science","Machine Learning"],"title":"5 Diferentes Aplica√ß√µes de Ci√™ncia de Dados","uri":"/posts/5_topics_data_science_application/"},{"categories":["Aplica√ß√µes"],"content":"3 ‚Äì Prevendo o custo de carga Prever o pre√ßo de uma carga pode ser complicado, porque o custo envolvido em um transporte varia de esta√ß√£o para esta√ß√£o e at√© do sol para a chuva no dia-a-dia ou na hora do dia. A IA pode ajudar a monitorar essas condi√ß√µes e escolher o pre√ßo certo com base no tempo de entrega e em qual ‚Äúfaixa‚Äù - rota e destino - um embarque √© encaminhado. Esses algoritmos monitoram uma s√©rie de par√¢metros como tr√°fego, clima e par√¢metros socioecon√¥micos que ajudam as empresas a alcan√ßar um pre√ßo justo com o qual ambas as partes podem concordar. Fonte: Towards Data Science e Forbes ","date":"2019-03-16","objectID":"/posts/5_topics_data_science_application/:1:3","tags":["Log√≠stica","Logistic","Data Science","Machine Learning"],"title":"5 Diferentes Aplica√ß√µes de Ci√™ncia de Dados","uri":"/posts/5_topics_data_science_application/"}]